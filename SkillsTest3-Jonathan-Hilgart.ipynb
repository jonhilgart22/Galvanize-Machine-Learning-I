{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name:  Jonathan Hilgart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills Test 3\n",
    "\n",
    "1.  The Gauss-Markov theorem proves that OLS is the best Best Linear Unbiased Estimator (BLUE) of regression coefficients.  What are the requirements for the Gauss-Markov theorem? (list all that apply)\n",
    "```\n",
    "    a. The errors in the labels are heteroscedastic\n",
    "    b. The estimate of the regression coefficients must be linear in the problem labels (aka y's, outcomes etc.).\n",
    "    c. The errors in the labels must be zero mean.\n",
    "    d. The features must be linear functions of one another\n",
    "    e. The errors in the labels must be independent of one another.\n",
    "    f. The errors in the labels must have fixed, finite variance.\n",
    "    g. The estimates of the regression coefficients must be linear in the X's\n",
    "    h. The relationship between the labels and the features (X's, predictors, etc.) is linear.\n",
    "    i. The matrix X must be full rank.\n",
    "    Answer:\n",
    "```\n",
    "\n",
    "2.  Given a column vector of labels y and a corresponding matrix of X's, give the matrix vector expression for the OLS coefficients (in Latex).\n",
    "\n",
    "\n",
    "3.  Which of the following are reasons you might suspect SVM would be better suited to your problem than penalized linear regression?  (pick all that apply)\n",
    "```\n",
    "    a. Large errors are exponentially more important than small ones.\n",
    "    b. Training must be very rapid.\n",
    "    c. Errors near the decision boundary are more important than large errors.\n",
    "    d. You need a technique that can model dependence of labels on non-linear functions of the attributes\n",
    "    e. You have a classification problem not a regression problem.\n",
    "    f. Points on the correct side of the decision boundary should not get penalized at all\n",
    "    Answer:\n",
    "```\n",
    "\n",
    "4.  Which of the following are properties of the \"Kernel Trick\"?  (list all that apply)\n",
    "```\n",
    "    a. Kernel trick is applied in dual space\n",
    "    b. Kernel trick performs basis expansion for SVM\n",
    "    c. Kernel trick makes training faster than with simple lineaer kernel\n",
    "    d. Any basis expansion (for example, all of the ones we studied earlier) can be used in the kernel trick\n",
    "    Answer:\n",
    "```\n",
    "\n",
    "5. Which of the following are correct comparisons between K-means and Gaussian mixture model clustering.\n",
    "```\n",
    "    a. Computational complexity for GMM and K-means both have quadratic dependence on number of rows of data.\n",
    "    b. K-means clusters are roughly the same size whereas GMM clusters can be radically different\n",
    "    c. K-means is sensitive to scaling whereas GMM is indifferent.\n",
    "    d. GMM only clusters numeric data, whereas K-means will work with categorical data.\n",
    "    e. K-means algo can lead to sub-optimal clusters when data is very well separated with large gaps between clusters.  Not true with GMM.\n",
    "    Answer:\n",
    "```\n",
    "\n",
    "6.  Which of the following statements about silhouette statistics are true?\n",
    "```\n",
    "    a. Silhouette always gives a clear indication of best number of clusters\n",
    "    b. Silhouette statistics give a visual indication of clustering performance.\n",
    "    c. Silhouette statistics measure the proximity between points within a cluster relative to their proximity to points in orther clusters.\n",
    "    d. Silhouette analysis is indifferent to scaling of your data\n",
    "    Answer:\n",
    "```\n",
    "\n",
    "The following function should do element wise multiplication. This means your input has to be  a numpy array. Use try-catch to determine if the input (x) is a numpy array and return an error message if it isn't.\n",
    "\n",
    "```python\n",
    "    def np_double_sum(x):\n",
    "       x = x * 2\n",
    "       return x.sum()\n",
    "```\n",
    "\n",
    "As an example if I run your code the output should be:\n",
    "\n",
    "```python \n",
    "    import numpy\n",
    "    a = np.array([1,2,3])\n",
    "    print(np_double_sum(a))\n",
    "    # 12\n",
    "    b = 2\n",
    "    print(np_double(b))\n",
    "    # Not a valid input. Needs to be a numpy array.\n",
    "```\n",
    "\n",
    "\n",
    "Annotate the code (figure out what it is going on at each step and write comments to help a reader follow the logic). \n",
    "\n",
    "```python\n",
    "    def mystery_function(x):\n",
    "        s = []\n",
    "        while len(x) > 1:\n",
    "            num = min(x)\n",
    "            s.append(num)\n",
    "            x.pop(x.index(num))\n",
    "        s += x\n",
    "        return s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.  The Gauss-Markov theorem proves that OLS is the best Best Linear Unbiased Estimator (BLUE) of regression coefficients.  What are the requirements for the Gauss-Markov theorem? (list all that apply)\n",
    "```\n",
    "    a. The errors in the labels are heteroscedastic\n",
    "    b. The estimate of the regression coefficients must be linear in the problem labels (aka y's, outcomes etc.).\n",
    "    c. The errors in the labels must be zero mean.\n",
    "    d. The features must be linear functions of one another\n",
    "    e. The errors in the labels must be independent of one another.\n",
    "    f. The errors in the labels must have fixed, finite variance.\n",
    "    g. The estimates of the regression coefficients must be linear in the X's\n",
    "    h. The relationship between the labels and the features (X's, predictors, etc.) is linear.\n",
    "    i. The matrix X must be full rank.\n",
    "    Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  c,e, f, h, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2 .  Given a column vector of labels y and a corresponding matrix of X's, give the matrix vector expression for the OLS coefficients (in Latex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ${\\hat {\\beta }}=(X^{T}X)^{-1}X^{T}y\\ .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 .  Which of the following are reasons you might suspect SVM would be better suited to your problem than penalized linear regression?  (pick all that apply)\n",
    "```\n",
    "    a. Large errors are exponentially more important than small ones.\n",
    "    b. Training must be very rapid.\n",
    "    c. Errors near the decision boundary are more important than large errors.\n",
    "    d. You need a technique that can model dependence of labels on non-linear functions of the attributes\n",
    "    e. You have a classification problem not a regression problem.\n",
    "    f. Points on the correct side of the decision boundary should not get penalized at all\n",
    "    Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c, d, e, f, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 .  Which of the following are properties of the \"Kernel Trick\"?  (list all that apply)\n",
    "```\n",
    "    a. Kernel trick is applied in dual space\n",
    "    b. Kernel trick performs basis expansion for SVM\n",
    "    c. Kernel trick makes training faster than with simple lineaer kernel\n",
    "    d. Any basis expansion (for example, all of the ones we studied earlier) can be used in the kernel trick\n",
    "    Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a,b, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5 . Which of the following are correct comparisons between K-means and Gaussian mixture model clustering.\n",
    "```\n",
    "    a. Computational complexity for GMM and K-means both have quadratic dependence on number of rows of data.\n",
    "    b. K-means clusters are roughly the same size whereas GMM clusters can be radically different\n",
    "    c. K-means is sensitive to scaling whereas GMM is indifferent.\n",
    "    d. GMM only clusters numeric data, whereas K-means will work with categorical data.\n",
    "    e. K-means algo can lead to sub-optimal clusters when data is very well separated with large gaps between clusters.  Not true with GMM.\n",
    "    Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a, b, c, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  Which of the following statements about silhouette statistics are true?\n",
    "```\n",
    "    a. Silhouette always gives a clear indication of best number of clusters\n",
    "    b. Silhouette statistics give a visual indication of clustering performance.\n",
    "    c. Silhouette statistics measure the proximity between points within a cluster relative to their proximity to points in orther clusters.\n",
    "    d. Silhouette analysis is indifferent to scaling of your data\n",
    "    Answer:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function should do element wise multiplication. This means your input has to be  a numpy array. Use try-catch to determine if the input (x) is a numpy array and return an error message if it isn't.\n",
    "\n",
    "```python\n",
    "    def np_double_sum(x):\n",
    "       x = x * 2\n",
    "       return x.sum()\n",
    "```\n",
    "\n",
    "As an example if I run your code the output should be:\n",
    "\n",
    "```python \n",
    "    import numpy\n",
    "    a = np.array([1,2,3])\n",
    "    print(np_double_sum(a))\n",
    "    # 12\n",
    "    b = 2\n",
    "    print(np_double(b))\n",
    "    # Not a valid input. Needs to be a numpy array.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def np_double_sum(x):\n",
    "    try:\n",
    "        type(x)=='numpy.ndarray' ## check type\n",
    "        x = x * 2\n",
    "        return x.sum()\n",
    "    except:\n",
    "        return ' Not a valid input. Needs to be a numpy array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "print(np_double_sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(a)\n",
    "b=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Not a valid input. Needs to be a numpy array\n"
     ]
    }
   ],
   "source": [
    "print(np_double_sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the code (figure out what it is going on at each step and write comments to help a reader follow the logic). \n",
    "\n",
    "```python\n",
    "    def mystery_function(x):\n",
    "        s = []\n",
    "        while len(x) > 1:\n",
    "            num = min(x)\n",
    "            s.append(num)\n",
    "            x.pop(x.index(num))\n",
    "        s += x\n",
    "        return s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mystery_function(x):\n",
    "        s = [] ### list to store the number of x as it is sorted\n",
    "        while len(x) > 1: ### if the length of x is greater than one\n",
    "            num = min(x) ## find the minimum number of x\n",
    "            s.append(num) ## append the minimum number of x to list s\n",
    "            x.pop(x.index(num)) ## take the location(index) of the minimum number in x, then remove this number from x\n",
    "            ## Continue through this loop, remove the smallest elements of x until there is only one element left\n",
    "        s += x ## add the final element to s (the largest element)\n",
    "        return s ## s is now a sorted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = [ 1,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystery_function(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
