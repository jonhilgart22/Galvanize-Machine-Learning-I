{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 Practicum I: Random Forests\n",
    "\n",
    "Your study of tree classifiers begins with random forests. \n",
    "\n",
    "## Implement Decision Trees\n",
    "\n",
    "In order to build a random forest you must first master building decision trees.\n",
    "\n",
    "1. If you have not yet completed working code for decision trees, start with getting a complete implementation using the annotated code stub DecisionTree.py and TreeNode.py provided to you in the /code directory. \n",
    "\n",
    "2. Use the run_decision_tree.py and test_decision_tree.py code stubs (with the command line) to ensure that your construction is correct. Use pycharm or sublime for a develop environment.\n",
    "\n",
    "3. Once your tree is capable of producing correct results, continue with the RandomForest.py stub, discussed below.\n",
    "\n",
    "4. You can check your performance of both the forest and trees against the setup of the executable in the practicum directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-89c2b566551c>, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-89c2b566551c>\"\u001b[0;36m, line \u001b[0;32m85\u001b[0m\n\u001b[0;31m    else: # otherwise we can split (again this comes out of choose_split_index\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from TreeNode import TreeNode\n",
    "\n",
    "\n",
    "class DecisionTree(object):\n",
    "    '''\n",
    "    A decision tree class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy'):\n",
    "        '''\n",
    "        Initialize an empty DecisionTree.\n",
    "        '''\n",
    "\n",
    "        self.root = None  # root Node\n",
    "        self.feature_names = None  # string names of features (for interpreting\n",
    "                                   # the tree)\n",
    "        self.categorical = None  # Boolean array of whether variable is\n",
    "                                 # categorical (or continuous)\n",
    "        self.impurity_criterion = self._entropy \\\n",
    "                                  if impurity_criterion == 'entropy' \\\n",
    "                                  else self._gini\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - feature_names: numpy array of strings\n",
    "        OUTPUT: None\n",
    "        Build the decision tree.\n",
    "        X is a 2 dimensional array with each column being a feature and each\n",
    "        row a data point.\n",
    "        y is a 1 dimensional array with each value being the corresponding\n",
    "        label.\n",
    "        feature_names is an optional list containing the names of each of the\n",
    "        features.\n",
    "        '''\n",
    "\n",
    "\n",
    "        # This piece of code is used to provide feature names to the Decision tree\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            # if the user has not provided feature names, just give them numbers\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            # otherwise, these are the names\n",
    "            self.feature_names = feature_names\n",
    "            \n",
    "        \n",
    "\n",
    "        # * Create True/False array of whether the variable is categorical\n",
    "        # use a lambda function called is_categorical to determine if the variable is an instance\n",
    "        # of str, bool or unicode - in that case is_categorical will be true\n",
    "        # otherwise False. Look up the function isinstance()\n",
    "\n",
    "        #is_categorical = lambda x: ?\n",
    "\n",
    "        # Each variable (organized by index) is given a label categorical or not\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        # Call the build_tree function\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - TreeNode\n",
    "        Recursively build the decision tree. Return the root node.\n",
    "        '''\n",
    "\n",
    "        #  * initialize a root TreeNode\n",
    "\n",
    "        # * set index, value, splits as the output of self._choose_split_index(X,y)\n",
    "\n",
    "        # if no index is returned from the split index or we cannot split\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            # * set the node to be a leaf\n",
    "\n",
    "            # * set the classes attribute to the number of classes\n",
    "            # * we have in this leaf with Counter()\n",
    "\n",
    "            # * set the name of the node to be the most common class in it\n",
    "\n",
    "        else: # otherwise we can split (again this comes out of choose_split_index\n",
    "            # * set X1, y1, X2, y2 to be the splits\n",
    "\n",
    "            # * the node column should be set to the index coming from split_index\n",
    "\n",
    "            # * the node name is the feature name as determined by\n",
    "            #   the index (column name)\n",
    "\n",
    "            # * set the node value to be the value of the split\n",
    "\n",
    "            # * set the categorical flag of the node to be the category of the column\n",
    "\n",
    "            # * now continue recursing down both branches of the split\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the entropy of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the total length of y. This is the p(C)\n",
    "            # * add the entropy p(C) ln p(C) to the total\n",
    "        return -total\n",
    "\n",
    "    def _gini(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the gini impurity of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the size of y. This is the p(C)\n",
    "            # * add p(C)**2 to the total\n",
    "        return 1 - total\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - split_index: int (index of feature)\n",
    "            - split_value: int/float/bool/str (value of feature)\n",
    "        OUTPUT:\n",
    "            - X1: 2d numpy array (feature matrix for subset 1)\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - X2: 2d numpy array (feature matrix for subset 2)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        Return the two subsets of the dataset achieved by the given feature and\n",
    "        value to split on.\n",
    "        Call the method like this:\n",
    "        X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "        X1, y1 is a subset of the data.\n",
    "        X2, y2 is the other subset of the data.\n",
    "        '''\n",
    "\n",
    "        # * slice the split column from X with the split_index\n",
    "        # * if the variable of this column is categorical\n",
    "            # * select the indices of the rows in the column\n",
    "            #  with the split_value (T/F) into one set of indices (call them A)\n",
    "            # * select the indices of the rows in the column\n",
    "            # that don't have the split_value into another\n",
    "            #  set of indices (call them B)\n",
    "        # * else if the variable is not categorical\n",
    "             # * select the indices of the rows in the column\n",
    "            #  less than the split value into one set of indices (call them A)\n",
    "            # * select the indices of the rows in the column\n",
    "            #  greater or equal to  the split value into\n",
    "            # another set of indices (call them B)\n",
    "        return X[A], y[A], X[B], y[B]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the information gain of making the given split.\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "        '''\n",
    "        # * set total equal to the impurity_criterion\n",
    "\n",
    "        # * for each of the possible splits y1 and y2\n",
    "            # * calculate the impurity_criterion of the split\n",
    "            # * subtract this value from the total, multiplied by split_size/y_size\n",
    "        return total\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - index: int (index of feature)\n",
    "            - value: int/float/bool/str (value of feature)\n",
    "            - splits: (2d array, 1d array, 2d array, 1d array)\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "        Call the method like this:\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "        X1, y1, X2, y2 = splits\n",
    "        '''\n",
    "\n",
    "        # set these initial variables to None\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        # we need to keep track of the maximum entropic gain\n",
    "        max_gain = 0\n",
    "\n",
    "        # * for each column in X\n",
    "            # * set an array called values to be the\n",
    "            # unique values in that column (use np.unique)\n",
    "\n",
    "            # if there are less than 2 values, move on to the next column\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "\n",
    "            # * for each value V in the values array\n",
    "\n",
    "                # * make a temporary split (using the column index and V) with make_split\n",
    "\n",
    "                # * calculate the information gain between the original y, y1 and y2\n",
    "\n",
    "                # * if this gain is greater than the max_gain\n",
    "                    # * set max_gain, split_index, and split_value to be equal\n",
    "                    # to the current max_gain, column and value\n",
    "\n",
    "                    # * set the output splits to the current split setup (X1, y1, X2, y2)\n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "        OUTPUT:\n",
    "            - y: 1d numpy array\n",
    "        Return an array of predictions for the feature matrix X.\n",
    "        '''\n",
    "\n",
    "        return np.apply_along_axis(self.root.predict_one, axis=1, arr=X)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Return string representation of the Decision Tree. This will allow you to $:print tree\n",
    "        '''\n",
    "        return str(self.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    A node class for a decision tree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - x: 1d numpy array (single data point)\n",
    "        OUTPUT:\n",
    "            - y: predicted label\n",
    "        Return the predicted label for a single data point.\n",
    "        '''\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "\n",
    "    # This is for visualizing your tree. You don't need to look into this code.\n",
    "    def as_string(self, level=0, prefix=\"\"):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - level: int (amount to indent)\n",
    "        OUTPUT:\n",
    "            - prefix: str (to start the line with)\n",
    "        Return a string representation of the tree rooted at this node.\n",
    "        '''\n",
    "        result = \"\"\n",
    "        if prefix:\n",
    "            indent = \"  |   \" * (level - 1) + \"  |-> \"\n",
    "            result += indent + prefix + \"\\n\"\n",
    "        indent = \"  |   \" * level\n",
    "        result += indent + \"  \" + str(self.name) + \"\\n\"\n",
    "        if not self.leaf:\n",
    "            if self.categorical:\n",
    "                left_key = str(self.value)\n",
    "                right_key = \"no \" + str(self.value)\n",
    "            else:\n",
    "                left_key = \"< \" + str(self.value)\n",
    "                right_key = \">= \" + str(self.value)\n",
    "            result += self.left.as_string(level + 1, left_key + \":\")\n",
    "            result += self.right.as_string(level + 1, right_key + \":\")\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.as_string().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build a Random Forest\n",
    "\n",
    "You will be using our implementation of Decision Trees to implement a Random Forest.\n",
    "\n",
    "You can use the `DecisionTree` class from `DecisionTree.py` with the following code:\n",
    "\n",
    "```python\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, y_train)\n",
    "predicted_y = dt.predict(X_test)\n",
    "```\n",
    "\n",
    "You can also visualize a Decision Tree by printing it. This may be helpful for understanding your Random Forest.\n",
    "\n",
    "```python\n",
    "print dt\n",
    "```\n",
    "\n",
    "While you're getting your code to work, use the play golf data set that we used for implementing Decision Trees.\n",
    "\n",
    "There's a file called `RandomForest.py` which contains a skeleton of the code. Your goal is to fill it in so that you can run it with the following lines of code:\n",
    "\n",
    "```python\n",
    "from RandomForest import RandomForest\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/playgolf.csv')\n",
    "y = df.pop('Result').values\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "rf = RandomForest(num_trees=10, num_features=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_predict = rf.predict(X_test)\n",
    "print \"score:\", rf.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "### A. Implement *Tree Bagging*\n",
    "\n",
    "Bagging, or *bootstrap aggregating*, is taking several random samples *with replacement* from the data set and building a model for each sample. Each of these models gets a vote on the prediction.\n",
    "\n",
    "Sampling with replacement means that we can repeat data points. In the basic random forest, we will always use a sample size that is the same as the size of the original data set. Many data points will not be included in each sample and many will be repeated.\n",
    "\n",
    "1. Implement the `build_forest` method. For right now, we will be ignoring the `num_features` parameter. Here is the pseudocode:\n",
    "\n",
    "      Repeat num_trees times:\n",
    "          Create a random sample of the data with replacement\n",
    "          Build a decision tree with that sample\n",
    "      Return the list of the decision trees created\n",
    "\n",
    "\n",
    "### B. Implement random feature selection\n",
    "\n",
    "1. Modify the `DecisionTree` class so that it takes an additional parameter: `num_features`. This is the number of features to consider at each node in choosing the best split. Which features to consider is randomly chosen at each node. You will need to modify the `__init__`, method to take a `num_features` parameter. In `_choose_split_index`, you should randomly select `num_features` of the potential features to consider. Only calculate and compare the features that were randomly chosen, so that the feature you choose is one of the randomly chosen features.\n",
    "\n",
    "2. Modify `build_forest` in your `RandomForest` class to pass the `num_features` parameter to the Decision Trees.\n",
    "\n",
    "\n",
    "### C. Implement classification and scoring\n",
    "\n",
    "1. In the `predict` method, you should have each Decision Tree classify each data point. Choose the label with the majority of trees. Break ties by choosing one of the labels arbitrarily.\n",
    "\n",
    "2. In the `score` method, you should first classify the data points and count the percent of them which match the given labels.\n",
    "\n",
    "\n",
    "### D. Try a bigger data set\n",
    "\n",
    "You won't be able to get great results cross validating with the play golf data set since it's so small. In the data folder, there's a dataset called 'congressional_voting.csv'. This contains congressman, how they voted on different issues and their party.\n",
    "\n",
    "Here are what the 17 columns refer to:\n",
    "\n",
    "* Class Name: 2 (democrat, republican)\n",
    "* handicapped-infants: 2 (y,n)\n",
    "* water-project-cost-sharing: 2 (y,n)\n",
    "* adoption-of-the-budget-resolution: 2 (y,n)\n",
    "* physician-fee-freeze: 2 (y,n)\n",
    "* el-salvador-aid: 2 (y,n)\n",
    "* religious-groups-in-schools: 2 (y,n)\n",
    "* anti-satellite-test-ban: 2 (y,n)\n",
    "* aid-to-nicaraguan-contras: 2 (y,n)\n",
    "* mx-missile: 2 (y,n)\n",
    "* immigration: 2 (y,n)\n",
    "* synfuels-corporation-cutback: 2 (y,n)\n",
    "* education-spending: 2 (y,n)\n",
    "* superfund-right-to-sue: 2 (y,n)\n",
    "* crime: 2 (y,n)\n",
    "* duty-free-exports: 2 (y,n)\n",
    "* export-administration-act-south-africa: 2 (y,n)\n",
    "\n",
    "The dataset came from UCI [here](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records).\n",
    "\n",
    "1. Based on the votes on the 16 issues, predict the party using your implementation of Random Forest. Start with 10 trees and a maximum of 5 features.\n",
    "\n",
    "2. Compare how well the Random Forest does versus the Decision Tree.\n",
    "\n",
    "3. Try modifying the number of trees and see how it affects your accuracy.\n",
    "\n",
    "4. Calculate the accuracy for each of your decision trees on the test set and compare it to the accuracy of the random forest on the test set.\n",
    "\n",
    "5. Predict how the congressmen will vote on a particular issue given the remaining columns.\n",
    "\n",
    "\n",
    "### Extra Credit: out-of-bag error and feature importance\n",
    "\n",
    "1. Out-of-bag error is a clever way of validating your model by testing individual trees based on samples that weren't including in their training set. It is described in the lecture notes, [Applied Data Science](http://columbia-applied-data-science.github.io/appdatasci.pdf) (9.4.3) and [Breiman's notes](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr).\n",
    "\n",
    "2. Feature importance is a way of determining which features contribute the most to being able to predict the result. It is discussed in the lecture notes and [Breiman's notes](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp). You can compare what features you get with Breiman's method vs [sklearn](http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
