{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 Practicum I: Random Forests\n",
    "\n",
    "Your study of tree classifiers begins with random forests. \n",
    "\n",
    "## Implement Decision Trees\n",
    "\n",
    "In order to build a random forest you must first master building decision trees.\n",
    "\n",
    "1. If you have not yet completed working code for decision trees, start with getting a complete implementation using the annotated code stub DecisionTree.py and TreeNode.py provided to you in the /code directory. \n",
    "\n",
    "2. Use the run_decision_tree.py and test_decision_tree.py code stubs (with the command line) to ensure that your construction is correct. Use pycharm or sublime for a develop environment.\n",
    "\n",
    "3. Once your tree is capable of producing correct results, continue with the RandomForest.py stub, discussed below.\n",
    "\n",
    "4. You can check your performance of both the forest and trees against the setup of the executable in the practicum directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the DecisionTree class so that it takes an additional parameter: num_features. This is the number of features to consider at each node in choosing the best split. Which features to consider is randomly chosen at each node. You will need to modify the __init__, method to take a num_features parameter. In _choose_split_index, you should randomly select num_features of the potential features to consider. Only calculate and compare the features that were randomly chosen, so that the feature you choose is one of the randomly chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#from TreeNode import TreeNode\n",
    "\n",
    "\n",
    "class DecisionTree(object):\n",
    "    '''\n",
    "    A decision tree class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy',number_features=None):\n",
    "        '''\n",
    "        Initialize an empty DecisionTree.\n",
    "        '''\n",
    "\n",
    "        self.root = None  # root Node\n",
    "        self.feature_names = None  # string names of features (for interpreting\n",
    "                                   # the tree)\n",
    "        self.categorical = None  # Boolean array of whether variable is\n",
    "                                 # categorical (or continuous)\n",
    "        self.impurity_criterion = self._entropy \\\n",
    "                                  if impurity_criterion == 'entropy' \\\n",
    "                                  else self._gini\n",
    "        self.num_features = number_features\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - feature_names: numpy array of strings\n",
    "        OUTPUT: None\n",
    "        Build the decision tree.\n",
    "        X is a 2 dimensional array with each column being a feature and each\n",
    "        row a data point.\n",
    "        y is a 1 dimensional array with each value being the corresponding\n",
    "        label.\n",
    "        feature_names is an optional list containing the names of each of the\n",
    "        features.\n",
    "        '''\n",
    "        \n",
    "        if self.num_features!=None:\n",
    "            if self.num_features>np.shape(X)[1]:\n",
    "                return \"You do not have this many features. Please select fewer features.\"\n",
    "\n",
    "\n",
    "        # This piece of code is used to provide feature names to the Decision tree\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            # if the user has not provided feature names, just give them numbers\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            # otherwise, these are the names\n",
    "            self.feature_names = feature_names\n",
    "            \n",
    "        \n",
    "\n",
    "        # * Create True/False array of whether the variable is categorical\n",
    "        # use a lambda function called is_categorical to determine if the variable is an instance\n",
    "        # of str, bool or unicode - in that case is_categorical will be true\n",
    "        # otherwise False. Look up the function isinstance()\n",
    "        \n",
    "\n",
    "        is_categorical = lambda x: isinstance(x,str) or isinstance(x,bool)\n",
    "\n",
    "        # Each variable (organized by index) is given a label categorical or not\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        # Call the build_tree function\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - TreeNode\n",
    "        Recursively build the decision tree. Return the root node.\n",
    "        '''\n",
    "        \n",
    "        node = TreeNode()\n",
    "\n",
    "        #  * initialize a root TreeNode\n",
    "        ####### use num_features here\n",
    "        \n",
    "        # * set index, value, splits as the output of self._choose_split_index(X,y)\n",
    "        index, value, splits = self._choose_split_index(X,y)\n",
    "        # splits is a tuple here\n",
    "\n",
    "        # if no index is returned from the split index or we cannot split\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            # * set the node to be a leaf\n",
    "            node.leaf = True\n",
    "\n",
    "            # * set the classes attribute to the number of classes\n",
    "            # * we have in this leaf with Counter()\n",
    "            node.classes=Counter(y)\n",
    "            #Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "                        \n",
    "\n",
    "            # * set the name of the node to be the most common class in it\n",
    "            node.name = node.classes.most_common(1)[0][0] # return the first element, the first part of tuple will be class\n",
    "            \n",
    "            \n",
    "\n",
    "        else: # otherwise we can split (again this comes out of choose_split_index\n",
    "            # * set X1, y1, X2, y2 to be the splits\n",
    "            \n",
    "            X1, y1, X2, y2  = splits\n",
    "            \n",
    "            # * the node column should be set to the index coming from split_index\n",
    "            node.column=index\n",
    "\n",
    "            # * the node name is the feature name as determined by\n",
    "            #   the index (column name)\n",
    "            node.name = self.feature_names[index]\n",
    "\n",
    "            # * set the node value to be the value of the split\n",
    "            node.value = value\n",
    "\n",
    "            # * set the categorical flag of the node to be the category of the column\n",
    "            ## find the category of the columns\n",
    "            #self.categorical[index]\n",
    "            node.categorical = self.categorical[index]\n",
    "\n",
    "            # * now continue recursing down both branches of the split\n",
    "            node.left =self._build_tree(X1,y1)\n",
    "            node.right=self._build_tree(X2,y2)\n",
    "            \n",
    "\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the entropy of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        unique_y = set(y)\n",
    "        number_of_items = len(y)\n",
    "        counter_of_class = Counter(y)\n",
    "        \n",
    "\n",
    "        for unique_class in unique_y:\n",
    "\n",
    "            \n",
    "            total += counter_of_class[unique_class]/number_of_items* np.log(counter_of_class[unique_class]/number_of_items)\n",
    "\n",
    "                \n",
    "        # * for each unique class C in y\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the total length of y. This is the p(C)\n",
    "            # * add the entropy p(C) ln p(C) to the total\n",
    "        return -total\n",
    "\n",
    "    def _gini(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the gini impurity of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        unique_y = set(y)\n",
    "        number_of_items = len(y)\n",
    "        counter_of_class = Counter(y)\n",
    "        \n",
    "        for key,value in counter_of_class.items():\n",
    "            \n",
    "            total += (counter_of_class[key]/number_of_items)**2\n",
    "        \n",
    "        # * for each unique class C in y\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the size of y. This is the p(C)\n",
    "            # * add p(C)**2 to the total\n",
    "        return 1 - total\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - split_index: int (index of feature)\n",
    "            - split_value: int/float/bool/str (value of feature)\n",
    "        OUTPUT:\n",
    "            - X1: 2d numpy array (feature matrix for subset 1)\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - X2: 2d numpy array (feature matrix for subset 2)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        Return the two subsets of the dataset achieved by the given feature and\n",
    "        value to split on.\n",
    "        Call the method like this:\n",
    "        X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "        X1, y1 is a subset of the data.\n",
    "        X2, y2 is the other subset of the data.\n",
    "        '''\n",
    "        \n",
    "        new_split = X[:,split_index]\n",
    "        try:\n",
    "            if self.categorical[split_index] == True:\n",
    "                A = np.where(new_split==split_value)\n",
    "                B = np.where(new_split!=split_value)\n",
    "            else:# variable is not categorical\n",
    "                A = np.where(new_split<split_value)\n",
    "                B = np.where(new_split>=split_value)\n",
    "            #print(\"Made a sccessful split of data\")\n",
    "        except:\n",
    "\n",
    "            if self.categorical[split_index] == True:\n",
    "                A = np.where(new_split==split_value)\n",
    "                B = np.where(new_split!=split_value)\n",
    "            else:# variable is not categorical, \n",
    "               \n",
    "                A = np.where(new_split<int(split_value))\n",
    "                B = np.where(new_split>=int(split_value))\n",
    "\n",
    "        # * slice the split column from X with the split_index\n",
    "        # * if the variable of this column is categorical\n",
    "            # * select the indices of the rows in the column\n",
    "            #  with the split_value (T/F) into one set of indices (call them A)\n",
    "            # * select the indices of the rows in the column\n",
    "            # that don't have the split_value into another\n",
    "            #  set of indices (call them B)\n",
    "        # * else if the variable is not categorical\n",
    "             # * select the indices of the rows in the column\n",
    "            #  less than the split value into one set of indices (call them A)\n",
    "            # * select the indices of the rows in the column\n",
    "            #  greater or equal to  the split value into\n",
    "            # another set of indices (call them B)\n",
    "        return X[A], y[A], X[B], y[B]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the information gain of making the given split.\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "        '''\n",
    "        # * set total equal to the impurity_criterion\n",
    "        total = self.impurity_criterion(y)\n",
    "\n",
    "        len_y1 = len(y1)\n",
    "        len_y2 = len(y2)\n",
    "        \n",
    "        total -= self.impurity_criterion(y1)*len_y1/len(y) # split one\n",
    "        total -=self.impurity_criterion(y2)*len_y2/len(y)\n",
    "        \n",
    "        # * for each of the possible splits y1 and y2\n",
    "            # * calculate the impurity_criterion of the split\n",
    "            # * subtract this value from the total, multiplied by split_size/y_size\n",
    "        return total\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        # you should randomly select num_features of the potential features to consider. Only calculate and compare\n",
    "        #the features that were randomly chosen, so that the feature you choose is one of the randomly chosen features\n",
    "        # self.num_features\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - index: int (index of feature)\n",
    "            - value: int/float/bool/str (value of feature)\n",
    "            - splits: (2d array, 1d array, 2d array, 1d array)\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "        Call the method like this:\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "        X1, y1, X2, y2 = splits\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # set these initial variables to None\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        # we need to keep track of the maximum entropic gain\n",
    "        max_gain = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.num_features !=None: ## building a random forest \n",
    "        \n",
    "           \n",
    "            index_of_columns = np.arange(np.shape(X)[1])\n",
    "            index_selection = np.random.choice(index_of_columns,self.num_features)\n",
    "     \n",
    "            \n",
    "            for col_index in index_selection: ##go the col indexes that were chosen above\n",
    "                values = np.unique(X[:,col_index])\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "            # * for each column in X\n",
    "                # * set an array called values to be the\n",
    "                # unique values in that column (use np.unique)\n",
    "\n",
    "                # if there are less than 2 values, move on to the next column\n",
    "                if len(values) < 2:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # * for each value V in the values array\n",
    "                for  val_index,value in enumerate(values): # each feature\n",
    "\n",
    "                    X_yes, y_yes, X_no, y_no =self._make_split(X,y,col_index,value) \n",
    "                    #_make_split(self, X, y, split_index, split_value):\n",
    "                    \n",
    "                    #make split returns  X[A], y[A], X[B], y[B]\n",
    "\n",
    "                    # * make a temporary split (using the column index and V) with make_split\n",
    "\n",
    "                    # * calculate the information gain between the original y, y1 and y2\n",
    "                    # _information_gain(self, y, y1, y2):\n",
    "                    info_gain  = self._information_gain(y,y_yes,y_no)\n",
    "                    if info_gain>max_gain:\n",
    "                        max_gain = info_gain\n",
    "                        split_index = col_index\n",
    "                        split_value = value\n",
    "                        splits = (X_yes, y_yes, X_no, y_no)\n",
    "\n",
    "\n",
    "                    # * if this gain is greater than the max_gain\n",
    "                        # * set max_gain, split_index, and split_value to be equal\n",
    "                        # to the current max_gain, column and value\n",
    "\n",
    "                        # * set the output splits to the current split setup (X1, y1, X2, y2)\n",
    "        \n",
    "\n",
    "        \n",
    "        else: ## building a regular decision tree\n",
    "          \n",
    "            \n",
    "            for col_index,col in enumerate(X.T):\n",
    "                values = np.unique(col)\n",
    "\n",
    "            # * for each column in X\n",
    "                # * set an array called values to be the\n",
    "                # unique values in that column (use np.unique)\n",
    "\n",
    "                # if there are less than 2 values, move on to the next column\n",
    "                if len(values) < 2:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # * for each value V in the values array\n",
    "                for  val_index,value in enumerate(values): # each feature\n",
    "\n",
    "                    X_yes, y_yes, X_no, y_no =self._make_split(X,y,col_index,value) #_make_split(self, X, y, split_index, split_value):\n",
    "                    #make split returns  X[A], y[A], X[B], y[B]\n",
    "\n",
    "\n",
    "                    # * make a temporary split (using the column index and V) with make_split\n",
    "\n",
    "                    # * calculate the information gain between the original y, y1 and y2\n",
    "                    # _information_gain(self, y, y1, y2):\n",
    "                    info_gain  = self._information_gain(y,y_yes,y_no)\n",
    "                    if info_gain>max_gain:\n",
    "                        max_gain = info_gain\n",
    "                        split_index = col_index\n",
    "                        split_value = value\n",
    "                        splits = (X_yes, y_yes, X_no, y_no)\n",
    "\n",
    "\n",
    "                    # * if this gain is greater than the max_gain\n",
    "                        # * set max_gain, split_index, and split_value to be equal\n",
    "                        # to the current max_gain, column and value\n",
    "\n",
    "                        # * set the output splits to the current split setup (X1, y1, X2, y2)\n",
    "                        \n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "        OUTPUT:\n",
    "            - y: 1d numpy array\n",
    "        Return an array of predictions for the feature matrix X.\n",
    "        '''\n",
    "\n",
    "        #return np.array(map(lambda x: self.root.predict_one(x) ,X)).reshape((1,-1))\n",
    "        \n",
    "\n",
    "\n",
    "        return np.apply_along_axis(self.root.predict_one, axis=1, arr=X)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Return string representation of the Decision Tree. This will allow you to $:print tree\n",
    "        '''\n",
    "        return str(self.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.array([[1,2],\n",
    "            [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    A node class for a decision tree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - x: 1d numpy array (single data point)\n",
    "        OUTPUT:\n",
    "            - y: predicted label\n",
    "        Return the predicted label for a single data point.\n",
    "        '''\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "\n",
    "    # This is for visualizing your tree. You don't need to look into this code.\n",
    "    def as_string(self, level=0, prefix=\"\"):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - level: int (amount to indent)\n",
    "        OUTPUT:\n",
    "            - prefix: str (to start the line with)\n",
    "        Return a string representation of the tree rooted at this node.\n",
    "        '''\n",
    "        result = \"\"\n",
    "        if prefix:\n",
    "            indent = \"  |   \" * (level - 1) + \"  |-> \"\n",
    "            result += indent + prefix + \"\\n\"\n",
    "        indent = \"  |   \" * level\n",
    "        result += indent + \"  \" + str(self.name) + \"\\n\"\n",
    "        if not self.leaf:\n",
    "            if self.categorical:\n",
    "                left_key = str(self.value)\n",
    "                right_key = \"no \" + str(self.value)\n",
    "            else:\n",
    "                left_key = \"< \" + str(self.value)\n",
    "                right_key = \">= \" + str(self.value)\n",
    "            result += self.left.as_string(level + 1, left_key + \":\")\n",
    "            result += self.right.as_string(level + 1, right_key + \":\")\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.as_string().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import pandas as pd\n",
    "#from DecisionTree import DecisionTree\n",
    "\n",
    "\n",
    "def test_tree(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    y = df.pop('Result').values\n",
    "    X = df.values\n",
    "    print(X)\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    tree.fit(X, y, df.columns)\n",
    "    print(tree)\n",
    "    print\n",
    "\n",
    "    y_predict = tree.predict(X)\n",
    "    print('%26s   %10s   %10s' % (\"FEATURES\", \"ACTUAL\", \"PREDICTED\"))\n",
    "    print('%26s   %10s   %10s' % (\"----------\", \"----------\", \"----------\"))\n",
    "    for features, true, predicted in zip(X, y, y_predict):\n",
    "        print('%26s   %10s   %10s' % (str(features), str(true), str(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sunny' 85 85 False]\n",
      " ['sunny' 80 90 True]\n",
      " ['overcast' 83 78 False]\n",
      " ['rain' 70 96 False]\n",
      " ['rain' 68 80 False]\n",
      " ['rain' 65 70 True]\n",
      " ['overcast' 64 65 True]\n",
      " ['sunny' 72 95 False]\n",
      " ['sunny' 69 70 False]\n",
      " ['rain' 75 80 False]\n",
      " ['sunny' 75 70 True]\n",
      " ['overcast' 72 90 True]\n",
      " ['overcast' 81 75 False]\n",
      " ['rain' 71 80 True]]\n",
      "Outlook\n",
      "  |-> overcast:\n",
      "  |     Play\n",
      "  |-> no overcast:\n",
      "  |     Temperature\n",
      "  |     |-> < 80:\n",
      "  |     |     Temperature\n",
      "  |     |     |-> < 75:\n",
      "  |     |     |     Temperature\n",
      "  |     |     |     |-> < 71:\n",
      "  |     |     |     |     Temperature\n",
      "  |     |     |     |     |-> < 68:\n",
      "  |     |     |     |     |     Don't Play\n",
      "  |     |     |     |     |-> >= 68:\n",
      "  |     |     |     |     |     Play\n",
      "  |     |     |     |-> >= 71:\n",
      "  |     |     |     |     Don't Play\n",
      "  |     |     |-> >= 75:\n",
      "  |     |     |     Play\n",
      "  |     |-> >= 80:\n",
      "  |     |     Don't Play\n",
      "                  FEATURES       ACTUAL    PREDICTED\n",
      "                ----------   ----------   ----------\n",
      "     ['sunny' 85 85 False]   Don't Play   Don't Play\n",
      "      ['sunny' 80 90 True]   Don't Play   Don't Play\n",
      "  ['overcast' 83 78 False]         Play         Play\n",
      "      ['rain' 70 96 False]         Play         Play\n",
      "      ['rain' 68 80 False]         Play         Play\n",
      "       ['rain' 65 70 True]   Don't Play   Don't Play\n",
      "   ['overcast' 64 65 True]         Play         Play\n",
      "     ['sunny' 72 95 False]   Don't Play   Don't Play\n",
      "     ['sunny' 69 70 False]         Play         Play\n",
      "      ['rain' 75 80 False]         Play         Play\n",
      "      ['sunny' 75 70 True]         Play         Play\n",
      "   ['overcast' 72 90 True]         Play         Play\n",
      "  ['overcast' 81 75 False]         Play         Play\n",
      "       ['rain' 71 80 True]   Don't Play   Don't Play\n"
     ]
    }
   ],
   "source": [
    "test_tree('data/playgolf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nose.tools as n\n",
    "import numpy as np\n",
    "#from DecisionTree import DecisionTree as DT\n",
    "#from TreeNode import TreeNode as TN\n",
    "\n",
    "\n",
    "def test_entropy():\n",
    "    array = [1, 1, 2, 1, 2]\n",
    "    result = DecisionTree()._entropy(np.array(array))\n",
    "    actual = 0.67301\n",
    "    message = 'Entropy value for %r: Got %.2f. Should be %.2f' \\\n",
    "              % (array, result, actual)\n",
    "    n.assert_almost_equal(result, actual, 4, message)\n",
    "\n",
    "\n",
    "def test_gini():\n",
    "    array = [1, 1, 2, 1, 2]\n",
    "    result = DecisionTree()._gini(np.array(array))\n",
    "    actual = 0.48\n",
    "    message = 'Gini value for %r: Got %.2f. Should be %.2f' \\\n",
    "              % (array, result, actual)\n",
    "    n.assert_almost_equal(result, actual, 4, message)\n",
    "\n",
    "\n",
    "def fake_data():\n",
    "    X = np.array([[1, 'bat'], [2, 'cat'], [2, 'rat'], [3, 'bat'], [3, 'bat']])\n",
    "    y = np.array([1, 0, 1, 0, 1])\n",
    "    X1 = np.array([[1, 'bat'], [3, 'bat'], [3, 'bat']])\n",
    "    y1 = np.array([1, 0, 1])\n",
    "    X2 = np.array([[2, 'cat'], [2, 'rat']])\n",
    "    y2 = np.array([0, 1])\n",
    "    return X, y, X1, y1, X2, y2\n",
    "\n",
    "\n",
    "def test_make_split():\n",
    "    X, y, X1, y1, X2, y2 = fake_data()\n",
    "    split_index, split_value = 1, 'bat'\n",
    "    dt = DecisionTree()\n",
    "    dt.categorical = np.array([False, True])\n",
    "    result = dt._make_split(X, y, split_index, split_value)\n",
    "    try:\n",
    "        X1_result, y1_result, X2_result, y2_result = result\n",
    "    except ValueError:\n",
    "        n.assert_true(False, 'result not in correct form: (X1, y1, X2, y2)')\n",
    "    actual = (X1, y1, X2, y2)\n",
    "    message = '_make_split got results\\n%r\\nShould be\\n%r' % (result, actual)\n",
    "    n.ok_(np.array_equal(X1, X1_result), message)\n",
    "    n.ok_(np.array_equal(y1, y1_result), message)\n",
    "    n.ok_(np.array_equal(X2, X2_result), message)\n",
    "    n.ok_(np.array_equal(y2, y2_result), message)\n",
    "\n",
    "\n",
    "def test_information_gain():\n",
    "    X, y, X1, y1, X2, y2 = fake_data()\n",
    "    result = DecisionTree()._information_gain(y, y1, y2)\n",
    "    actual = 0.01384\n",
    "    message = 'Information gain for:\\n%r, %r, %r:\\nGot %.3f. Should be %.3f' \\\n",
    "              % (y, y1, y2, result, actual)\n",
    "    n.assert_almost_equal(result, actual, 4, message)\n",
    "\n",
    "\n",
    "def test_choose_split_index():\n",
    "    X, y, X1, y1, X2, y2 = fake_data()\n",
    "    index, value = 1, 'cat'\n",
    "    dt = DecisionTree()\n",
    "    dt.categorical = np.array([False, True])\n",
    "    result = dt._choose_split_index(X, y)\n",
    "    try:\n",
    "        split_index, split_value, splits = result\n",
    "    except ValueError:\n",
    "        message = 'result not in correct form. Should be:\\n' \\\n",
    "                  '    split_index, split_value, splits'\n",
    "        n.assert_true(False, message)\n",
    "    message = 'choose split for data:\\n%r\\n%r\\n' \\\n",
    "              'split index, split value should be: %r, %r\\n' \\\n",
    "              'not: %r, %r' \\\n",
    "              % (X, y, index, value, split_index, split_value)\n",
    "    n.eq_(split_index, index, message)\n",
    "    n.eq_(split_value, value, message)\n",
    "\n",
    "def test_predict():\n",
    "    root = TreeNode()\n",
    "    root.column = 1\n",
    "    root.name = 'column 1'\n",
    "    root.value = 'bat'\n",
    "    root.left = TreeNode()\n",
    "    root.left.leaf = True\n",
    "    root.left.name = \"one\"\n",
    "    root.right = TreeNode()\n",
    "    root.right.leaf = True\n",
    "    root.right.name = \"two\"\n",
    "    data = [10, 'cat']\n",
    "    result = root.predict_one(data)\n",
    "    actual = \"two\"\n",
    "    message = 'Predicted %r. Should be %r.\\nTree:\\n%r\\ndata:\\n%r' \\\n",
    "              % (result, actual, root, data)\n",
    "    n.eq_(result, actual, message)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_entropy()\n",
    "test_make_split()\n",
    "\n",
    "test_gini()\n",
    "test_make_split()\n",
    "test_information_gain()\n",
    "test_choose_split_index()\n",
    "test_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build a Random Forest\n",
    "\n",
    "You will be using our implementation of Decision Trees to implement a Random Forest.\n",
    "\n",
    "You can use the `DecisionTree` class from `DecisionTree.py` with the following code:\n",
    "\n",
    "```python\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, y_train)\n",
    "predicted_y = dt.predict(X_test)\n",
    "```\n",
    "\n",
    "You can also visualize a Decision Tree by printing it. This may be helpful for understanding your Random Forest.\n",
    "\n",
    "```python\n",
    "print dt\n",
    "```\n",
    "\n",
    "While you're getting your code to work, use the play golf data set that we used for implementing Decision Trees.\n",
    "\n",
    "There's a file called `RandomForest.py` which contains a skeleton of the code. Your goal is to fill it in so that you can run it with the following lines of code:\n",
    "\n",
    "```python\n",
    "from RandomForest import RandomForest\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/playgolf.csv')\n",
    "y = df.pop('Result').values\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "rf = RandomForest(num_trees=10, num_features=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_predict = rf.predict(X_test)\n",
    "print \"score:\", rf.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "### A. Implement *Tree Bagging*\n",
    "\n",
    "Bagging, or *bootstrap aggregating*, is taking several random samples *with replacement* from the data set and building a model for each sample. Each of these models gets a vote on the prediction.\n",
    "\n",
    "Sampling with replacement means that we can repeat data points. In the basic random forest, we will always use a sample size that is the same as the size of the original data set. Many data points will not be included in each sample and many will be repeated.\n",
    "\n",
    "1. Implement the `build_forest` method. For right now, we will be ignoring the `num_features` parameter. Here is the pseudocode:\n",
    "\n",
    "      Repeat num_trees times:\n",
    "          Create a random sample of the data with replacement\n",
    "          Build a decision tree with that sample\n",
    "      Return the list of the decision trees created\n",
    "\n",
    "\n",
    "### B. Implement random feature selection\n",
    "\n",
    "1. Modify the `DecisionTree` class so that it takes an additional parameter: `num_features`. This is the number of features to consider at each node in choosing the best split. Which features to consider is randomly chosen at each node. You will need to modify the `__init__`, method to take a `num_features` parameter. In `_choose_split_index`, you should randomly select `num_features` of the potential features to consider. Only calculate and compare the features that were randomly chosen, so that the feature you choose is one of the randomly chosen features.\n",
    "\n",
    "2. Modify `build_forest` in your `RandomForest` class to pass the `num_features` parameter to the Decision Trees.\n",
    "\n",
    "\n",
    "### C. Implement classification and scoring\n",
    "\n",
    "1. In the `predict` method, you should have each Decision Tree classify each data point. Choose the label with the majority of trees. Break ties by choosing one of the labels arbitrarily.\n",
    "\n",
    "2. In the `score` method, you should first classify the data points and count the percent of them which match the given labels.\n",
    "\n",
    "\n",
    "### D. Try a bigger data set\n",
    "\n",
    "You won't be able to get great results cross validating with the play golf data set since it's so small. In the data folder, there's a dataset called 'congressional_voting.csv'. This contains congressman, how they voted on different issues and their party.\n",
    "\n",
    "Here are what the 17 columns refer to:\n",
    "\n",
    "* Class Name: 2 (democrat, republican)\n",
    "* handicapped-infants: 2 (y,n)\n",
    "* water-project-cost-sharing: 2 (y,n)\n",
    "* adoption-of-the-budget-resolution: 2 (y,n)\n",
    "* physician-fee-freeze: 2 (y,n)\n",
    "* el-salvador-aid: 2 (y,n)\n",
    "* religious-groups-in-schools: 2 (y,n)\n",
    "* anti-satellite-test-ban: 2 (y,n)\n",
    "* aid-to-nicaraguan-contras: 2 (y,n)\n",
    "* mx-missile: 2 (y,n)\n",
    "* immigration: 2 (y,n)\n",
    "* synfuels-corporation-cutback: 2 (y,n)\n",
    "* education-spending: 2 (y,n)\n",
    "* superfund-right-to-sue: 2 (y,n)\n",
    "* crime: 2 (y,n)\n",
    "* duty-free-exports: 2 (y,n)\n",
    "* export-administration-act-south-africa: 2 (y,n)\n",
    "\n",
    "The dataset came from UCI [here](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records).\n",
    "\n",
    "1. Based on the votes on the 16 issues, predict the party using your implementation of Random Forest. Start with 10 trees and a maximum of 5 features.\n",
    "\n",
    "2. Compare how well the Random Forest does versus the Decision Tree.\n",
    "\n",
    "3. Try modifying the number of trees and see how it affects your accuracy.\n",
    "\n",
    "4. Calculate the accuracy for each of your decision trees on the test set and compare it to the accuracy of the random forest on the test set.\n",
    "\n",
    "5. Predict how the congressmen will vote on a particular issue given the remaining columns.\n",
    "\n",
    "\n",
    "### Extra Credit: out-of-bag error and feature importance\n",
    "\n",
    "1. Out-of-bag error is a clever way of validating your model by testing individual trees based on samples that weren't including in their training set. It is described in the lecture notes, [Applied Data Science](http://columbia-applied-data-science.github.io/appdatasci.pdf) (9.4.3) and [Breiman's notes](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr).\n",
    "\n",
    "2. Feature importance is a way of determining which features contribute the most to being able to predict the result. It is discussed in the lecture notes and [Breiman's notes](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp). You can compare what features you get with Breiman's method vs [sklearn](http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def test_tree(filename):\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    y = df.pop('Result').values\n",
    "    X = df.values\n",
    "    print(X)\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    tree.fit(X, y, df.columns)\n",
    "    print(tree)\n",
    "    print\n",
    "\n",
    "    y_predict = tree.predict(X)\n",
    "    print('%26s   %10s   %10s' % (\"FEATURES\", \"ACTUAL\", \"PREDICTED\"))\n",
    "    print('%26s   %10s   %10s' % (\"----------\", \"----------\", \"----------\"))\n",
    "    for features, true, predicted in zip(X, y, y_predict):\n",
    "        print('%26s   %10s   %10s' % (str(features), str(true), str(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.arange(10),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from DecisionTree import DecisionTree\n",
    "\n",
    "class RandomForest(object):\n",
    "    '''A Random Forest class'''\n",
    "\n",
    "    def __init__(self, num_trees, num_features):\n",
    "        '''\n",
    "           num_trees:  number of trees to create in the forest:\n",
    "        num_features:  the number of features to consider when choosing the\n",
    "                           best split for each node of the decision trees\n",
    "        '''\n",
    "        self.num_trees = num_trees\n",
    "        self.num_features = num_features\n",
    "        self.forest = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X:  two dimensional numpy array representing feature matrix\n",
    "                for test data\n",
    "        y:  numpy array representing labels for test data\n",
    "        '''\n",
    "        self.forest = self.build_forest(X, y, X.shape[0])\n",
    "\n",
    "    def build_forest(self, X, y, num_samples): #num_samples is the number of rows\n",
    "\n",
    "        # * Return a list of num_trees DecisionTrees.\n",
    "#         index_of_columns=np.arange(np.shape(X)[1]) # get the number of columns\n",
    "        \n",
    "#         subset_selection = np.random.choice(index_of_columns,num_features)\n",
    "\n",
    "        trees=[]\n",
    "        list_of_indexes =np.arange(np.shape(X)[0])\n",
    "        \n",
    "        for tree_num in range(self.num_trees):\n",
    "            \n",
    "            indexes= np.random.choice(list_of_indexes,num_samples)\n",
    "            \n",
    "            tree = DecisionTree(number_features=self.num_features)\n",
    "            \n",
    "            tree.fit(X[indexes],y[indexes])\n",
    "            trees.append(tree)\n",
    "            #print(tree,'__________________')\n",
    "            \n",
    "            \n",
    "            \n",
    "        return trees\n",
    "\n",
    "\n",
    "        #In the predict method, you should have each Decision Tree classify each data \n",
    "        #point. Choose the label with the majority of trees. Break ties by choosing one of the labels arbitrarily.\n",
    "#In the score method, you should first classify the data \n",
    "#points and count the percent of them which match the given labels.\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        '''\n",
    "        Return a numpy array of the labels predicted for the given test data.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        results={} # this is now a list of lists\n",
    "        tree_results=[] # prediction from each tree\n",
    "        final_results=[] # this is from counter most common\n",
    "        \n",
    "        for tree_count,tree in enumerate(self.forest):\n",
    "            tree_results.append(tree.predict(X)) # this is a list\n",
    "\n",
    "            for prediction in range(np.shape(X)[0]):\n",
    "                if prediction in results:\n",
    "\n",
    "\n",
    "                    #print(final_results[predic])\n",
    "                    results[prediction].append(tree_results[tree_count][prediction])\n",
    "\n",
    "                else:\n",
    "                    results[prediction]=[tree_results[tree_count][prediction]]\n",
    "\n",
    "        \n",
    "        for k,v in results.items():\n",
    "            final_results.append(Counter(v).most_common()[0][0]) # return the value of the most common index\n",
    "            \n",
    "        # * Each one of the trees is allowed to predict on the same row of input data. The majority vote\n",
    "        # is the output of the whole forest. This becomes a single prediction.\n",
    "        #print(results,'results')\n",
    "        return final_results\n",
    "\n",
    "    def score(self, X, y):\n",
    "\n",
    "        '''\n",
    "        Return the accuracy of the Random Forest for the given test data.\n",
    "        '''\n",
    "\n",
    "        # * In this case you simply compute the accuracy formula as we have defined in class. Compare predicted y to\n",
    "        # the actual input y.\n",
    "\n",
    "        return sum(self.predict(X)==y)/len(y)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the tree\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/playgolf.csv')\n",
    "y = df.pop('Result').values\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(9,2)\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "#dt.build_forest(X_train,y_train,5)\n",
    "dt.predict(X)\n",
    "dt.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = [\"Don't Play\", \"Play\",\"Don't Play\", \"Don't Play\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't Play\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(r).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/playgolf.csv')\n",
    "y = df.pop('Result').values\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(500,3) ## 500 tree , 3 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "dt.predict(X)\n",
    "dt.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We see accuracy of ~85% 500 trees in our random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Dataset -Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['class_name',\\\n",
    "   'handicapped-infants',\\\n",
    "   'water-project-cost-sharing',\\\n",
    "   'adoption-of-the-budget-resolution',\\\n",
    "   'physician-fee-freeze',\\\n",
    "   'el-salvador-aid',\\\n",
    "   'religious-groups-in-schools',\\\n",
    "   'anti-satellite-test-ban',\\\n",
    "   'aid-to-nicaraguan-contras',\\\n",
    "  'mx-missile',\\\n",
    "  'immigration',\\\n",
    "  'synfuels-corporation-cutback',\\\n",
    "  'education-spending',\\\n",
    "  'superfund-right-to-sue',\\\n",
    "  'crime',\\\n",
    "  'duty-free-exports',\\\n",
    "  'export-administration-act-south-africa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voting_records = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\\\n",
    "    ',header=None,names=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 435 entries, 0 to 434\n",
      "Data columns (total 17 columns):\n",
      "class_name                                435 non-null object\n",
      "handicapped-infants                       435 non-null object\n",
      "water-project-cost-sharing                435 non-null object\n",
      "adoption-of-the-budget-resolution         435 non-null object\n",
      "physician-fee-freeze                      435 non-null object\n",
      "el-salvador-aid                           435 non-null object\n",
      "religious-groups-in-schools               435 non-null object\n",
      "anti-satellite-test-ban                   435 non-null object\n",
      "aid-to-nicaraguan-contras                 435 non-null object\n",
      "mx-missile                                435 non-null object\n",
      "immigration                               435 non-null object\n",
      "synfuels-corporation-cutback              435 non-null object\n",
      "education-spending                        435 non-null object\n",
      "superfund-right-to-sue                    435 non-null object\n",
      "crime                                     435 non-null object\n",
      "duty-free-exports                         435 non-null object\n",
      "export-administration-act-south-africa    435 non-null object\n",
      "dtypes: object(17)\n",
      "memory usage: 57.9+ KB\n"
     ]
    }
   ],
   "source": [
    "voting_records.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the ? marks in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class_name', 'handicapped-infants', 'water-project-cost-sharing',\n",
       "       'adoption-of-the-budget-resolution', 'physician-fee-freeze',\n",
       "       'el-salvador-aid', 'religious-groups-in-schools',\n",
       "       'anti-satellite-test-ban', 'aid-to-nicaraguan-contras', 'mx-missile',\n",
       "       'immigration', 'synfuels-corporation-cutback', 'education-spending',\n",
       "       'superfund-right-to-sue', 'crime', 'duty-free-exports',\n",
       "       'export-administration-act-south-africa'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_records.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = voting_records[~((voting_records['class_name'] == '?' ) | \\\n",
    "                       (voting_records['handicapped-infants'] == '?' ) | (voting_records['water-project-cost-sharing'] == '?' ) |\\\n",
    "                      (voting_records['adoption-of-the-budget-resolution'] == '?' ) | (voting_records['physician-fee-freeze'] == '?' ) |\\\n",
    "                      (voting_records['el-salvador-aid'] == '?' ) | (voting_records['religious-groups-in-schools'] == '?' ) |\\\n",
    "                      (voting_records['anti-satellite-test-ban'] == '?' ) | (voting_records['aid-to-nicaraguan-contras'] == '?' ) | \\\n",
    "                     (voting_records['mx-missile'] == '?' ) | (voting_records['immigration'] == '?' ) | \\\n",
    "                      (voting_records['synfuels-corporation-cutback'] == '?' ) | (voting_records['education-spending'] == '?' ) |  \\\n",
    "                      (voting_records['superfund-right-to-sue'] == '?' ) | (voting_records['crime'] == '?' ) | \\\n",
    "                      (voting_records['duty-free-exports'] == '?' ) | (voting_records['export-administration-act-south-africa'] == '?' ))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 232 entries, 5 to 431\n",
      "Data columns (total 17 columns):\n",
      "class_name                                232 non-null object\n",
      "handicapped-infants                       232 non-null object\n",
      "water-project-cost-sharing                232 non-null object\n",
      "adoption-of-the-budget-resolution         232 non-null object\n",
      "physician-fee-freeze                      232 non-null object\n",
      "el-salvador-aid                           232 non-null object\n",
      "religious-groups-in-schools               232 non-null object\n",
      "anti-satellite-test-ban                   232 non-null object\n",
      "aid-to-nicaraguan-contras                 232 non-null object\n",
      "mx-missile                                232 non-null object\n",
      "immigration                               232 non-null object\n",
      "synfuels-corporation-cutback              232 non-null object\n",
      "education-spending                        232 non-null object\n",
      "superfund-right-to-sue                    232 non-null object\n",
      "crime                                     232 non-null object\n",
      "duty-free-exports                         232 non-null object\n",
      "export-administration-act-south-africa    232 non-null object\n",
      "dtypes: object(17)\n",
      "memory usage: 32.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset came from UCI here.\n",
    "Based on the votes on the 16 issues, predict the party using your implementation of Random Forest. Start with 10 trees and a maximum of 5 features.\n",
    "Compare how well the Random Forest does versus the Decision Tree.\n",
    "Try modifying the number of trees and see how it affects your accuracy.\n",
    "Calculate the accuracy for each of your decision trees on the test set and compare it to the accuracy of the random forest on the test set.\n",
    "Predict how the congressmen will vote on a particular issue given the remaining columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test on the original data frame with ? marks, and the reduced DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(voting_records.iloc[:,1:])\n",
    "y=np.array(voting_records.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93577981651376152"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(10,5) ## 500 tree , 3 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "dt.predict(X_test)\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64220183486238536"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increase the number of trees\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(50,9) ## 50 tree , 9 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "dt.predict(X_test)\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We achieve an 56% accuracy with the original data frame (including the ? marks) to predict the party of the senator.\n",
    "- Next, try the reduced DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_red = np.array(df1.iloc[:,1:])\n",
    "y_red=np.array(df1.iloc[:,0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_red, y_red)\n",
    "\n",
    "dt = RandomForest(10,5) ## 10 tree , 5 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "dt.predict(X_test)\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we remove the question marks, our accuracy increases to over 91%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98275862068965514"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more trees on the reduced df and more features\n",
    "\n",
    "\n",
    "X_red = np.array(df1.iloc[:,1:])\n",
    "y_red=np.array(df1.iloc[:,0]) #predict the party\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_red, y_red)\n",
    "\n",
    "dt = RandomForest(50,9) ## 50 tree , 9 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "dt.predict(X_test)\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More trees and more features slightly increases our accuracy (depending on when you run the cells this value might change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict how the congressmen will vote on a particular issue given the remaining columns.\n",
    "- change the target classification to predict how people will vote on the last four columns (superfund-right-to-sue                    232 non-null object\n",
    "crime                                     232 non-null object\n",
    "duty-free-exports                         232 non-null object\n",
    "export-administration-act-south-africa )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>handicapped-infants</th>\n",
       "      <th>water-project-cost-sharing</th>\n",
       "      <th>adoption-of-the-budget-resolution</th>\n",
       "      <th>physician-fee-freeze</th>\n",
       "      <th>el-salvador-aid</th>\n",
       "      <th>religious-groups-in-schools</th>\n",
       "      <th>anti-satellite-test-ban</th>\n",
       "      <th>aid-to-nicaraguan-contras</th>\n",
       "      <th>mx-missile</th>\n",
       "      <th>immigration</th>\n",
       "      <th>synfuels-corporation-cutback</th>\n",
       "      <th>education-spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>democrat</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>?</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_name handicapped-infants water-project-cost-sharing  \\\n",
       "0  republican                   n                          y   \n",
       "1  republican                   n                          y   \n",
       "2    democrat                   ?                          y   \n",
       "3    democrat                   n                          y   \n",
       "4    democrat                   y                          y   \n",
       "\n",
       "  adoption-of-the-budget-resolution physician-fee-freeze el-salvador-aid  \\\n",
       "0                                 n                    y               y   \n",
       "1                                 n                    y               y   \n",
       "2                                 y                    ?               y   \n",
       "3                                 y                    n               ?   \n",
       "4                                 y                    n               y   \n",
       "\n",
       "  religious-groups-in-schools anti-satellite-test-ban  \\\n",
       "0                           y                       n   \n",
       "1                           y                       n   \n",
       "2                           y                       n   \n",
       "3                           y                       n   \n",
       "4                           y                       n   \n",
       "\n",
       "  aid-to-nicaraguan-contras mx-missile immigration  \\\n",
       "0                         n          n           y   \n",
       "1                         n          n           n   \n",
       "2                         n          n           n   \n",
       "3                         n          n           n   \n",
       "4                         n          n           n   \n",
       "\n",
       "  synfuels-corporation-cutback education-spending  \n",
       "0                            ?                  y  \n",
       "1                            n                  y  \n",
       "2                            y                  n  \n",
       "3                            y                  n  \n",
       "4                            y                  ?  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_records.iloc[:,:-4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vote, Actual Vote\n",
      "[('y', '?'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('n', 'y'), ('y', 'y'), ('y', '?'), ('y', '?'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('?', '?'), ('n', 'y'), ('?', 'y'), ('y', 'n'), ('y', '?'), ('y', '?'), ('n', '?'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', '?'), ('y', 'y'), ('y', '?'), ('n', 'n'), ('y', '?'), ('y', '?'), ('y', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('n', 'y'), ('?', 'y'), ('y', 'n'), ('y', 'y'), ('n', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('n', 'n'), ('y', '?'), ('n', '?'), ('y', 'y'), ('y', 'y'), ('n', '?'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', '?'), ('y', '?'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('?', '?'), ('y', '?'), ('n', 'n'), ('n', '?'), ('y', 'n'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('y', 'n'), ('?', '?'), ('y', '?'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', '?'), ('y', 'y'), ('y', 'y'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', '?'), ('y', '?'), ('n', 'n'), ('y', '?'), ('n', 'y'), ('y', 'y'), ('y', '?'), ('y', 'y')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59633027522935778"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(voting_records.iloc[:,:-3]) ## not including last three\n",
    "y = np.array(voting_records.iloc[:,16]) # predict export-administration-act-south-africa\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(200,10) ## 200 tree , 10 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "print('Predicted vote, Actual Vote')\n",
    "print([(pred_vot,actual_vote) for pred_vot,actual_vote in zip(dt.predict(X_test),y_test)])\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only ~58% chance of predicting the ourcomt of export administration act in south america.\n",
    "- Next, predict duty-free-exports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vote, Actual Vote\n",
      "[('n', 'y'), ('y', 'n'), ('n', 'n'), ('y', 'n'), ('n', '?'), ('n', 'n'), ('n', 'y'), ('n', 'n'), ('y', 'n'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'y'), ('y', 'n'), ('n', 'y'), ('y', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'n'), ('n', 'n'), ('n', 'n'), ('n', '?'), ('n', 'n'), ('n', '?'), ('y', '?'), ('n', 'n'), ('n', '?'), ('y', '?'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'n'), ('n', 'n'), ('?', '?'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', '?'), ('y', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'n'), ('n', 'n'), ('y', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', '?'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('y', '?'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'n'), ('n', 'n'), ('y', '?'), ('n', 'n'), ('y', '?')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68807339449541283"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(voting_records.iloc[:,:-3]) ## not including last three\n",
    "y = np.array(voting_records.iloc[:,15]) # predict duty free exports\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(200,10) ## 200 tree s, 10 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "print('Predicted vote, Actual Vote')\n",
    "print([(pred_vot,actual_vote) for pred_vot,actual_vote in zip(dt.predict(X_test),y_test)])\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slightly easier to predict duty free exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vote, Actual Vote\n",
      "[('y', 'y'), ('y', 'y'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'y'), ('n', 'y'), ('y', '?'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'n'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('?', '?'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', '?'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('?', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'y')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84403669724770647"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(voting_records.iloc[:,:-3]) ## not including last three\n",
    "y = np.array(voting_records.iloc[:,14]) # predict crime\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(200,3) ## 200 trees , 3 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "print('Predicted vote, Actual Vote')\n",
    "print([(pred_vot,actual_vote) for pred_vot,actual_vote in zip(dt.predict(X_test),y_test)])\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This easiest thing to predict crime with ~80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vote, Actual Vote\n",
      "[('y', 'n'), ('n', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'y'), ('n', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82758620689655171"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test crime with only yes or no options available in the data (not ?)\n",
    "\n",
    "X = np.array(df1.iloc[:,:-3]) ## not including last three\n",
    "y = np.array(df1.iloc[:,14]) # predict crime\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(200,3) ## 200 trees , 3 features\n",
    "dt.fit(X_train, y_train)\n",
    "#predicted_y = dt.predict(X_test)\n",
    "print('Predicted vote, Actual Vote')\n",
    "print([(pred_vot,actual_vote) for pred_vot,actual_vote in zip(dt.predict(X_test),y_test)])\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, if there are fewer options to predict (two versus three) the prediction accuracy increases (usually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physician-fee-freeze\n",
      "  |-> n:\n",
      "  |     aid-to-nicaraguan-contras\n",
      "  |     |-> n:\n",
      "  |     |     superfund-right-to-sue\n",
      "  |     |     |-> n:\n",
      "  |     |     |     adoption-of-the-budget-resolution\n",
      "  |     |     |     |-> n:\n",
      "  |     |     |     |     y\n",
      "  |     |     |     |-> no n:\n",
      "  |     |     |     |     el-salvador-aid\n",
      "  |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     n\n",
      "  |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     anti-satellite-test-ban\n",
      "  |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     y\n",
      "  |     |     |-> no n:\n",
      "  |     |     |     y\n",
      "  |     |-> no n:\n",
      "  |     |     adoption-of-the-budget-resolution\n",
      "  |     |     |-> y:\n",
      "  |     |     |     religious-groups-in-schools\n",
      "  |     |     |     |-> n:\n",
      "  |     |     |     |     water-project-cost-sharing\n",
      "  |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     immigration\n",
      "  |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     handicapped-infants\n",
      "  |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     synfuels-corporation-cutback\n",
      "  |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     anti-satellite-test-ban\n",
      "  |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     superfund-right-to-sue\n",
      "  |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     handicapped-infants\n",
      "  |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     synfuels-corporation-cutback\n",
      "  |     |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     synfuels-corporation-cutback\n",
      "  |     |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     n\n",
      "  |     |     |     |-> no n:\n",
      "  |     |     |     |     el-salvador-aid\n",
      "  |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     education-spending\n",
      "  |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     mx-missile\n",
      "  |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     handicapped-infants\n",
      "  |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     synfuels-corporation-cutback\n",
      "  |     |     |     |     |     |     |     |     |-> y:\n",
      "  |     |     |     |     |     |     |     |     |     anti-satellite-test-ban\n",
      "  |     |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     |     immigration\n",
      "  |     |     |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     |     |     y\n",
      "  |     |     |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |-> no y:\n",
      "  |     |     |     |     |     |     |     |     |     immigration\n",
      "  |     |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     |     y\n",
      "  |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     synfuels-corporation-cutback\n",
      "  |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     water-project-cost-sharing\n",
      "  |     |     |     |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     |     |     |     y\n",
      "  |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     n\n",
      "  |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     n\n",
      "  |     |     |-> no y:\n",
      "  |     |     |     water-project-cost-sharing\n",
      "  |     |     |     |-> n:\n",
      "  |     |     |     |     handicapped-infants\n",
      "  |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     y\n",
      "  |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     class_name\n",
      "  |     |     |     |     |     |-> democrat:\n",
      "  |     |     |     |     |     |     religious-groups-in-schools\n",
      "  |     |     |     |     |     |     |-> n:\n",
      "  |     |     |     |     |     |     |     n\n",
      "  |     |     |     |     |     |     |-> no n:\n",
      "  |     |     |     |     |     |     |     y\n",
      "  |     |     |     |     |     |-> no democrat:\n",
      "  |     |     |     |     |     |     y\n",
      "  |     |     |     |-> no n:\n",
      "  |     |     |     |     n\n",
      "  |-> no n:\n",
      "  |     education-spending\n",
      "  |     |-> n:\n",
      "  |     |     synfuels-corporation-cutback\n",
      "  |     |     |-> n:\n",
      "  |     |     |     y\n",
      "  |     |     |-> no n:\n",
      "  |     |     |     adoption-of-the-budget-resolution\n",
      "  |     |     |     |-> n:\n",
      "  |     |     |     |     n\n",
      "  |     |     |     |-> no n:\n",
      "  |     |     |     |     y\n",
      "  |     |-> no n:\n",
      "  |     |     y\n",
      "Predicted vote, Actual Vote\n",
      "[('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('n', 'n'), ('y', 'n'), ('y', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'n'), ('y', 'y'), ('n', 'n'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('y', 'y'), ('y', 'y'), ('n', 'n'), ('n', 'y'), ('n', 'n'), ('y', 'y'), ('n', 'n')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86206896551724133"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test crime with only yes or no options available in the data (not ?), print out the tree\n",
    "\n",
    "X = np.array(df1.iloc[:,:-3]) ## not including last three\n",
    "y = np.array(df1.iloc[:,14]) # predict crime\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "dt = RandomForest(5,10) ## 5 trees , 10 features\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "tree = DecisionTree()\n",
    "tree.fit(X_train, y_train, df1.columns[:-3])\n",
    "print(tree)\n",
    "\n",
    "    \n",
    "#predicted_y = dt.predict(X_test)\n",
    "print('Predicted vote, Actual Vote')\n",
    "print([(pred_vot,actual_vote) for pred_vot,actual_vote in zip(dt.predict(X_test),y_test)])\n",
    "dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
