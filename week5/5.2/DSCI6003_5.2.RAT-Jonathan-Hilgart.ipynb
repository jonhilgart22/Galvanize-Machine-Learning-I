{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RAT 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a description of Decision Trees\n",
    "- Describe use cases, weaknesses and strengths of Decision Trees\n",
    "- Describe the similarities, differences and use of Gini index and Entropy in the Construction of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 1) Decision Trees are a method to create predictions from a data set. They are useful because the shape is very flexible, so you do not need to know what the distribution of the data looks like before analyzing it. These trees are a flowchart like structure that help you arrive at a classification from some data. At each node in the tree, you decide which path to taken based the rule there (i.e. >5 or <=5). The ultimate goal is to split the data set into unique, distinct, clusters to predict new data points into.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2) Decision Trees are very flexible and can accomodate all data distributions.\n",
    "\n",
    "- Plus: In addition they are very easy to understand and explain to stakeholders.\n",
    "- Plus: Relatively easy to calculate\n",
    "- Negative: If you are doing k-folds validation, you will receive a different treee every time (due to how classes are split), which can make it more difficult to combine models.\n",
    "- Negative: It is easy to overfit trees, need to \"prune them\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> )3\n",
    "\n",
    "- Gini Index: This index attempts to look at the probability of labeling an item correctly according to the probability of labels from the current node.\n",
    "\n",
    "- Entropy: information gain looks at the probability of a particular class times the log of that same probability.\n",
    "\n",
    "- They are both used to decide how  to split our features for each node in a decision tree. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
