{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAT 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a description of the Random Forest algorithm\n",
    "- Describe use cases, weaknesses and strengths of Random Forest\n",
    "- How are the Decision Trees in Random Forests different from standard Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Random Forest is a technique that creates an ensemble of trees. At each node in the tree, you randomly select a set of the features (i.e. select three features). In addition, at the beginning of the tree you randomlly select random rows to build the tree of of from each node. This technique is used for both categorical and numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use cases: Random Forest is a very powerful algorithm because it combines multiple decision trees in an ensemble, and then has each tree in the forest vote. The answer with the greates number of \"votes\" for categorical variables, or an average of the predicted values for regression, is used as the answer. Random forest as very flexible. You should use it whenever you do not know the structure of your data beforehand. \n",
    "\n",
    "- Weaknesses: They are considered a black box because you can not trace the exact logic that the random forest took to arrive at the answer. These random forest are also slow to predict answer due to the number of trees the data must be run through. Also, there is a danger of overfitting with Random Forests.\n",
    "\n",
    "- Strengths: Very flexible, generally recieves very strong accuracy and RMSE ranting. Can estimate what variables are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Standard  Decision trees go through each variables and creates a fake split. Then, the entropy across every rows is calculated from the resulting child nodes. This is completed across every varibles until entropy is minimized. In contrast, Random Forests select a random set of feataures, and for each feature selects a random set of rows to create a decision tree with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
