{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Auto reload will automatically reload your .py file so you do not have to keep\n",
    "# importing it.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './rf_practicum')\n",
    "from RandomForest import RandomForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 Practicum I: Random Forests\n",
    "\n",
    "Your study of tree classifiers begins with random forests. \n",
    "\n",
    "## Implement Decision Trees\n",
    "\n",
    "In order to build a random forest you must first master building decision trees.\n",
    "\n",
    "1. If you have not yet completed working code for decision trees, start with getting a complete implementation using the annotated code stub DecisionTree.py and TreeNode.py provided to you in the /code directory. \n",
    "\n",
    "2. Use the run_decision_tree.py and test_decision_tree.py code stubs (with the command line) to ensure that your construction is correct. Use pycharm or sublime for a develop environment.\n",
    "\n",
    "3. Once your tree is capable of producing correct results, continue with the RandomForest.py stub, discussed below.\n",
    "\n",
    "4. You can check your performance of both the forest and trees against the setup of the executable in the practicum directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build a Random Forest\n",
    "\n",
    "You will be using our implementation of Decision Trees to implement a Random Forest.\n",
    "\n",
    "You can use the `DecisionTree` class from `DecisionTree.py` with the following code:\n",
    "\n",
    "```python\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, y_train)\n",
    "predicted_y = dt.predict(X_test)\n",
    "```\n",
    "\n",
    "You can also visualize a Decision Tree by printing it. This may be helpful for understanding your Random Forest.\n",
    "\n",
    "```python\n",
    "print dt\n",
    "```\n",
    "\n",
    "While you're getting your code to work, use the play golf data set that we used for implementing Decision Trees.\n",
    "\n",
    "There's a file called `RandomForest.py` which contains a skeleton of the code. Your goal is to fill it in so that you can run it with the following lines of code:\n",
    "\n",
    "```python\n",
    "from RandomForest import RandomForest\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/playgolf.csv')\n",
    "y = df.pop('Result').values\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "rf = RandomForest(num_trees=10, num_features=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_predict = rf.predict(X_test)\n",
    "print \"score:\", rf.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "### A. Implement *Tree Bagging*\n",
    "\n",
    "Bagging, or *bootstrap aggregating*, is taking several random samples *with replacement* from the data set and building a model for each sample. Each of these models gets a vote on the prediction.\n",
    "\n",
    "Sampling with replacement means that we can repeat data points. In the basic random forest, we will always use a sample size that is the same as the size of the original data set. Many data points will not be included in each sample and many will be repeated.\n",
    "\n",
    "1. Implement the `build_forest` method. For right now, we will be ignoring the `num_features` parameter. Here is the pseudocode:\n",
    "\n",
    "      Repeat num_trees times:\n",
    "          Create a random sample of the data with replacement\n",
    "          Build a decision tree with that sample\n",
    "      Return the list of the decision trees created\n",
    "\n",
    "\n",
    "### B. Implement random feature selection\n",
    "\n",
    "1. Modify the `DecisionTree` class so that it takes an additional parameter: `num_features`. This is the number of features to consider at each node in choosing the best split. Which features to consider is randomly chosen at each node. You will need to modify the `__init__`, method to take a `num_features` parameter. In `_choose_split_index`, you should randomly select `num_features` of the potential features to consider. Only calculate and compare the features that were randomly chosen, so that the feature you choose is one of the randomly chosen features.\n",
    "\n",
    "2. Modify `build_forest` in your `RandomForest` class to pass the `num_features` parameter to the Decision Trees.\n",
    "\n",
    "\n",
    "### C. Implement classification and scoring\n",
    "\n",
    "1. In the `predict` method, you should have each Decision Tree classify each data point. Choose the label with the majority of trees. Break ties by choosing one of the labels arbitrarily.\n",
    "\n",
    "2. In the `score` method, you should first classify the data points and count the percent of them which match the given labels.\n",
    "\n",
    "\n",
    "### D. Try a bigger data set\n",
    "\n",
    "You won't be able to get great results cross validating with the play golf data set since it's so small. In the data folder, there's a dataset called 'congressional_voting.csv'. This contains congressman, how they voted on different issues and their party.\n",
    "\n",
    "Here are what the 17 columns refer to:\n",
    "\n",
    "* Class Name: 2 (democrat, republican)\n",
    "* handicapped-infants: 2 (y,n)\n",
    "* water-project-cost-sharing: 2 (y,n)\n",
    "* adoption-of-the-budget-resolution: 2 (y,n)\n",
    "* physician-fee-freeze: 2 (y,n)\n",
    "* el-salvador-aid: 2 (y,n)\n",
    "* religious-groups-in-schools: 2 (y,n)\n",
    "* anti-satellite-test-ban: 2 (y,n)\n",
    "* aid-to-nicaraguan-contras: 2 (y,n)\n",
    "* mx-missile: 2 (y,n)\n",
    "* immigration: 2 (y,n)\n",
    "* synfuels-corporation-cutback: 2 (y,n)\n",
    "* education-spending: 2 (y,n)\n",
    "* superfund-right-to-sue: 2 (y,n)\n",
    "* crime: 2 (y,n)\n",
    "* duty-free-exports: 2 (y,n)\n",
    "* export-administration-act-south-africa: 2 (y,n)\n",
    "\n",
    "The dataset came from UCI [here](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records).\n",
    "\n",
    "1. Based on the votes on the 16 issues, predict the party using your implementation of Random Forest. Start with 10 trees and a maximum of 5 features.\n",
    "\n",
    "2. Compare how well the Random Forest does versus the Decision Tree.\n",
    "\n",
    "3. Try modifying the number of trees and see how it affects your accuracy.\n",
    "\n",
    "4. Calculate the accuracy for each of your decision trees on the test set and compare it to the accuracy of the random forest on the test set.\n",
    "\n",
    "5. Predict how the congressmen will vote on a particular issue given the remaining columns.\n",
    "\n",
    "\n",
    "### Extra Credit: out-of-bag error and feature importance\n",
    "\n",
    "1. Out-of-bag error is a clever way of validating your model by testing individual trees based on samples that weren't including in their training set. It is described in the lecture notes, [Applied Data Science](http://columbia-applied-data-science.github.io/appdatasci.pdf) (9.4.3) and [Breiman's notes](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr).\n",
    "\n",
    "2. Feature importance is a way of determining which features contribute the most to being able to predict the result. It is discussed in the lecture notes and [Breiman's notes](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp). You can compare what features you get with Breiman's method vs [sklearn](http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Play' 'Play' 'Play' 'Play']\n",
      "score: 0.5\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/playgolf.csv')\n",
    "y = df.pop('Result').values\n",
    "X = df.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "rf = RandomForest(num_trees=10, num_features=3)\n",
    "rf.fit(X_train, y_train)\n",
    "y_predict = rf.predict(X_test)\n",
    "print(y_predict)\n",
    "print(\"score:\", rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "votes = pd.read_csv('./data/congressional_voting.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_labels(value):\n",
    "    if value == 'y':\n",
    "        return 1\n",
    "    elif value == 'n':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "y = votes.pop(0).values\n",
    "y = np.array([1 if z == 'democrat' else 0 for z in y])\n",
    "votes = votes.applymap(fix_labels)\n",
    "X = votes.values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Statistics | Accuracy: 0.993| Precision: 0.998 | Recall: 0.990\n",
      "Test Data Statistics | Accuracy: 0.956| Precision: 0.980 | Recall: 0.949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "def get_scores(model, X,y):\n",
    "    pred = model.predict(X)\n",
    "    return accuracy_score(y, pred), precision_score(y, pred), recall_score(y, pred)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    rf_votes = RandomForest(num_trees = 10, num_features = 5)\n",
    "    rf_votes.fit(X_train, y_train)\n",
    "    train_scores.append(get_scores(rf_votes, X_train, y_train))\n",
    "    test_scores.append(get_scores(rf_votes, X_test, y_test))\n",
    "\n",
    "train_scores_overall = np.array(list(train_scores)).mean(axis = 0)\n",
    "test_scores_overall = np.array(list(test_scores)).mean(axis = 0)\n",
    "\n",
    "print(\"Train Data Statistics | Accuracy: {:.3f}| Precision: {:.3f} | Recall: {:.3f}\".format(train_scores_overall[0],train_scores_overall[1],train_scores_overall[2]))\n",
    "print(\"Test Data Statistics | Accuracy: {:.3f}| Precision: {:.3f} | Recall: {:.3f}\".format(test_scores_overall[0],test_scores_overall[1],test_scores_overall[2]))\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Statistics | Accuracy: 0.996| Precision: 0.999 | Recall: 0.995\n",
      "Test Data Statistics | Accuracy: 0.961| Precision: 0.977 | Recall: 0.959\n"
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "def get_scores(model, X,y):\n",
    "    pred = model.predict(X)\n",
    "    return accuracy_score(y, pred), precision_score(y, pred), recall_score(y, pred)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    rf_votes_sklearn = RandomForestClassifier(n_estimators = 10, max_features = 5)\n",
    "    rf_votes_sklearn.fit(X_train, y_train)\n",
    "    train_scores.append(get_scores(rf_votes_sklearn, X_train, y_train))\n",
    "    test_scores.append(get_scores(rf_votes_sklearn, X_test, y_test))\n",
    "\n",
    "train_scores_overall = np.array(list(train_scores)).mean(axis = 0)\n",
    "test_scores_overall = np.array(list(test_scores)).mean(axis = 0)\n",
    "\n",
    "print(\"Train Data Statistics | Accuracy: {:.3f}| Precision: {:.3f} | Recall: {:.3f}\".format(train_scores_overall[0],train_scores_overall[1],train_scores_overall[2]))\n",
    "print(\"Test Data Statistics | Accuracy: {:.3f}| Precision: {:.3f} | Recall: {:.3f}\".format(test_scores_overall[0],test_scores_overall[1],test_scores_overall[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '1', '0', ..., '1', '0', '1'],\n",
       "       ['0', '1', '0', ..., '1', '0', '-1'],\n",
       "       ['-1', '1', '1', ..., '1', '0', '0'],\n",
       "       ..., \n",
       "       ['0', '-1', '0', ..., '1', '0', '1'],\n",
       "       ['0', '0', '0', ..., '1', '0', '1'],\n",
       "       ['0', '1', '0', ..., '1', '-1', '0']], \n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3env]",
   "language": "python",
   "name": "Python [py3env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
