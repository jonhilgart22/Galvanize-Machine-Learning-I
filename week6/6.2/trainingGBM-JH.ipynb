{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training sequence for GBM\n",
    "The key to training a gradient boosting machine is to use the following sequence.  \n",
    "1.  Pick good nominal values for most of the parameters (ignore most of the parameters in the API list).\n",
    "2.  Dial in the learning rate.\n",
    "3.  Dial in the tree depth.  \n",
    "4.  For all of the dialing-in use the staged performance to evaluate progress.  \n",
    "5.  Try different loss functions.\n",
    "\n",
    "After doing the dialing-in you might used grid search for fine tuning the results. \n",
    "\n",
    "## Pick good nominal values\n",
    "The values you need to set to begin are:  \n",
    "1.  Number of trees in the ensemble\n",
    "2.  Tree depth\n",
    "3.  Learning rate parameter\n",
    "\n",
    "#### 1. Number of trees in the ensemble.  \n",
    "2000  \n",
    "In his documentation of gbm (the R package for gradient boosting), Greg Ridgeway recommends starting with 3000 - 4000 trees.  That is what I would start with unless that makes the training time unbearably long.  If the training time with 3000 trees is unbearably long, then do an early stage of training with a smaller number - say 500.  Go through the sequence here and when you've finished a first pass, take another pass with more trees.  \n",
    "\n",
    "#### 2. Tree depth\n",
    "1 - 3  \n",
    "Gradient boosting is different from random forests.  As we discussed in class and demonstrated in the simple visual example, gb recursively diminishes the error and even with stumps will generate a good global approximation to the underlying function being approximated.  Pick a tree depth between 1 and 3 to begin.  \n",
    "\n",
    "#### 3. Learning rate parameter\n",
    "1.0  \n",
    "I like to get the gradient descent to explode in order to get a feel for how touchy the convergence is going to be.  The next step is to tune to learning rate parameter, so this gives us a starting point.  \n",
    "\n",
    "#### Other parameters in the sklearn implementation\n",
    "-Ignore the min_split, min_leaf etc.  Those are other regularization parameters (besides tree depth).  They are useful for training an individual tree.  They aren't useful here. \n",
    "-Ignore the error types (for now).  Just use least squared error.  \n",
    "\n",
    "\n",
    "## Use staged performance to evaluate progress\n",
    "Staged performance gives you a plot of test and training performance each time a new tree is added to the ensemble.  You really are doing gradient descent and this plot shows you the progress of training.  How is it converging.  The goal here is to adjust the learning rate parameter so that the performance improvement just flattens out at the right hand edge of the plot.  The number of trees is fixed at 500 trees if you're impatient and 2000 trees if you're patient.  \n",
    "\n",
    "## Dial in the learning rate parameter\n",
    "Start with a learning rate of 1.0.  If the error on test data doesn't blow up with 1.0, then add a zero (10.0) and see if that does it.  Then go to 0.1, 0.01, 0.001 etc until you get test error curve to just flatten out at the right hand edge of the plot.  You may have take finer steps after you've bracketed the answer\n",
    "\n",
    "\n",
    "## Dial in the number of trees\n",
    "I'd try something in the range of 1 - 7 for the tree depth parameter.  Frequently there won't be much difference between these in which case use the smallest value.  Sometimes there's a surprising difference.  Try some outlandish values to make sure you've got the answer bracketed.  You can use grid search for this, but you won't be able to monitor the staged error curves with grid search, so be sure to look at them before selecting a model.  It may be that the error won't have flattened out for the tree depth that grid search selects.  \n",
    "\n",
    "## Picking a loss function\n",
    "The main reason for tweaking the loss function is to adjust some element of the model's performance.  Usually that has to do with how the model behaves in use.  Does it have good behavior on outliers?  Does it meet the performance requirements of your project?  Performance on the project may be measured differently from the performance on the model optimization.  For example log likelihood may not quite capture click-through rate on ads.  MSE may not quite capture accumulated trading losses in a trading system.  After you've got everything else dialed in, try alternative loss functions to see if you can squeeze out a little more performance on project metrics.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
