{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 6.2 Lecture - Gradient Boosting\n",
    "\n",
    "## By the End of this Lecture You Will\n",
    "\n",
    "1. Be more familiar with loss functions.\n",
    "2. Be able to write down common loss functions.\n",
    "3. Be able to describe in your own words the general Gradient Boosting algorithm.\n",
    "4. Write the pseudocode for Gradient Boosting Regression Trees.\n",
    "\n",
    "##  References\n",
    "https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf   \n",
    "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions - Regression and Classification\n",
    "\n",
    "Loss functions are computationally feasible loss functions representing the price paid for inaccuracy of prediction.\n",
    "They are quite literally invented, based upon the research and intuition of the investigators who developed them. Sound statistical practice requires selecting an estimator consistent with the  acceptable variation known in the context of an applied problem. Loss functions are selected based on apriori knowledge of the losses that will be experienced from being wrong. This usually amounts to a theoretical argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss Functions for Classification\n",
    "\n",
    "Classification and regression require different types of loss measures. In particular, classification loss for an individual point is characterized in terms of its a binary measure:\n",
    "\n",
    "$$ l_i = y_i \\cdot f(x_i)$$\n",
    "\n",
    "Where it is defined that $f(x) \\in \\{-1,1\\}$, a sort of raw binary output rather than specific class label (see the adaboost lecture for another example of this).  It is standard practice to discuss loss functions as functions of only one variable, in order for simplicity.\n",
    "\n",
    "### 0-1 Function\n",
    "\n",
    "The most natural selection for loss would be the 0-1 binary loss function, a function that scores 1 (loss) for an incorrect classification, and 0 for an correct classification. We can formulate it in terms of the [heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function):\n",
    "\n",
    "$$l(y_i, f(\\theta,x_i)) = H(-y_i \\cdot f(\\theta,x_i))$$\n",
    "\n",
    "However, 0-1 is computationally miserable to handle. It's not differentiable and constitutes a computationally intractable problem in terms of formulating a working solution. Instead we substitute surrogate functions that closely mimic the 0-1 function close to inflection points, but are differentiable and convex. (thus optimizable)\n",
    "\n",
    "### Exponential Loss\n",
    "\n",
    "This is the loss function that we use in discrete adaboost:\n",
    "\n",
    "$$l(y_i, f(\\theta, x_i)) = ln(1+e^{-y_i \\cdot f(\\theta,x_i)})$$\n",
    "\n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "The hinge loss is very popular and is the backbone of many linear models (like SVMs). It has a discontinuity, but is differentiable everywhere else.\n",
    "\n",
    "$$l(y_i, f(\\theta, x_i)) = argmax(0,1-y_i \\cdot f(\\theta, x_i)) $$\n",
    "\n",
    "$$\\dfrac{\\partial l(y_i, f(\\theta, x_i))}{\\partial \\theta} = \\begin{cases} -y_i \\cdot f'(\\theta,x_i) & y_i \\cdot f(\\theta, x_i) < 0\\\\\n",
    "0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "### Square Loss (also used for regression)\n",
    "\n",
    "$$l(y_i, f(\\theta, x_i)) = (1-y_i \\cdot f(\\theta, x_i))^2$$\n",
    "\n",
    "![loss functions classification](/images/loss_function_surrogates_2.png)\n",
    "\n",
    "* Indigo: 0-1 Loss \n",
    "* Green: Square Loss\n",
    "* Yellow: Logistic Loss\n",
    "* Purple: Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Regression\n",
    "\n",
    "Loss functions for regression are often somewhat simpler than classification due to the fact that the models predict continuous values. \n",
    "\n",
    "### Square Loss (also used for regression)\n",
    "\n",
    "$$l(y_i, f(\\theta, x_i)) = (y_i - f(\\theta, x_i))^2$$\n",
    "\n",
    "\n",
    "### Absolute Error\n",
    "\n",
    "$$l(y_i, f(\\theta, x_i)) = |y_i - f(\\theta, x_i)|$$\n",
    "\n",
    "\n",
    "### Huber Loss\n",
    "\n",
    "Where $\\delta$ is an input value representing the slope of the loss function away from 0\n",
    "\n",
    "$$l(y_i, f(\\theta, x_i)) = \\begin{cases} \\dfrac{1}{2}(y_i - f(\\theta, x_i))^2 &   |y_i - f(\\theta, x_i)| \\leq \\delta \\\\\n",
    "\\delta|y_i - f(\\theta, x_i)| - \\dfrac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "\n",
    "![loss funct regression](./images/loss_functions_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Boosting as a Whole\n",
    "\n",
    "So far you have learned the discrete adaboost algorithm for trees, but adaboost can be applied to any situation, given the correct cost function and implementation. In fact, a great deal of effort has gone into developing the mathematical foundation for a variety of ensemble boosting algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Adaboost\n",
    "\n",
    "    Construct base learner F_0(theta, w, x, y)\n",
    "    \n",
    "    for t in (1,T):\n",
    "        given current error weighting alpha, optimize weak learner h_t(theta, w, x, y)\n",
    "        add to ensemble F_t = F_t-1 + alpha*h_t(theta, w, x, y)\n",
    "        eps = (w[y' != y])/sum(q) (weights from the set of misclassified points)\n",
    "        update error weighting for this tree, alpha = 0.5 * ln((1-eps)/eps))\n",
    "        update weights w with loss function\n",
    "\n",
    "Any learner (classifier or regressor) that produces a simple output of predictions can be employed this way, much like a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Gradient Boosting\n",
    "\n",
    "Gradient Boosting, like Adaptive Boosting, is a technique that can easily be applied to any classifier or regressor. \n",
    "\n",
    "    Construct base learner of type f, F_0(theta, x, y) with loss function L(y, F(x))\n",
    "    \n",
    "    for t in (1,T):\n",
    "        compute partial derivatives r' of loss function w.r.t. last learner, F_t-1 for all i: 1...n\n",
    "        fit a weak learner on r': f_t(theta, x, r')\n",
    "        find an optimum (lagrange) multiplier lambda on the loss function L(y, F_t-1 + lambda*f_t(theta, x, r'))\n",
    "        add this learner to the new model: F_t = F_t-1 + lambda*f_t(theta, x, r')\n",
    "   \n",
    "The fundamental difference between gradient boosting and adaptive boosting is the fitting of subsequent stages of learners on the **gradient of the loss function**. Why?\n",
    "\n",
    "The reasoning behind this is rather simple and hearkens back to basic calculus.\n",
    "\n",
    "Suppose we are trying to make an estimate of the objective (loss plus regularization for model complexity) function $O$ at iteration t of the boost:\n",
    "\n",
    "$$O_{t} = \\sum_{i=1}^N L(y, F_{t-1}(x_i) + \\lambda\\ f_t(\\theta, x_i, r')) + \\Omega(f_t(\\theta, x, r'))$$\n",
    "\n",
    "Where the loss function is again $L$ and the complexity (regularization term) is $\\Omega$. We take the Taylor expansion of a function f (any function, not just the weak learner) at two degrees of order:\n",
    "\n",
    "$$f(x+\\Delta\\ x) \\simeq f(x) + f'(x)\\Delta x + \\dfrac{1}{2}f''(x)\\Delta x^2$$\n",
    "\n",
    "About the previous estimator $F_{t-1}$:\n",
    "\n",
    "$$L(y, F_{t-1}) + \\partial_{F_{t-1}}L(y, F_{t-1})f_{t} + \\dfrac{1}{2}\\partial_{F_{t-1}}^2L(y, F_{t-1})f_{t}^2$$\n",
    "\n",
    "Set \n",
    "\n",
    "$$ g_i = \\partial_{F_{t-1}}L(y, F_{t-1}(x_i))$$\n",
    "\n",
    "$$ h_i = \\partial_{F_{t-1}}^2L(y, F_{t-1}(x_i))$$\n",
    "\n",
    "This makes the first estimate of the objective function to be:\n",
    "\n",
    "$$O_{t} = \\sum_{i=1}^N [L(y, F_{t-1}(x_i)) + g_{i}f_t(x_i) + \\dfrac{1}{2}h_{i}f_t(x_i)^2] + \\Omega(h_t(\\theta, x, r'))$$\n",
    "\n",
    "This transformation benefits us because it has simplified the composite loss function in terms of an expansion of derivatives, which if we choose the right base loss function, can still be not too hard to find. Note that we also have to have the output of $f_t(x_i)$, the base learner, at this stage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Regression Trees\n",
    "\n",
    "In order to make the above equation meaningful for trees, we start with making the complexity function simpler:\n",
    "\n",
    "$$\\Omega(h_t(\\theta, x, r')) = \\gamma T + \\dfrac{\\lambda}{2} \\sum_{j=1}^{T}w_j^2$$\n",
    "\n",
    "Where $T$ is the height of the tree, and $w^2$ represents the score for the regression **coming from an individual leaf**. This changes the objective function to:\n",
    "\n",
    "$$O_{t} = \\sum_{i=1}^N [g_{i}f_t(x_i) + \\dfrac{1}{2}h_{i}f_t(x_i)^2] + \\gamma T + \\dfrac{\\lambda}{2}\\sum_{j=1}^{T}w^2 + C$$\n",
    "\n",
    "Where we now take the sum over all tree branches in each tree the component from evaluating the previous loss function at $t-1$ is held as a constant $C$. We also take the score from a leaf and set $f_t = w_t$.\n",
    "\n",
    "$$O_{t} = \\sum_{j=1}^{T}\\sum_{i=1}^N [g_{i}w_j(x_i) + \\dfrac{1}{2}(h_{i}+ \\lambda)w_j(x_i)^2] + \\gamma T + C$$\n",
    "\n",
    "$$O_{t} = \\sum_{j=1}^{T}[\\sum_{i=1}^N g_{i}w_j(x_i) + \\dfrac{1}{2}(\\sum_{i=1}^N h_{i}+ \\lambda)w_j(x_i)^2] + \\gamma T + C$$\n",
    "\n",
    "We can simplify these equations by counting up the derivatives over the set of datapoints belonging to that split in the leaf, called the *instance* $I$.\n",
    "\n",
    "$$ G_j = \\sum_{i=1}^{N_{I}} g_{i} $$\n",
    "\n",
    "$$ H_j = \\sum_{i=1}^{N_{I}} h_{i} $$\n",
    "\n",
    "$$O_{t} = \\sum_{j=1}^{T}[G_{j}w_j(x_i) + \\dfrac{1}{2}(H_{j}+ \\lambda)w_j(x_i)^2] + \\gamma T + C$$\n",
    "\n",
    "We need to find the optimal leaf score. Taking the first derivative of $O$ with respect to $w_j$ and setting to zero: \n",
    "\n",
    "$$\\dfrac{\\partial O_{t}}{\\partial w_{j}} = \\sum_{j=1}^{T}[G_{j} + (H_{j}+ \\lambda)w_j(x_i)] = 0$$\n",
    "\n",
    "\n",
    "$$\\sum_{j=1}^{T}G_{j} =  -\\sum_{j=1}^{T}(H_{j}+ \\lambda)w_j(x_i)$$\n",
    "\n",
    "Assuming the tree is built, we have a fixed structure at every point $t$, so we can write the *optimal leaf score* at every point j:\n",
    "\n",
    "$$ w_{j}^{\\dagger} = -\\dfrac{G_{j}}{H_{j}+\\lambda}$$\n",
    "\n",
    "We can substitute $w_{j}^{\\dagger}$ everywhere we see $w_j$ in the above equation to get an optimal objective function:\n",
    "                                                 \n",
    "$$O_{t}^{\\dagger} = \\sum_{j=1}^{T}[-\\dfrac{G_{j}^2}{H_{j}+ \\lambda} + \\dfrac{1}{2}\\dfrac{G_{j}^2}{H_{j}+ \\lambda}] + \\gamma T $$\n",
    "\n",
    "$$O_{t}^{\\dagger} = -\\dfrac{1}{2}\\sum_{j=1}^{T}\\dfrac{G_{j}^2}{H_{j}+ \\lambda} + \\gamma T $$\n",
    "\n",
    "Meaning that we need to find the minimum objective score that is a sum over all the leaves in the tree. This amounts to no more than a dynamic programming problem, wherein we need only find the best possible tree, optimizing the score of the leaf at every branch.\n",
    "\n",
    "### In practice\n",
    "\n",
    "In practice, this does not differ in any way from a regular tree. we grow the tree just as we would a standard regression tree, looking for the local optimum at each leaf. The information gain at each branch is computed from the partition, just as it would be in a regular decision tree, except that we use the $g_i$ and $h_i$ as above.\n",
    "\n",
    "$$\\text{Gain} = \\dfrac{G_{L}^2}{H_{L}+\\lambda}+\\dfrac{G_{R}^2}{H_{R}+\\lambda}-\\dfrac{(G_{L}+G_{R})^2}{H_{L}+H_{R}+\\lambda} - \\gamma$$\n",
    "\n",
    "Where $L$ is the partition of points in the left child, $H$ is the partition of points in the right child, and the latter term is the value of the score if we do not split. \n",
    "\n",
    "The current state-of-the-art loss function is Huber loss, discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GBRT algorithm\n",
    "\n",
    "    Construct base tree F_0(theta, x, y) with loss function L(y, F(x))\n",
    "    \n",
    "    for t in (1,T):\n",
    "        Compute partial derivatives g_t, h_t with the needed loss function\n",
    "        Construct tree f_t optimizing the structure score at each split\n",
    "        Prune tree as necessary\n",
    "        find an optimum (lagrange) multiplier lambda that minimizes RMSD error\n",
    "        add f_t to the new model: F_t = F_t-1 + lambda*f_t(theta, x, r')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
