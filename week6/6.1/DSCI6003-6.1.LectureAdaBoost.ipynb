{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 6.1 Lecture\n",
    "\n",
    "## Adaptive Boosting\n",
    "\n",
    "## By the End of This Lecture You Will:\n",
    "1. Be able to describe the AdaBoost algorithm in your own words\n",
    "2. Be able to write the pseudocode for the AdaBoost algorithm\n",
    "\n",
    "## References\n",
    "http://www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf - short paper with clean derivation  \n",
    "https://en.wikipedia.org/wiki/AdaBoost  \n",
    "http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf - Includes description of importance sampling\n",
    "http://statweb.stanford.edu/~jhf/ftp/boost.pdf - Friedman's analysis of adaboost  \n",
    " \n",
    "## Boosting in Trees\n",
    "\n",
    "There are two types of boosting used in trees in common practice today. The first type of boosting is the use of one model to precondition the inputs of another model This is the most common jargon usage of this word in data science. If you hear fellow data scientists say \"boosting\", they are often referring to boosted trees. \n",
    "\n",
    "### Adding bias into the training process\n",
    "\n",
    "In the boosted tree algorithms, a ensemble of trees is \"grown\" over successive generations of trees, by making each successor tree an expert on attacking the weaknesses of the other. What results is a forest of designed to defeat all weaknesses of the training set. Members of the ensemble are weighted by the ratio of errors not covered by the previous members.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "AdaBoost is the simpler of the two boosting learners we shall discuss. It focuses on providing each tree a description of the mistakes of the last tree (through the cost \"loss\" function). Each tree is given the opportunity to **Ada**pt itself to the weaknesses of previous trees. Finally the end classifier is something like a random forest, where each of the trees votes, weighted inversely to its own total error.\n",
    "\n",
    "![adaboost_scheme](./images/AdaBoost_scheme.png)\n",
    "\n",
    "Below is shown a plot of the AdaBoost partition:\n",
    "\n",
    "![adaboost_part](./images/AdaBoost_part.png)\n",
    "\n",
    "Successive trees end up covering the limitations of previous trees. The size of the points reflect their weighting at that point. The misclassified points are *individually* penalized, just as misclassified points are penalized in SVMs. \n",
    "\n",
    "The next tree in the family may or may not correctly classify these points (it is still just a simple decision tree), but if it does, then the total cost function comes down, and it is more favorable to keep the tree. If it does not, the weights continue to increase until a tree comes along that correctly classifies these points. \n",
    "\n",
    "### How the Weights are Used\n",
    "\n",
    "The weights $w_{i}$ are used in the impurity estimator during the split, making it progressively more unfavorable when we calculate the information gain if they are misclassified (fall inside a leaf belonging to another class - the weights increase the equivalent counts of a class belonging to that point). This results in a powerful increase in impurity if this point is misclassified.\n",
    "\n",
    "\n",
    "\n",
    "## The AdaBoost Algorithm\n",
    "\n",
    "    Fit:\n",
    "    \n",
    "        Initialize weights W to be 1/N\n",
    "        for m in range(1,M): (M determined by user)\n",
    "            fit a standard decision tree G(X,w)\n",
    "            compute total error err <- sum(W*misclassified points)/sum(W)\n",
    "            compute alpha_i <- log((1-err)/err)\n",
    "            change weights by an exponent if misclassified w_i <- \n",
    "            w_i * exp(alpha_i)\n",
    "   \n",
    "    Predict:\n",
    "     \n",
    "        for m in range(1, M):\n",
    "            sum += alpha_m * G_m(X)\n",
    "        return sign(sum)\n",
    "\n",
    "## QUIZ:\n",
    "\n",
    "Can you think of another algorithm that might benefit from adaptive boosting?\n",
    "\n",
    "### Some Details:\n",
    "\n",
    "Thus the most specific trees (at the end of the boost chain) are often the most dilute (reflect the fewest points), but they are weighted (by error score) much more than the earliest. Many of the later trees may specify for only a tiny number of points. The mixture of different trees can manage outliers and noise much better than otherwise.\n",
    "\n",
    "Adaboost is often affected profoundly by the choice of prepruning parameters. This can be thought of in terms of the tree's selection of partition boundary. If the boundaries for an individual tree become complex, we are eschewing the algorithm's ability to weight the votes of later trees to manage small numbers of specific points in favor of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
