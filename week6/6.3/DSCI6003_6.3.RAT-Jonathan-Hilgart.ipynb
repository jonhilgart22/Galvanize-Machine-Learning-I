{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAT 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe the Gradient Boosting Algorithm and its use cases\n",
    "\n",
    "- Why are we able to cross validate our Random Forest with our training set (OOB error)? I.e. Why doesn't this count as testing on my training set?\n",
    "\n",
    "- In Random Forests, what do you expect to happen to the test error as you build more trees? How is this different from increasing the number of trees in Boosting?\n",
    "\n",
    "- In building an ensemble model, what part of the calculation is dependent on the number of rows of data?  How can you spread that over multiple machines?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 ) Gradient boosting is a type of boosting algorithm that trains each subsequent tree on the error of the previous tree. This error is measured as a learning rate (alpha) times the gradient of that tree. Therefore, when you train the next tree, you fit the gradient boosting model on X , and the grad (where grad is the error from the previous tree times a leraning rate).\n",
    "\n",
    "- The idea behind gradient boosting is the 'wisdom of the crowd'. If you have a bunch of weak learners (can predict an outcome slightly better than 50%), then you can combine these weak learners predictions together to create a strong learner. \n",
    "\n",
    "- Gradient boosting is great for learning non-linear boundaries and decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) We are able to cross validate our Random Forest with OOB error because we do not see all of the data when we build the tree! Since we are randomly choose features for split points, there will be about 33% of the data that is unseen by the tree. Therefore, you can use this data to cross validate the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3) As you build more trees, we expect the test error to decrease (variance to decrease). This is because you will have more trees that are voting at each particular step for the outcome measure. \n",
    "\n",
    "- This is different than the number of trees in boosting because in boosting each tree is built sequentially. Tree at time T+1 is built off of the error from tree at time T. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4) In a tree ensemble model, the max depth of the tree is dependent on the number of rows of data (i.e. log2(number of rows of data) gives you the maximum tree depth possible. You can spread this over multiple machines by having each machine build a tree independently of the other machines (for Random Forest models). Then, compare the votes of each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
