{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put import statements here\n",
    "from sklearn.datasets import load_iris\n",
    "import random\n",
    "random.seed(15)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 Lab - SVM\n",
    "\n",
    "Today we will be implementing sklearn's version of SVM. For those of you who are curious, the SVM practicums contain more information about implementing your own version of SVM.\n",
    "\n",
    "Today We Will:\n",
    "\n",
    "    1. Learn how the parameters influence the decision boundary for SVM\n",
    "    2. Compare and contrast an SVM to see how it differs from Logistic Regression.\n",
    "    3. Using SVMs to deal with unbalaanced classes\n",
    "\n",
    "### Part 1 - Parameter Tuning\n",
    "\n",
    "1. Load in the rbf_data and the rbf_labels dataset using pandas (make sure to set delim_whitespace=True and header = None).\n",
    "2. Plot the data using matplotlib, setting the c attribute to labels for the points. \n",
    "3. Plot gamma from [0.1,0.3,1,3,10], keeping C constant at 1. What do you notice?\n",
    "4. Plot C from [1E-1,1,10,100] holding gamma constant at 3. What do you notice?\n",
    "5. This may take a while, but plot gamma at 250. What do you notice? \n",
    "\n",
    "\n",
    "### Part 2 - Compare and Contrast\n",
    "\n",
    "1. We will be using the iris dataset. Load in the iris dataset from sklearn. \n",
    "2. Make the classfication binary by changing any 2 label to a 1.\n",
    "3. Plot the the third and fourth columns of the dataset (watch out for indexing!). Use plt.copper() before plt.show() to change the color of the points (or use your favorite colormap).\n",
    "4. Run a Logistic Regression Classifier (LRC) on the third and fourth columns and plot the boundary. What do you notice about the boundary? (Use the function below to plot the decision boundary.\n",
    "5. Now, run an SVM on the third and fourth column, and use the function below to plot the boundary. What do you notice? Which kernel can you use to correctly classify the last point?\n",
    "6. Now run steps 3 - 5 again, except now you will change every 2 into a 0 rather than a one. What do you notice about the decision boudaries? \n",
    "7. To get an even better understanding of why we might prefer SVMs over LRC, load in the data_scientist.csv data. Plot it with a logistic regression and SVM decision boundary. What do you notice? \n",
    "\n",
    "### Part 3 - Imbalanced Classes\n",
    "\n",
    "Let's pretend this data now corresponds to credit card fraud, where a true positive means saving thousands of dollars and maintaining customer loyalty while a false positive means us calling the customer and having them confirm that they were the ones to make the purchase (a small cost for letting fraudsters escape). How can you catch as many true positives (fraudsters) as possible? \n",
    "\n",
    "1. Now create variables X_small, y_small which are subsets of the iris data. You can run the \"annihilate_data\" function to remove the data.\n",
    "2. What do the class counts look like now? Plot the data.\n",
    "3. Run an LRC and plot the decision boundary. What is the behavior of the model? \n",
    "4. Now plot the decision boundary for an SVM. What is the behavior? Change the kernels. Does anything happen? \n",
    "5. Now as the data scientist, you should be able to look at documentation and figure out what the best tool for the job will be. Looking at the SVC inputs, what variable can you change to fix this problem? Plot the decision boundary after you have made this adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annihilate_data(X,y,num=10):\n",
    "    y_0 = len(X[y == 0])\n",
    "    y_1 = len(X[y == 1])\n",
    "    smaller = 0 if y_0 < y_1 else 1\n",
    "    idx = np.random.choice(np.where(y == smaller)[0],size = num)\n",
    "    full_idx = np.append(np.where(y != smaller)[0],idx)\n",
    "    return X[full_idx],y[full_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_boundary(clf, X, Y, h=.02):\n",
    "    \"\"\"Inputs:\n",
    "        clf - a trained classifier, with a predict method\n",
    "    \"\"\"\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py3env]",
   "language": "python",
   "name": "Python [py3env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
