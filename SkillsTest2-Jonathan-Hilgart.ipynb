{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name:\n",
    "Remember to include your name in the filename as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills Test 2\n",
    "\n",
    "## Part 1: Multiple Choice\n",
    "Notice some of these questions require more than 1 answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     1. In which cases would you expect penalized regression to be a better choice than boosted trees? (Type out all the letters corresponding to points that are true in the answer space).\n",
    "         a. Lots of rows relative to the number of columns\n",
    "         b. The best possible decision boundary is a straight line\n",
    "         c. Lots more columns than rows\n",
    "         d. The best possible decision boundary is jagged and non-linear\n",
    "         \n",
    "         Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1\n",
    "> b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "     2. In which cases would suspect that Lasso Regression would outperform Ridge Regression?\n",
    "      (Type out all the letters corresponding to points that are true in the answer space).\n",
    "        a. When it gives better performance on cross validated data\n",
    "        b. To select a group of collinear varibles that have a large affect on the outcome\n",
    "        c. Selecting a single variable that has a large affect on the outcome\n",
    "        \n",
    "        Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2\n",
    "> a,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "    3. Why would we need to use basis expansion? What are different ways that you can perform basis expansion? (Type out all the letters corresponding to points that are true in the answer space).\n",
    "        a. Fitting a linear model to non-linear data\n",
    "        b. For extracting variable importances \n",
    "        c. Adding squared and cubed terms to a linear model \n",
    "        d. For removing collinear data from the dataset\n",
    "        \n",
    "        Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3\n",
    "> a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       \n",
    "     4. What is the bias/variance trade off (Choose one) \n",
    "        a.  The trade off between interpretability of a model and how well it performs\n",
    "        b.  The trade off between how well a model fits to the training and testing data\n",
    "        c.  The trade off between having less or more features included in your model\n",
    "        \n",
    "        Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4\n",
    "> b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      \n",
    "     5. MSE and R^2 - Interpret these metrics. What types of models are these appropriate for? \n",
    "      (Type out all the letters corresponding to points that are true in the answer space)\n",
    "        a. They are used to measure success of a classification model\n",
    "        b. They are used to measure success of a regression model\n",
    "        c. MSE is a value between 0 and 1\n",
    "        d. R^2 is a value between 0 and 1\n",
    "        e. MSE corresponds to the amount of variance that is being captured by the model \n",
    "        f. MSE corresponds to the average for the square of the deviations\n",
    "\n",
    "        Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5\n",
    "> b, d, e, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6.  Derive and write the cost function for Logistic Regression using LaTex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ h(Xi) = \\frac{1}{ 1+ e ^{-(XiBi)}} $\n",
    "\n",
    "Cost Function:  $ C(Bi) = \\frac{-1}{m} \\sum _{i{\\mathop {=}}1}^{m}yi*log(h(Xi)) + (1-Yi)log(1-h(Xi)) $\n",
    "\n",
    "- solve for **log(h(xi))**: $ log(\\frac{1}{ 1+ e ^{-(XiBi)}}) => log(1) - log({ 1+ e ^{-(XiBi)}}) = >**-log({ 1+ e ^{-(XiBi)}})** $\n",
    "- solve for **log(1-H(xi))**: $ log( 1 - \\frac{1}{ 1+ e ^{-(XiBi)}}) => log [ (\\frac{{ 1+ e ^{-(XiBi)}}}{{ 1+ e ^{-(XiBi)}}) }  - \\frac {1} { 1+ e ^{-(XiBi)}}] => log[ \\frac {e ^{-(XiBi)}} { 1+ e ^{-(XiBi)}}] = > ** -BiXi - log({ 1+ e ^{-(XiBi)}})** $\n",
    "\n",
    "\n",
    "- Plug these solutions into the original Cost Function\n",
    "\n",
    "\n",
    "Updated Cost Function:  $ C(Bi) = \\frac{-1}{m} \\sum _{i{\\mathop {=}}1}^{m}-yi*log({ 1+ e ^{-(XiBi)}}) + (1-Yi)(-XiBi - log({ 1+ e ^{-(XiBi)}}) $\n",
    "\n",
    "- Simplify the cost function using the fact that 1+e^x ~ e^x and combining the XiBi terms\n",
    "\n",
    "$ C(Bi) = \\frac{-1}{m} \\sum _{i{\\mathop {=}}1}^{m} (yi*XiBi -XiBi - log({ 1+ e ^{-(XiBi)}}) $\n",
    "\n",
    "- Using the fact that: $ -XiBi - log({ 1+ e ^{-(XiBi)}}) =  log({ 1+ e ^{-(XiBi)}}) $\n",
    "\n",
    "$ C(Bi) = \\frac{-1}{m} \\sum _{i{\\mathop {=}}1}^{m} (yi*XiBi -log({ 1+ e ^{-(XiBi)}}) $\n",
    "\n",
    "- Now take the partial derivative of the Cost Function with respect to the weights (Bi)\n",
    "\n",
    "$\\frac{{\\partial C}}{{\\partial Bi}} = \\frac{-1}{m}\\sum [ YiXi +  \\frac {e ^{-(XiBi)}*(XiBi)}{ 1+ e ^{(XiBi)}} ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Define the following terms pertaining to confusion matrices and provide formulas for each (using TP, TN, FP, FN). What situations do each of these quantities correspond with given that you are trying to predict whether or not a patient has cancer (you can skip the F1 statistic). For example, accuracy corresponds to the fraction of all the medical cases that we predicted correctly. \n",
    "    \n",
    "        * Accuracy - \n",
    "        * Precision - \n",
    "        * Recall - \n",
    "        * F1 Statistic - \n",
    "        * Type I Error - \n",
    "        * Type II Error - \n",
    "        \n",
    "       What part of the confusion matrix represents Type I and Type II errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Accuracy: The number of correct terms over the total number of terms. (TP+FP) / (TP+FP+FP+FN)\n",
    "- Precision:  is the measure of reproducibility of a given medical test on the same patient (i.e. achieve the same result after running a test multiple times).  (TP) / (TP+FN)\n",
    "- Recall: Is the neumber of relevant tests (i.e. tests that help diagnose a patients condition) over all of the tests run. TP / (TP+FP) \n",
    "- F1 Statistic: ${\\displaystyle F_{1}\\cdot {\\frac {1}{{\\tfrac {1}{\\mathrm {recall} }}+{\\tfrac {1}{\\mathrm {precision} }}}}}.$ The overall effectiveness of a suite of tests given to a patient to asses his/her condition.\n",
    "- Type 1 Error: This is the probability of incorrectly rejecting the null hypothesis. FP / (TN+FP). The probability that the doctor said the patient was sick when the patient was healthy.\n",
    "- Type II Error: This is the probability of failing to reject the null hypothesis. FN / (FN+TP). The probability that the doctor said the patient was healthy when the patient was actually sick. \n",
    "- Type I error in a confusion matrix (matrix A) ( prediction on top vs actual on the rows) is the  A01 / (A00 + A01)\n",
    "- Type II error in a confusion matrix (matrix B) is B10 / (B10 + B11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Model\n",
    "\n",
    "# Wine Quality Modeling\n",
    "\n",
    "Use **ensembles of trees** (Random Forest and Gradient Boosting) to build a good model for the <a href = \n",
    "'https://archive.ics.uci.edu/ml/datasets/Wine+Quality'>wine quality dataset</a>. Make the required parametric adjustments to get the best performance from each model. Report the final MSE and R^2. **Extract and compare** the variable importances generated by each model. We are not going to hold your hand here. Use whatever tools/visuals you need to help you come up with the best model possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics  import r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the data\n",
    "whitewine_df = pd.read_csv('winequality-white.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           4898 non-null float64\n",
      "volatile acidity        4898 non-null float64\n",
      "citric acid             4898 non-null float64\n",
      "residual sugar          4898 non-null float64\n",
      "chlorides               4898 non-null float64\n",
      "free sulfur dioxide     4898 non-null float64\n",
      "total sulfur dioxide    4898 non-null float64\n",
      "density                 4898 non-null float64\n",
      "pH                      4898 non-null float64\n",
      "sulphates               4898 non-null float64\n",
      "alcohol                 4898 non-null float64\n",
      "quality                 4898 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "#inspect the data\n",
    "whitewine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the columns labels\n",
    "wine_columns = whitewine_df.columns[:-1]\n",
    "wine_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitewine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitewine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(whitewine_df.iloc[:,:-1])\n",
    "y = np.array(whitewine_df.iloc[:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y)\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First, use random forest with random search for best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_forest_parameters = {'n_estimators':[i for i in range(1,500)],'max_depth':[i for i in range(1,500)],\\\n",
    "                            'min_samples_split':[i for i in range(2,50)],\\\n",
    "                            'min_samples_leaf':[i for i in range(1,50)],\\\n",
    "                            'max_features':['auto','sqrt','log2'],\\\n",
    "                            'max_leaf_nodes':[i for i in range(2,500,1)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(RandomForestRegressor(),random_forest_parameters,cv=3,n_iter=100,scoring = 'neg_mean_squared_error',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69...25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_squared_error',\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=345,\n",
       "           max_features='sqrt', max_leaf_nodes=335,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=3,\n",
       "           min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=90, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best estimator from randomized CV\n",
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now , use gridsearch to find the best parameters around the ones randomized search found\n",
    "\n",
    "random_forest_grid = {'bootstrap':['True'],'criterion':['mse'], 'max_depth':[i for i in range(340,350)],'max_features':['sqrt'],\\\n",
    "            'max_leaf_nodes':[i for i in range(330,340)],\\\n",
    "                      'min_samples_leaf':[i for i in range(2,4)],\\\n",
    "           'min_samples_split':[i for i in range(7,10)],'n_estimators':[i for i in range(85,95)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search_RF = GridSearchCV(RandomForestRegressor(),random_forest_grid,cv=2,scoring = 'neg_mean_squared_error',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9000 candidates, totalling 18000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 18000 out of 18000 | elapsed: 93.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'criterion': ['mse'], 'max_depth': [340, 341, 342, 343, 344, 345, 346, 347, 348, 349], 'max_features': ['sqrt'], 'bootstrap': ['True'], 'n_estimators': [85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'max_leaf_nodes': [330, 331, 332, 333, 334, 335, 336, 337, 338, 339], 'min_samples_split': [7, 8, 9], 'min_samples_leaf': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the best parameters for Random Forest RandomForestRegressor(bootstrap='True', criterion='mse', max_depth=348,\n",
      "           max_features='sqrt', max_leaf_nodes=333,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "           min_samples_split=7, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=89, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print( 'These are the best parameters for Random Forest',grid_search_RF.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COmpare best parameters with default parameters\n",
    "best_random_forest = RandomForestRegressor(bootstrap='True', criterion='mse', max_depth=348,\n",
    "           max_features='sqrt', max_leaf_nodes=333,\n",
    "           min_impurity_split=1e-07, min_samples_leaf=2,\n",
    "           min_samples_split=7, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=89, n_jobs=1, oob_score=False, random_state=None,\n",
    "           verbose=0, warm_start=False)\n",
    "default_random_forest = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_forest.fit(X_train,y_train)\n",
    "default_random_forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare MSE from optimal RF and default RF on test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Test) for optimal Random Forest Parameters: 0.3850340189423642\n",
      "R_2 (Test) for optimal Random Forest Parameters: 49.48%\n",
      "MSE (Training) for optimal Random Forest Parameters: 0.1788815608723825\n",
      "R_2 (Training) for optimal Random Forest Parameters: 77.40%\n",
      "\n",
      "Top features for optimal RF\n",
      "[(0.18674401096615545, 'fixed acidity'), (0.12539720134793811, 'sulphates'), (0.10993007752029067, 'pH'), (0.10535345379022022, 'citric acid'), (0.083780168335381017, 'residual sugar'), (0.07329678383525759, 'total sulfur dioxide'), (0.071984921757565587, 'chlorides'), (0.065716070032036222, 'free sulfur dioxide'), (0.064773845711464037, 'volatile acidity'), (0.056782887696681515, 'density'), (0.056240579007009621, 'alcohol')]\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics  import r2_score\n",
    "top_features = best_random_forest.feature_importances_\n",
    "top_features_sorted = sorted(top_features, reverse=True)\n",
    "print('MSE (Test) for optimal Random Forest Parameters: {}'.format(mean_squared_error(y_test,best_random_forest.predict(X_test))))\n",
    "print('R_2 (Test) for optimal Random Forest Parameters: {:.2%}'.format(r2_score(y_test , best_random_forest.predict(X_test))))\n",
    "print('MSE (Training) for optimal Random Forest Parameters: {}'.format(mean_squared_error(y_train,best_random_forest.predict(X_train))))\n",
    "print('R_2 (Training) for optimal Random Forest Parameters: {:.2%}'.format(r2_score(y_train , best_random_forest.predict(X_train))))\n",
    "print()\n",
    "print('Top features for optimal RF')\n",
    "print([(i,l) for i,l in zip(top_features_sorted,wine_columns[np.argsort(top_features)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Test) for default Random Forest Parameters: 0.396734693877551\n",
      "R_2 (Test) for default Random Forest Parameters: 47.95%\n",
      "MSE (Training) for default Random Forest Parameters: 0.07704873400490063\n",
      "R_2 (Training) for default Random Forest Parameters: 90.26%\n",
      "\n",
      "Top features for deafult RF\n",
      "[(0.18674401096615545, 'fixed acidity'), (0.12539720134793811, 'sulphates'), (0.10993007752029067, 'pH'), (0.10535345379022022, 'citric acid'), (0.083780168335381017, 'residual sugar'), (0.07329678383525759, 'total sulfur dioxide'), (0.071984921757565587, 'chlorides'), (0.065716070032036222, 'free sulfur dioxide'), (0.064773845711464037, 'volatile acidity'), (0.056782887696681515, 'density'), (0.056240579007009621, 'alcohol')]\n"
     ]
    }
   ],
   "source": [
    "top_features_d = default_random_forest.feature_importances_\n",
    "top_features_sorted_d = sorted(top_features, reverse=True)\n",
    "\n",
    "print('MSE (Test) for default Random Forest Parameters: {}'.format(mean_squared_error(y_test,default_random_forest.predict(X_test))))\n",
    "print('R_2 (Test) for default Random Forest Parameters: {:.2%}'.format(r2_score(y_test , default_random_forest.predict(X_test))))\n",
    "print('MSE (Training) for default Random Forest Parameters: {}'.format(mean_squared_error(y_train,default_random_forest.predict(X_train))))\n",
    "print('R_2 (Training) for default Random Forest Parameters: {:.2%}'.format(r2_score(y_train , default_random_forest.predict(X_train))))\n",
    "\n",
    "\n",
    "print()\n",
    "print('Top features for deafult RF')\n",
    "print([(i,l) for i,l in zip(top_features_sorted_d,wine_columns[np.argsort(top_features)])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The optimal RF model produces a 2.78% lower MSE compared to the default parameters. The most important features are 1) Fixed acidity 2) Sulphates 3) pH . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, Gradient Boosting on the white wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_boost_parameters = {'loss':['ls', 'lad', 'huber', 'quantile'], 'learning_rate':[i for i in np.linspace(.001,.8,20)],\\\n",
    "                            'n_estimators':[i for i in range(50,500,10)], 'criterion':['friedman_mse'],\\\n",
    "                            'min_samples_split':[i for i in range(2,20)], 'min_samples_leaf':[i for i in range(1,20)],\\\n",
    "                            'max_depth':[i for i in range(1,50)],'subsample':[i for i in np.linspace(.001,1,30)],\\\n",
    "                              'alpha':[i for i in np.linspace(.001,.99,30)],'max_features':['auto','sqrt','log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_random_search = RandomizedSearchCV(GradientBoostingRegressor(),gradient_boost_parameters,n_iter=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'criterion': ['friedman_mse'], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], 'learning_rate': [0.001, 0.04305263157894737, ...62068965511, 0.86220689655172411, 0.89665517241379311, 0.93110344827586211, 0.965551724137931, 1.0]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.88768965517241383, criterion='friedman_mse',\n",
       "             init=None, learning_rate=0.04305263157894737, loss='huber',\n",
       "             max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=6,\n",
       "             min_samples_split=17, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=470, presort='auto', random_state=None,\n",
       "             subsample=0.75886206896551722, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_gradient_search = GradientBoostingRegressor(alpha=0.88768965517241383, criterion='friedman_mse',\n",
    "             init=None, learning_rate=0.04305263157894737, loss='huber',\n",
    "             max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
    "             min_impurity_split=1e-07, min_samples_leaf=6,\n",
    "             min_samples_split=17, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=470, presort='auto', random_state=None,\n",
    "             subsample=0.75886206896551722, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_gradient_search = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gradient_search.fit(X_train,y_train)\n",
    "default_gradient_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Test) for optimal Gradient Boost Parameters: 0.34666758683238624\n",
      "R_2 (Test) for optimal Gradient Boost Parameters: 54.52%\n",
      "MSE (Training) for optimal Gradient Boost Parameters: 0.01596919104136864\n",
      "R_2 (Training) for optimal Gradient Boost Parameters: 97.98%\n",
      "\n",
      "Top features for optimal Gradient Boost\n",
      "[(0.11731126313227053, 'fixed acidity'), (0.099941114170622888, 'citric acid'), (0.097284767715744125, 'alcohol'), (0.093960396173075472, 'sulphates'), (0.091788446471496005, 'volatile acidity'), (0.090276037121821862, 'chlorides'), (0.086489814267343354, 'pH'), (0.084795433578980006, 'residual sugar'), (0.083641815432960981, 'density'), (0.078926396683801012, 'free sulfur dioxide'), (0.075584515251883769, 'total sulfur dioxide')]\n"
     ]
    }
   ],
   "source": [
    "#Top features from optimal gradient_search\n",
    "top_features = best_gradient_search.feature_importances_\n",
    "top_features_sorted = sorted(top_features, reverse=True)\n",
    "print('MSE (Test) for optimal Gradient Boost Parameters: {}'.format(mean_squared_error(y_test,best_gradient_search.predict(X_test))))\n",
    "print('R_2 (Test) for optimal Gradient Boost Parameters: {:.2%}'.format(r2_score(y_test , best_gradient_search.predict(X_test))))\n",
    "print('MSE (Training) for optimal Gradient Boost Parameters: {}'.format(mean_squared_error(y_train,best_gradient_search.predict(X_train))))\n",
    "print('R_2 (Training) for optimal Gradient Boost Parameters: {:.2%}'.format(r2_score(y_train , best_gradient_search.predict(X_train))))\n",
    "print()\n",
    "print('Top features for optimal Gradient Boost')\n",
    "print([(i,l) for i,l in zip(top_features_sorted,wine_columns[np.argsort(top_features)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Test) for default Gradient Boost Parameters: 0.4449806315561436\n",
      "R_2 (Test) for default Gradient Boost Parameters: 41.62%\n",
      "MSE (Training) for default Gradient Boost Parameters: 0.4008639077309373\n",
      "R_2 (Training) for default Gradient Boost Parameters: 49.35%\n",
      "\n",
      "Top features for default Gradient Boost\n",
      "[(0.11731126313227053, 'fixed acidity'), (0.099941114170622888, 'citric acid'), (0.097284767715744125, 'alcohol'), (0.093960396173075472, 'sulphates'), (0.091788446471496005, 'volatile acidity'), (0.090276037121821862, 'chlorides'), (0.086489814267343354, 'pH'), (0.084795433578980006, 'residual sugar'), (0.083641815432960981, 'density'), (0.078926396683801012, 'free sulfur dioxide'), (0.075584515251883769, 'total sulfur dioxide')]\n"
     ]
    }
   ],
   "source": [
    "#Default Gradient Search\n",
    "top_features = best_gradient_search.feature_importances_\n",
    "top_features_sorted = sorted(top_features, reverse=True)\n",
    "print('MSE (Test) for default Gradient Boost Parameters: {}'.format(mean_squared_error(y_test,default_gradient_search.predict(X_test))))\n",
    "print('R_2 (Test) for default Gradient Boost Parameters: {:.2%}'.format(r2_score(y_test , default_gradient_search.predict(X_test))))\n",
    "print('MSE (Training) for default Gradient Boost Parameters: {}'.format(mean_squared_error(y_train,default_gradient_search.predict(X_train))))\n",
    "print('R_2 (Training) for default Gradient Boost Parameters: {:.2%}'.format(r2_score(y_train , default_gradient_search.predict(X_train))))\n",
    "print()\n",
    "print('Top features for default Gradient Boost')\n",
    "print([(i,l) for i,l in zip(top_features_sorted,wine_columns[np.argsort(top_features)])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Gradient Boosting, the optimal model improves MSE by 22%. In addition, the most important variables are 1) fixed acidity 2) citric acid 3) alcohol . This differes from RF where the to three features are 1) Fixed Acidity 2) sulphates 3) pH. This difference is due to how these models are built, Gradient Boosting uses the gradient of the loss function while adaptive boosting assigns weights to both the weak learners and the individual data points within the weak learners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
