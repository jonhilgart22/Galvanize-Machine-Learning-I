{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 7.3 Introduction to Proximity Clustering\n",
    "\n",
    "## By the End of This Lesson You Will\n",
    "1. Be able to describe in your own words what proximity Clustering is and what DBSCAN does.\n",
    "2. Be able to describe verbally how the DBSCAN algorithm works.\n",
    "3. Be able to identify basic circumstances where DBSCAN (or other proximity clustering algorithms) would be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximity Clustering: A basic introduction\n",
    "\n",
    "Proximity clustering is less of an algorithm and more of an entire field, worthy of a 4 week survey.  It's more important that you understand why it exists and how to understand it. Since the publication of DBSCAN, something like 15 established upgraded versions of the original algorithm exist, i.e. AUTODBSCAN, VDBSCAN, PDBSCAN, DENCLUE, DBSCAN-DLP, GDBSCAN, OPTICS, and so on. Maybe 1000+ other publications have attempted improvements. Research on these algorithms continues at a rapid pace even today.\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "The proximity methods **deliberately eliminate any presumption of underlying distributions.** This makes them more of a relative of K-means than probabilistic methods. However unlike K-means, proximity methods do not operate under the assumption of clean decision boundaries between clusters. The fact that K-means uses a Voronoi tesselation makes it prefer spherical distributions. Nonlinear boundaries \n",
    "\n",
    "Proximity clustering allows the **clustered data to describe the clusters**. It does this with two parameters, $\\epsilon$ and minPoints. Unfortunately, it turns out that these end up being quite a restriction on the applicability of the method. \n",
    "\n",
    "### Hypothesis:\n",
    "\n",
    "\n",
    "\n",
    "We can't make any assumptions about the clusters present, or their density. There could be no clusters, or many.\n",
    "We describe the relationships in terms of *neighborhoods*.\n",
    "\n",
    "![dbscan_neighborhood](./images/DBSCAN_neighborhood.png)\n",
    "\n",
    "\n",
    "A neighborhood is defined as:\n",
    "\n",
    "1. Having a minimum number of points, minPoints, within it.\n",
    "2. Points that belong to that neighborhood are no more than $\\epsilon$ distance apart.\n",
    "3. If a point is within another point's neighborhood and that neighborhood is in a cluster, it is labeled as a member of that cluster.\n",
    "4. Points that don't belong to a cluster are labeled as noise.\n",
    "\n",
    "You will also see neighborhoods described in terms of their density. This is a reflection of the distance $\\epsilon$ and the minPoints number of points that belong to the neighborhood. Any cluster's minimum density is defined in terms of these two parameters: \n",
    "\n",
    "$$\\rho_{min} \\propto \\frac{minPoints}{\\epsilon}$$\n",
    "\n",
    "There are two types of relationships worth describing here: *density connectedness* and *density reachability*. If two points are density connected, they fall within each other's neighborhood. In the above diagram, the points *A* are density connected. The points *B* and *C* are density *reachable* but **not** density *connected* to points *A*. They will still be part of the *A* cluster, but these points will not become new centers of the same cluster themselves.\n",
    "\n",
    "## QUIZ:  \n",
    "\n",
    "What would happen if we added an additional groups of points within  to *B* but not *A*\n",
    "\n",
    "\n",
    "### Cost:\n",
    "\n",
    "There are no cost functions to minimize here. The algorithm proceeds until it can't change the labels on any of the points.\n",
    "\n",
    "### Optimization:\n",
    "\n",
    "There is no function to optimize but we can think of the clustering process as, where the process reaches an optimum when it can't change the labels on points any further.\n",
    "\n",
    "There are numerous ways to define neighborhoods and distances. The most sophisticated algorithms carefully define neighborhoods into regions of similar density and provide multiple ways to define a cluster.  \n",
    "\n",
    "Really, that's it!\n",
    "\n",
    "### Reasoning: \n",
    "\n",
    "We are looking for ways to define clusters of unknown shape and size. \n",
    "\n",
    "## QUIZ:\n",
    "Is there an upper limit on cluster density? How do you think variable density will affect the clustering process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "DBSCAN (Density Based Spatial Clustering Analysis with Noise) is the original version of the algorithm and the only one we expect you to learn in detail. As its acronym suggests, it is fast (SCAN) and capable of performing clustering on large databases (DB). One of the reasons it's so fast is because it's so simple. \n",
    "\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "    Pick an arbitrary point A from the dataset. This is marked as a visited point.\n",
    "    For all points within eps of A:\n",
    "        Mark if visited\n",
    "        Count all points within $\\epsilon$ of the original point. This is the neighborhood.\n",
    "        If there are more than minPoints points within a distance of $\\epsilon$ from that point (including the original point itself), label them all as members of a new cluster.\n",
    "        Recurse:\n",
    "            For all points in the neighborhood of every point in the cluster, check to see if they also have minPoints within an $\\epsilon$ neighborhood. Those that are, are also cluster members.\n",
    "    \n",
    "   If a point has been visited by the algorithm and doesn't belong to a cluster yet, it is marked as noise.\n",
    "\n",
    "\n",
    "\n",
    "###Demonstration:\n",
    "\n",
    "Heres a great demonstration page for both DBSCAN and K-Means:\n",
    "\n",
    "http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\n",
    "\n",
    "I encourage you to work with this after class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
