{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 7.2 Lecture\n",
    "\n",
    "### By the End of This Lecture You Will\n",
    "1. Be able to describe the PAM algorithm in your own words\n",
    "\n",
    "\n",
    "## PAM and Evaluation metrics\n",
    "\n",
    "K-means is a simple, fairly efficient, and commonly used clustering/partitioning algorithm.  It does however have some shortcomings that today's lecture will address. Mainly:\n",
    "\n",
    "* How to choose K?\n",
    "* Curse of dimensionality (using other similarity metrics)\n",
    "\n",
    "## K-medoids (PAM)\n",
    "\n",
    "Medoids are simply representative points of a dataset/cluster (loosely similar to the median but not).  Just like in k-means, there are a few different methods for solving the k-medoid problem.  For this class we will focus on Partitioning Around Medoids (PAM).\n",
    "\n",
    "The general [pseudo-code](https://en.wikipedia.org/wiki/K-medoids) is quite similar:\n",
    "\n",
    "## References \n",
    "https://web.stanford.edu/~hastie/Papers/gap.pdf - paper defining gap statistic  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Randomly select (without replacement) k of the n data points as the medoids m\n",
    "\n",
    "    Associate each data point to its closest medoid. (Euclidean distance, Manhattan distance or Minkowski distance)\n",
    "\n",
    "    While cost decreases:\n",
    "        For each medoid m\n",
    "            For each non-medoid data point o in the cluster\n",
    "                Try to swap m and o\n",
    "                    Compute the total cost of that configuration\n",
    "                    If cost has not increased with swap, implement swap\n",
    "                    Else, reject swap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference being step `3.`, instead of simply moving the centroid to the mean of the assigned points we have to compute the cost of assigning a different data point (of the cluster) to be the new medoid.  We pick the new assignment that minimizes the cost.\n",
    "\n",
    "## Choosing K\n",
    "\n",
    "One of the unsolved problems in unsupervised learning is determining the ideal value for certain hyperparameters.  For clustering (and topic modelling), this is often a parameter `K` that specifies apriori how many groups you believe to exist in the data.\n",
    "\n",
    "### Within cluster Dispersion\n",
    "\n",
    "Often the most basic metric of how well your clustering alorgithm is performing is the within-cluster dispersion.\n",
    "\n",
    "$$D_{r} = \\sum_{i, i' \\in C_{r}} d_{ii'}$$\n",
    "\n",
    "$$W_{k} = \\sum_{r = 1}^{k} \\frac{1}{2n_{r}}D_{r}$$\n",
    "\n",
    "The difficulty with this metric is that as you increase $k$, $W_{k}$ monotonically decreases.  So the optimal $k$ is when each data point is its own cluster center, but then this is quite useless.  So our goal then becomes to determine the smallest number of clusters $k$ with the lowest error...\n",
    "\n",
    "The most basic heuristic is to use a graphical method to determine when we are getting diminishing returns for every increase in $k$.  Aptly named the Elbow method, this technique looks for a kink in the error plot.\n",
    "\n",
    "![](http://i.stack.imgur.com/BzwBY.png)\n",
    "\n",
    "But as you can probably tell this is a very imprecise method.  How can we improve on this?\n",
    "\n",
    "### Silhouette\n",
    "\n",
    "Trying to provide a better metric, the Silhouette was described by Peter Rousseeuw in 1986.  It improves upon the Within Cluster Dispersion by also accounting for how similar each point is to every other point in its cluster (rather than just the centroid) as well as neighboring potential clusters.\n",
    "\n",
    "We define:\n",
    "\n",
    "$a(i)$ to be the average dissimilarity of point $i$ to every other data point in its cluster.\n",
    "\n",
    "$b(i)$ is the lowest dissimilarity of point $i$ to any _other_ cluster (neighboring clusters).\n",
    "\n",
    "$$ s(i) = 1 - \\frac{a(i)}{b(i)} \\qquad if \\qquad a(i) < b(i) $$\n",
    "$$ s(i) = 0 \\qquad if \\qquad a(i) = b(i) $$\n",
    "$$ s(i) = \\frac{b(i)}{a(i)} - 1 \\qquad  if \\qquad a(i) > b(i)$$\n",
    "\n",
    "The silhouette is defined to be greatest when $a(i) \\ll b(i)$.  And it is lowest when $b(i) \\gg a(i)$.\n",
    "\n",
    "With this definition we can see that the silhouette metric ranges from $-1 \\leq s(i) \\geq 1$.\n",
    "\n",
    "In contrast to the elbow method, the silhouette does not monotonically decrease as $k$ increases.  Because of this we can simply vary $k$ and look for when the silhouette is at a maximum.\n",
    "\n",
    "### Gap Statistic\n",
    "\n",
    "The Gap Statistic is another technique to estimate the optimal number of clusters in a data set.  It is somewhat unique in that it not only uses the data points, but compares them to a synthetic \"null\" distribution or random noise.  The Gap statistic can be thought of an automated way to determine the \"kink\" or \"elbow\" in the plot of the dispersion.\n",
    "\n",
    "Also, one of the most apparent weakness of the Silhouette (and most other metrics) is when the true data has no discernible clusters or only one true cluster. The Gap statistic solves some of the issues with the Silhouette by introducing the concept of a reference null dataset to compare performance against.  Let's see how it works!\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Just as with the other two methods we will be using the within cluster dispersion ($W_{k}$) to measure \"tightness\" of clusters. In addition to this measure, we will also utilize a reference null distribution of the data (pure noise) and compare the dispersion on our data vs the reference.  The ideal value of $k$ will then be the value for which $log(W_{k})$ drops furthest below this reference curve:\n",
    "\n",
    "![](images/gap.png)\n",
    "\n",
    "And we will define the Gap statistic to be at a maximum exactly when this is so:\n",
    "\n",
    "$$Gap_{n}(k) = E_{n}^{*}\\{log(W_{k})\\} - log(W_{k})$$\n",
    "\n",
    "Where $E_{n}^{*}$ denotes the expectation under a sample size of $n$ from the reference distribution.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Why use a reference distribution?  Think of this as a sort of *null hypothesis: There are no discernible clusters in the data*\n",
    "\n",
    "The Gap Statistic is a metric for us to disprove this hypothesis.  But sometimes we won't, and the fact that we have a null hypothesis will allow us to conclude that the ideal number of clusters is 1 (or really 0).\n",
    "\n",
    "Let us think about how the gap statistic changes as we increase $k$ (assuming $K$ actually clusters) in comparison to the reference distribution (since the reference distribution's dispersion will decrease at a rate proportional to $1/k$:\n",
    "\n",
    "$log(W_{k})$ should decrease *faster* than the expected rate of the null distribution as we increase $k s.t. k \\le K$ since we are in the steep section of the \"elbow\".\n",
    "\n",
    "and $log(W_{k})$ should decrease *slower* than the expected rate of the null distribution as we increase $k s.t. k > K$ since we are essentially adding a unnecessary centroid in the middle of an already decent cluster.\n",
    "\n",
    "#### Reference Distribution\n",
    "\n",
    "In our formulation, we assume a null model of a single component/cluster and reject it in favor of a $k$-component model ($k > 1$) if there is sufficient evidence contrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
