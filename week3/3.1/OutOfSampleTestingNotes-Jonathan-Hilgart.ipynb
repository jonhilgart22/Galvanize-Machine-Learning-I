{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Sample Testing Procedures\n",
    "\n",
    "Modern machine learning algorithms have regularization parameter(s) that allow you to adjust model complexity.  As you've seen by example, this process involves holding some data out of the training set and then adjusting the regularization (or complexity) parameter to get the best performance on out-of-sample data (the data held out of training).  Adjusting the complexity parameter looks a little like another fitting procedure.  How can you avoid over-fitting the test data.  For one thing, usually there aren't as many parameters involved in adjusting complexity as there are in the machine learning model itself.  But to be sure the best practice is to hold out another sample of the data in what is called a \"validation\" set.  The validation set is used last to validate that the error you've achieved on test by optimizing the regularization parameter is legitimate.  \n",
    "\n",
    "## Three methods for holding out test data and when you might use them.  \n",
    "\n",
    "Generally, people hold out a single set for validation data.  For test data three approaches are used.  \n",
    "1.  Fixed test data set\n",
    "2.  Bootstrap sampling\n",
    "3.  Cross-validation sampling.  \n",
    "\n",
    "### 1.  Holding out a fixed test data set. \n",
    "You've seen this approach in lectures and lab exercises.  It's the simplest process and often the best choice.  Define a fixed test data set and exclude it from model training.  Then apply the trained model(s) to the test data set and measure performance.  \n",
    "\n",
    "### 2.  Boostrap sampling\n",
    "Fixed test set sampling may not give you the richness of test examples that you need to be confident of your results.  In order to overcome this nagging feeling, you can take more than one random sample of test data.  For each sample run through the training and testing procedure and then average the results.  This approach gives you more samples of the test performance and correspondingly more confidence.  \n",
    "\n",
    "### 3.  K-fold cross validation\n",
    "K-fold cross validation is a systematic approach for generating enough test data to gather statistics on the errors your model generates.  The idea is that you divide the data set into K equal subsets.  The hold out on of the K subsets for testing and train on the remaining K-1.  After you've done this K times all of the original data will have been part of the test data.  The Figure below gives a graphical depiction of the process. \n",
    "\n",
    "![KFold Cross Validation](images/kfold.jpeg)\n",
    "\n",
    "## Which hold-out process should you use?  \n",
    "I almost never use bootstrap sampling.  I use either a fixed holdout set or K-fold cross validation.  The decision to use fixed holdout or K-fold cross validation depends on how much data is available and how long the model takes to train.  Usually those two things are related.  The main driver for using K-fold cross validation is when your data set is small enough that its size is constraining the performance that you can achieve.  You've seen examples where increasing the size of the data set improves the classification performance that you can achieve.  \n",
    "\n",
    "If holding out a significant amount of test and validation data impairs the performance of your model, then go with K-fold cross validation.  Doing K-fold cross validation lets you hold out a smaller amount of data during each of the K training runs yet still get good statistics on your model errors.  \n",
    "\n",
    "If you've got plenty of data, then use fixed holdout.  Having a large, representative test data set will yield representative error statistics.  It will help speed up training because you only have to train once (as opposed to K times with cross validation).  If you've got plenty of data training times can become an issue.  \n",
    "\n",
    "## Other considerations in test data selection\n",
    "1.  Representative feature statistics - It's well known that ML models perform better in the middle of the feature space than at the edges.  Check the test set histogram to see that it represents the data as a whole.  If your data has serious outlier issues you may have to resort to stratified sampling as described below. \n",
    "2.  Representative label statistics - For regression problems this means that the test set shows a similar label histogram as the training data.  For classification problems this means that the label-by-label fractions of the test population match those of the training population.  If the fractions vary too widely, you may have to resort to stratified sampling.  For a classification problem, stratified sampling means breaking the test data up by class label and pulling an appropriate number of samples from each class.  For a regression problem, stratified sampling means stratifying the data by label values (into quartiles or deciles, for example) and then sampling an appropriate number of test examples from each stratum.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
