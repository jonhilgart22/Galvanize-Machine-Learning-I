{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicum: Regularized Logistic Regression and ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration: Graduate School Admissions\n",
    "\n",
    "The data we will be using is admission data on Grad school acceptances.\n",
    "\n",
    "* `admit`: whether or not the applicant was admitted to grad school\n",
    "* `gpa`: undergraduate GPA\n",
    "* `GRE`: score of GRE test\n",
    "* `rank`: prestige of undergraduate school (1 is highest prestige)\n",
    "\n",
    "We will use the GPA, GRE, and rank of the applicants to try to predict whether or not they will be accepted into graduate school.\n",
    "\n",
    "Before we get to predictions, we should do some data exploration.\n",
    "\n",
    "**1)** Load the dataset into pandas: `data/grad.csv`.  \n",
    "\n",
    "\n",
    "**2)** Use the pandas `describe` method to get some preliminary summary statistics on the data. In particular look at the mean values of the features.  \n",
    "\n",
    "\n",
    "**3)** Use the pandas `crosstab` method to see how many applicants from each rank of school were accepted. You should get a dataframe that looks like this:\n",
    "\n",
    "    ```\n",
    "    rank    1   2   3   4\n",
    "    admit\n",
    "    0      28  ..  ..  ..\n",
    "    1      33  ..  ..  ..\n",
    "    ```\n",
    "\n",
    "\n",
    "**4)** Make a bar plot of the percent of applicants from each rank who were accepted. You can do `.plot(kind=\"bar\")` on a pandas dataframe.  \n",
    "    \n",
    "\n",
    "**5)** What does the distribution of the GPA and GRE scores look like? Do the distributions differ much?\n",
    "\n",
    "    Hint: Use the pandas `hist` method.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_df = pd.read_csv('data/grad.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load the dataset into pandas: data/grad.csv.\n",
    "\n",
    "2) Use the pandas describe method to get some preliminary summary statistics on the data. In particular look at the mean values of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317500</td>\n",
       "      <td>587.700000</td>\n",
       "      <td>3.389900</td>\n",
       "      <td>2.48500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466087</td>\n",
       "      <td>115.516536</td>\n",
       "      <td>0.380567</td>\n",
       "      <td>0.94446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.395000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre         gpa       rank\n",
       "count  400.000000  400.000000  400.000000  400.00000\n",
       "mean     0.317500  587.700000    3.389900    2.48500\n",
       "std      0.466087  115.516536    0.380567    0.94446\n",
       "min      0.000000  220.000000    2.260000    1.00000\n",
       "25%      0.000000  520.000000    3.130000    2.00000\n",
       "50%      0.000000  580.000000    3.395000    2.00000\n",
       "75%      1.000000  660.000000    3.670000    3.00000\n",
       "max      1.000000  800.000000    4.000000    4.00000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use the pandas crosstab method to see how many applicants from each rank of school were accepted. You should get a dataframe that looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rank</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>97</td>\n",
       "      <td>93</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rank    1   2   3   4\n",
       "admit                \n",
       "0      28  97  93  55\n",
       "1      33  54  28  12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(grad_df['admit'],grad_df['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Make a bar plot of the percent of applicants from each rank who were accepted. You can do .plot(kind=\"bar\") on a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11a42cb38>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ca10f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.crosstab(grad_df['admit'],grad_df['rank']).apply(lambda x:x/sum(x)).plot(kind='bar')\n",
    "plt.ylabel('admission rate')\n",
    "plt.title('Admission vs Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularized Logistic Regression\n",
    "\n",
    "Now we're ready to try to fit our data with Regularized Logistic Regression.  \n",
    "\n",
    "In this part, we will use the gradient descent algorithm to estimate the logistic regression coefficients. \n",
    "\n",
    "The hypothesis function of the logistic regression is defined as, \n",
    "\n",
    "$$ h(x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3})}} $$\n",
    "\n",
    "\n",
    "**1)** In `code/regularized_logistic_regression.py`, implement `hypothesis` and `predict` functions. `hypothesis` will calculate the value of the hypothesis function for the given coefficients. (Remember to add a column of 1's to the feature meatrix.) This returns float values between 0 and 1. `predict` will round these values so that you get a prediction of either 0 or 1. You can assume that the threshold we're using is 0.5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.array([1.4,2.1,3.5,4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.array([[random.choice(2,9)]]).reshape(3,3)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  1, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0,:]*np.array([1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f09db8c0a0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "np.concatenate( (r,np.ones(len(r))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47140452,  0.        ,  0.47140452])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(r,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hypothesis( X, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "\n",
    "        Calculate the predicted percentages (floats between 0 and 1)\n",
    "        for the given data with the given coefficients.\n",
    "        '''\n",
    "\n",
    "        # * The hypothesis function is going to return a proposed probability for each of the test data points\n",
    "        # * this will be done using the logistic function and the coefficients you've derived from the gradient descent\n",
    "        X = np.column_stack([np.ones(len(X)).reshape(len(X),1),X]) ## add in the coefficient term\n",
    "        print(X[:,0])\n",
    "\n",
    "\n",
    "\n",
    "        hypothesis = []\n",
    "#         for row_index,row in enumerate(X):\n",
    "        return   float(1/(1+np.exp(-(np.dot(X,coeffs)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function without regularization is given by  \n",
    "\n",
    "$$ J(\\beta) = - \\frac{1}{n} \\sum_{i = 1}^{n} \\left[ y_i log(h(x_i)) + (1 - y_i) log(1 - h(x_i)) \\right] $$  \n",
    "\n",
    "With Ridge regularization, the cost function becomes  \n",
    "\n",
    "$$ J(\\beta) = - \\frac{1}{n} \\sum_{i = 1}^{n} \\left[ y_i log(h(x_i)) + (1 - y_i) log(1 - h(x_i)) \\right] + \\frac{\\lambda}{2n} \\sum_{j = 1}^{p} \\beta_j^2$$  \n",
    "\n",
    "**2)** In `regularized_logistic_regression.py`, implement `cost_function` (without regularization) and `cost_regularized` (with regularization) functions. You should be able to use the `hypothesis` function you implemented above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The gradient of the cost function without regularization is given by  \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_j} J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h(x_i) - y_i \\right) x_{ij}$$  \n",
    "\n",
    "where $j = 0, 1, 2, 3$ and $x_{i0} = 1$ for all $i$ (the column of 1's in the feature matrix).  \n",
    "\n",
    "\n",
    "With regularization, the gradient is  \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_0} J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h(x_i) - y_i \\right) \\text{ when } j = 0$$  \n",
    "\n",
    "and  \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_j} J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h(x_i) - y_i \\right) x_{ij} + \\frac{\\lambda}{n} \\beta_j \\text{ for } j = 1, 2, 3$$\n",
    "\n",
    "\n",
    "\n",
    "**3)** In `regularized_logistic_regression.py`, implement `cost_gradient` (without regularization) and `gradient_regularized` (with regularization) functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to implement gradient descent. Below is psuedocode for the gradient descent algorithm. In this pseudocode and in our implementation, we will stop after a given number of iterations. Another valid approach is to stop once the incremental improvement in the optimization function (the cost function) is sufficiently small.\n",
    "\n",
    "    Gradient Descent:\n",
    "        input: J: optimization function (cost function)\n",
    "               alpha: learning rate\n",
    "               n: number of iterations\n",
    "        output: local minimum of optimization function J\n",
    "\n",
    "        initialize b (often as all 0's)\n",
    "        repeat for n iterations:\n",
    "            update b as b - alpha * gradient(J)\n",
    "\n",
    "You are going to be completing the code stub in `gradient_descent.py`.\n",
    "\n",
    "**4)** Start by taking a look at the starter code. Note how the `GradientDescent` object is initialized. It takes a cost function and a gradient function. We will pass it the functions that we wrote above. Here's example code of how we'll be able to run the Gradient Descent code.\n",
    "\n",
    "    ```python\n",
    "    from regularized_logistic_regression import cost_regularized, gradient_regularized\n",
    "    gd = GradientDescent(cost_regularized, gradient_regularized, predict)\n",
    "    gd.run(X, y)\n",
    "    print \"coeffs:\", gd.coeffs\n",
    "    predictions = gd.predict(X)\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "**5)** Implement the `run` method. Follow the pseudocode from above.  \n",
    "\n",
    "\n",
    "**6)** Implement the `predict` method. It should just call the `predict` function that was taken as a parameter.  \n",
    "\n",
    "\n",
    "**7)** Run your version of gradient descent on the admission data.\n",
    "\n",
    "    **Note:** If you're having trouble getting it to converge, run it for just\n",
    "    a few iterations and print out the cost at each iteration. The value should\n",
    "    be going down. If it isn't, you might need to decrease your learning rate.\n",
    "    And of course check your implementation to make sure it's correct. You can\n",
    "    also try printing out the cost every 100 iterations if you want to run it\n",
    "    longer and not get an insane amount of printing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "__author__ = \"Jonathan Hilgart\"\n",
    "\n",
    "\n",
    "class GradientDescent(object):\n",
    "    def __init__(self, fit_intercept=True, normalize=False, gradient=None, mu=None, sigma=None):\n",
    "        '''\n",
    "        INPUT: GradientDescent, boolean\n",
    "        OUTPUT: None\n",
    "        Initialize class variables. cost is the function used to compute the\n",
    "        cost.\n",
    "        '''\n",
    "\n",
    "        # * initialize and store None to the local copy of coefficients (weights) and alpha (input in the run function)\n",
    "        # * store boolean values for fit_intercept and normalize\n",
    "        # * store local copies of gradient, mu and sigma\n",
    "        self.coeffs=None\n",
    "        self.alpha=None\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.gradient = gradient\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, X, y, coeffs=None, alpha=0.01, num_iterations=100):\n",
    "\n",
    "\n",
    "        # * calculate normalization factors\n",
    "        calculate_normalization_factors(X)\n",
    "        # * run maybe_modify_matrix here and return the modified matrix\n",
    "\n",
    "        X= maybe_modify_matrix(X)\n",
    "\n",
    "        # * update the local copies of coefficents and alpha\n",
    "\n",
    "        \n",
    "\n",
    "        # * if there aren't input coefficients, initialize them to zero.\n",
    "        if coeffs ==None:\n",
    "            self.coeffs=np.zeros(len(X))\n",
    "        # * Recall that there should be as many coefficients as features.\n",
    "\n",
    "        # I give you this line here. if fit_intercept = True, set the intercept to 0\n",
    "        if self.fit_intercept:\n",
    "            self.coeffs = np.insert(self.coeffs, 0, 0)\n",
    "\n",
    "        # * for each of the num_iterations, update self.coeffs at each step.\n",
    "\n",
    "        for n in range(num_iterations):\n",
    "            #update b as b - alpha * gradient(J)\n",
    "            self.coeffs = self.coeffs- alpha*self.gradient(X,self.coeffs)\n",
    "\n",
    "\n",
    "    def calculate_normalization_factors(self, X):\n",
    "        '''\n",
    "        INPUT: GradientDescent, 2 dimensional numpy array\n",
    "        OUTPUT: None\n",
    "        Initialize mu and sigma instance variables to be the numpy arrays\n",
    "        containing the mean and standard deviation for each column of X.\n",
    "        '''\n",
    "\n",
    "        # * set the local copy of mu to be the average of each column of X\n",
    "        self.mu=np.mean(X,axis=0)\n",
    "\n",
    "        # * set the local copy of sigma to be the standard deviation of each column of X\n",
    "        self.sigma=np.std(X,axis=0)\n",
    "\n",
    "        # I give this to you - Don't normalize the intercept column\n",
    "        self.mu[self.sigma == 0] = 0\n",
    "        self.sigma[self.sigma == 0] = 1\n",
    "\n",
    "    def add_intercept(self, X):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array\n",
    "        OUTPUT: 2 dimensional numpy array\n",
    "        Return a new 2d array with a column of ones added as the first\n",
    "        column of X.\n",
    "        '''\n",
    "        # I give this line to you\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    def maybe_modify_matrix(self, X):\n",
    "        '''\n",
    "        INPUT: GradientDescent, 2 dimensional numpy array\n",
    "        OUTPUT: 2 dimensional numpy array\n",
    "        Depending on the settings, normalizes X and adds a feature for the\n",
    "        intercept.\n",
    "        '''\n",
    "        # I give this line to you\n",
    "\n",
    "        if self.normalize:\n",
    "            X = (X - self.mu) / self.sigma\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            return self.add_intercept(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from gradient_descent import GradientDescent\n",
    "from math import e\n",
    "\n",
    "__author__ = \"Jonathan Hilgart\"\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, fit_intercept = True, scale = True, norm = \"L2\"):\n",
    "        '''\n",
    "        INPUT: GradientDescent, function, function, function\n",
    "        OUTPUT: None\n",
    "\n",
    "        Initialize class variables. Takes three functions:\n",
    "        cost: the cost function to be minimized\n",
    "        gradient: function to calculate the gradient of the cost function\n",
    "        predict: function to calculate the predicted values (0 or 1) for\n",
    "        the given data\n",
    "        '''\n",
    "\n",
    "        self.cost_gradient_lasso = None\n",
    "        self.cost_gradient_ride = None\n",
    "\n",
    "        gradient_choices = {None: self.cost_gradient, \"L1\": self.cost_gradient_lasso, \"L2\": self.cost_gradient_ridge}\n",
    "\n",
    "\n",
    "        # * You'll need to initialize alpha, gama, and the weights (coefficients for the regression)\n",
    "        self.coeffs = None\n",
    "        self.alpha = None\n",
    "        self.gamma = None\n",
    "\n",
    "        # * You'll also need to store the number of iterations\n",
    "        \n",
    "        \n",
    "\n",
    "        # * You'll also need to store a boolean value for whether or\n",
    "        # * not you fit the intercept and scale\n",
    "\n",
    "        # I give these lines to you\n",
    "        if norm:\n",
    "            self.norm = norm\n",
    "            self.normalize = True\n",
    "        self.gradient = gradient_choices[norm]\n",
    "\n",
    "    def fit(self,  X, y, alpha=0.01, num_iterations=10000, gamma=0.1):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, float, int, float\n",
    "        OUTPUT: numpy array\n",
    "\n",
    "        Main routine to train the model coefficients to the data\n",
    "        the given coefficients.\n",
    "        '''\n",
    "\n",
    "        # * You'll need to store the dimensions of the input here\n",
    "        shape = np.shape(X)\n",
    "        \n",
    "        # * You'll also need to store the inputs for\n",
    "        # * alpha (the lagrange multiplier) and gamma\n",
    "        alpha = alpha\n",
    "        gamma =gamma\n",
    "\n",
    "        # * you'll need to update the stored value of num_iterations\n",
    "        \n",
    "        \n",
    "\n",
    "        # * randomly initialize the regression weights\n",
    "        \n",
    "        # start with zeros\n",
    "        self.coeffs = np.zeros(np.shape(X)[1])\n",
    "\n",
    "        # * Create an instance of GradientDescent\n",
    "        \n",
    "    \n",
    "        # * Run gradient descent\n",
    "\n",
    "        # * store the coefficients obtained from the gradient descent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "\n",
    "        Calculate the predicted values (0 or 1) for the given data with\n",
    "        the given coefficients.\n",
    "        '''\n",
    "\n",
    "        # * The hypothesis function wil predict probabilities (floats between 0 and 1) for each input.\n",
    "\n",
    "        # * you will need to be able to return a set of values between 0 and 1 for each of these.\n",
    "\n",
    "        # * return a bool (t/f) value for each percentage, such that percentages above 0.5 are\n",
    "        # * returned as 1, else 0.\n",
    "    \n",
    "        threshold = .5\n",
    "        bool_t = hypothesis(self, X, self.coeffs)>threshold \n",
    "        bool_f = [1 for i in bool_t if i == True ]\n",
    "        return bool_f\n",
    "\n",
    "\n",
    "\n",
    "    def hypothesis( X, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "\n",
    "        Calculate the predicted percentages (floats between 0 and 1)\n",
    "        for the given data with the given coefficients.\n",
    "        '''\n",
    "\n",
    "        # * The hypothesis function is going to return a proposed probability for each of the test data points\n",
    "        # * this will be done using the logistic function and the coefficients you've derived from the gradient descent\n",
    "\n",
    "\n",
    "#         for row_index,row in enumerate(X):\n",
    "        return   float(1/(1+np.exp(-(coeffs[0]+np.dot(X,coeffs[1:].T)))))\n",
    "\n",
    "\n",
    "\n",
    "    def cost_function(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: float\n",
    "\n",
    "        Calculate the value of the cost function for the data with the\n",
    "        given coefficients.\n",
    "        '''\n",
    "\n",
    "        # * call the hypothesis function to return a set of probabilities into a single vector h\n",
    "\n",
    "        # * return the log-likelihood for each of these predictions  1/M sum y_i*h_i + (1-y_i)*(1-h_i)\n",
    "        # * using the dot product will help\n",
    "\n",
    "\n",
    "        h = hypothesis(X,coeffs)\n",
    "\n",
    "        length = len(X)\n",
    "        log_likelihood = []\n",
    "\n",
    "        cost = [(-1/len(X))*sum(y*(log(h)) + (1-y)*(1-h))]\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def cost_gradient(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "\n",
    "        Calculate the gradient of the cost function at the given value\n",
    "        for the coeffs.\n",
    "\n",
    "        Return an array of the same size as the coeffs array.\n",
    "        '''\n",
    "\n",
    "        # This function is not used in the above code, just kept here for measuring the current state of cost\n",
    "\n",
    "        # * Calculate the hypothesis function with the input coefficients\n",
    "\n",
    "        # * Return Sum x_i*(y_i - h_i)\n",
    "        h = hypothesis(X,coeffs)\n",
    "\n",
    "        return [(1/len(X)) * sum(((h-y))*(X))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'cost_gradient_ridge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-622610490c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-163-c59b69a5d3cc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fit_intercept, scale, norm)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_gradient_ride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mgradient_choices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_gradient_lasso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_gradient_ridge\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'cost_gradient_ridge'"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ROC Curve \n",
    "\n",
    "One of the best ways to evaluate how a classifier performs is an ROC curve. (http://en.wikipedia.org/wiki/Receiver_operating_characteristic) \n",
    "\n",
    "![](images/roc_curve.png)\n",
    "\n",
    "To understand what is actually happening with an ROC curve, we can create one ourselves.  Here is pseudocode to plot it.\n",
    "\n",
    "The `probabilities` are the predicted probabilities in (0,1) returned from Logistic Regression. The standard default threshold is 0.5 where \n",
    "0-0.5 values are interpreted as the negative class ($y = 0$) and 0.5-1 values are predicted as the positive class ($y = 1$).\n",
    "\n",
    "The `labels` are the true/observed values of $y$.\n",
    "\n",
    "```\n",
    "function ROC_curve(probabilities, labels):\n",
    "    Sort instances by their prediction strength (the probabilities)\n",
    "    For every instance in increasing order of probability:\n",
    "        Set the threshold to be the probability\n",
    "        Set everything above the threshold to the positive class\n",
    "        Calculate the True Positive Rate (aka sensitivity or recall)\n",
    "        Calculate the False Positive Rate (1 - specificity)\n",
    "    Return three lists: TPRs, FPRs, thresholds\n",
    "```\n",
    "\n",
    "Recall that the *true positive rate* is\n",
    "\n",
    "```\n",
    " number of true positives     number correctly predicted positive\n",
    "-------------------------- = -------------------------------------\n",
    " number of positive cases           number of positive cases\n",
    "```\n",
    "\n",
    "and the *false positive rate* is\n",
    "\n",
    "```\n",
    " number of false positives     number incorrectly predicted positive\n",
    "--------------------------- = ---------------------------------------\n",
    "  number of negative cases           number of negative cases\n",
    "```\n",
    "\n",
    "You're going to be implementing the `roc_curve` function.\n",
    "\n",
    "Here's some example code that you should be able to use to plot the ROC curve with your function. This uses a fake dataset.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=2, n_samples=1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "tpr, fpr, thresholds = roc_curve(probabilities, y_test)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\")\n",
    "plt.title(\"ROC plot of fake data\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Write an ROC curve function to compute the above in `roc_curve.py`.\n",
    "\n",
    "    It should take as input the predicted probabilities and the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Run the above code to verify that it's working correctly. You can also validate your correctness against [scikit-learns built-in function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Make a plot of the ROC curve for the regularized logistic regression model from Part 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Is it possible to pick a threshold where TPR > 60% and FPR < 40%? What is the threshold?\n",
    "\n",
    "    Note that even if it appears to be in the middle of the graph it doesn't make the threshold 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** Say we are using this as a first step in the admission process. We want to weed out clearly unqualified candidates, but not reject too many candidates. What might be a good choice of threshold?\n",
    "\n",
    "    There isn't a single correct answer, so explain your choice."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
