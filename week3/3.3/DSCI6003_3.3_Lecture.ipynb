{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 3.3 Lecture\n",
    "\n",
    "\n",
    "## Introduction Validation\n",
    "\n",
    "### By the End of This Lecture You Will:\n",
    "\n",
    "1. Distinguish Validation from Evaluation\n",
    "2. Distinguish a classifier from a model\n",
    "3. Be able to describe the three different types of Cross-Validation\n",
    "4. Give case examples of where we might use the different types\n",
    "5. Be able to use the sklearn regression API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation vs. Evaluation\n",
    "\n",
    "These two concepts are often entangled in the mind of an inexperienced investigator. When we speak of models in machine learning, **evaluation** refers to the use of established metrics or measurements to characterize the expected future performance of a model.\n",
    "\n",
    "**Validation** has a specific scientific meaning. It refers to the verification of a prediction using other methods of observation(within established criteria of acceptance). For example if we obtain a result with one experiment, we must verify our conclusion of that experiment with other experiments. In order to make sure that our result is **valid** the additional studies on this predicition must be a representative sample of potential observers.\n",
    "\n",
    "In traditional science this is done with the process of **peer review.** Historically, scientists would publish results in **letters,** and other scientists of similar skill would repeat these experiments. Eventually the community would come to a consensus, and these final validated conclusions would be printed as an **article**. \n",
    "\n",
    "There are two types of validation: cross-validation, wherein the same experiment (or method or model) is repeated different times, and orthogonal validation, where an entirely different technique capable of obtaining approximately the same conclusion is used and compared against the first experiment. \n",
    "\n",
    "In machine learning, we always use **cross-validation every time a new model is constructed** in order to verify the model's consistency and performance over a fixed set of data. \n",
    "\n",
    "**Orthogonal validation** should be used if you are not certain about results or are using a new classifier not yet tested. The word \"orthogonal\" suggests that the method being used is entirely different from the original. [Benchmark data sets and standard classifiers](http://scikit-learn.org/stable/modules/clustering.html) are typiclly used to perform comparison, with an appropriate visualization technique. We teach you most of the standard classfiers in this class. When you implement your own classifier, you can use orthogonal methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "\n",
    "*We use cross validation as a means to get a sense of the error. Our final model will be built on all of the data so that we can have the best model possible.*\n",
    "\n",
    "\n",
    "#### Validation Set\n",
    "\n",
    "A validation (or hold out) set is a random sample of our data that we reserve for testing. We don't use this part of our data for building our model, just for assessing how well it did.\n",
    "\n",
    "* A typical breakdown is:\n",
    "    - 80% of our data in the training set (which we use the build the model)\n",
    "    - 20% of our data in the test set (which we use to evaluate the model)\n",
    "* Make sure that you randomize your data! It's really dangerous to pick the first 80% of the data to train and the last 20% to test since data is often sorted by a feature or the target! It would cause trouble if all the expensive houses were in your test set and never in the training set!\n",
    "* Concerns:\n",
    "    - *Variable:* Depending what random sample we get, we will get different values\n",
    "    - *Underestimating:* We are actually underestimating a little bit since we testing a model built on just 80% of the dataset instead of the whole 100%.\n",
    "\n",
    "\n",
    "#### KFold Cross Validation\n",
    "\n",
    "In K-fold cross validation, the data is split into **k** groups. One group\n",
    "out of the k groups will be the test set, the rest (**k-1**) groups will\n",
    "be the training set. In the next iteration, another group will be the test set,\n",
    "and the rest will be the training set. The process repeats for k iterations (k-fold).\n",
    "In each fold, a metric for accuracy will be calculated and\n",
    "an overall average of that metric will be calculated over k-folds. \n",
    "\n",
    "![KFold Cross Validation](images/kfold.jpeg)\n",
    "\n",
    "\n",
    "#### Stratified KFold Cross Validation\n",
    "\n",
    "    Stratified KFold works just like KFolds, except \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Leave P Out Cross Validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with sklearn\n",
    "\n",
    "There are several good modules with implementations of regression. We've\n",
    "used [statsmodels](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html).\n",
    "Today we will be using [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "[numpy](docs.­scipy.­org/­doc/­numpy/­reference/­generated/­numpy.­polyfit.­html) and\n",
    "[scipy](http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.linregress.html)\n",
    "also have implementations.\n",
    "\n",
    "Resources:\n",
    "* [sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* [sklearn example](http://scikit-learn.org/0.11/auto_examples/linear_model/plot_ols.html)\n",
    "\n",
    "For all `sklearn` modules, the `fit` method is used to train and the `score`\n",
    "method is used to test. You can also use the `predict` method to see the\n",
    "predicted y values.\n",
    "\n",
    "#### Example\n",
    "\n",
    "This is the general workflow for working with sklearn. Any algorithm we use from sklearn will have the same form.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load data from csv file\n",
    "df = pd.read_csv('data/housing_prices.csv')\n",
    "X = df[['square_feet', 'num_rooms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Run Linear Regression\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print \"Intercept:\", regr.intercept_\n",
    "print \"Coefficients:\", regr.coef_\n",
    "print \"R^2 error:\", regr.score(X_test, y_test)\n",
    "predicted_y = regr.predict(X_test)\n",
    "```\n",
    "\n",
    " `LinearRegression` is a class and you have to create an instance of it. \n",
    " If there are any parameters to the model, you should set them when you instantiate the object.\n",
    " For example, with `LinearRegression`, you can choose whether to normalize you data:\n",
    "\n",
    "```\n",
    "regr = LinearRegression(normalize=True)    # the default is False\n",
    "```\n",
    "\n",
    "\n",
    "You should call the `fit` method once. Here you give it the training data and it will train your model. Once you have that, you can get the coefficients for the equation (`intercept_` and `coef_`) and also get the score for your test set (`score` method). You can also get the predicted values for any new data you would like to give to the model (`predict` method).\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "Here's an example of cross-validation using kfold:\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn import cross_validation\n",
    "kf = cross_validation.KFold(X.shape[0], n_folds=5, shuffle=True)\n",
    "results = []\n",
    "for train_index, test_index in kf:\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X[train_index], y[train_index])\n",
    "    results.append(regr.score(X[test_index], y[test_index]))\n",
    "print \"average score:\", np.mean(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cross_validation, datasets, linear_model\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "\n",
    "lasso = linear_model.Lasso()\n",
    "alphas = np.logspace(-4, -.5, 30)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "for alpha in alphas: # for each one of the selected reg. parameters, CV the new model\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_validation.cross_val_score(lasso, X, y, cv=KFold(len(X), n_folds=5), n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "print(scores)\n",
    "print(scores_std)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)),\n",
    "             'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)),\n",
    "             'b--')\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "\n",
    "##############################################################################\n",
    "# Bonus: how much can you trust the selection of alpha?\n",
    "\n",
    "# To answer this question we use the LassoCV object that sets its alpha\n",
    "# parameter automatically from the data by internal cross-validation (i.e. it\n",
    "# performs cross-validation on the training data it receives).\n",
    "# We use external cross-validation to see how much the automatically obtained\n",
    "# alphas differ across different cross-validation folds.\n",
    "lasso_cv = linear_model.LassoCV(alphas=alphas)\n",
    "k_fold = cross_validation.KFold(len(X), 10)\n",
    "\n",
    "print(\"Answer to the bonus question:\",\n",
    "      \"how much can you trust the selection of alpha?\")\n",
    "print()\n",
    "print(\"Alpha parameters maximising the generalization score on different\")\n",
    "print(\"subsets of the data:\")\n",
    "for k, (train, test) in enumerate(k_fold):\n",
    "    lasso_cv.fit(X[train], y[train])\n",
    "    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n",
    "          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))\n",
    "print()\n",
    "print(\"Answer: Not very much since we obtained different alphas for different\")\n",
    "print(\"subsets of the data and moreover, the scores for these alphas differ\")\n",
    "print(\"quite substantially.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Trade Off\n",
    "\n",
    "We are assuming that our dataset is a random sample of the data. We want to make sure to answer this question: Does the model I'm building represent the whole population well? (i.e. not just the sample dataset that I have!)\n",
    "\n",
    "* Bias: Is the average of the residuals of the models close to the true model?\n",
    "\n",
    "    * A biased model would center around the incorrect solution. How you collect data can lead to bias (for example, you only get user data from San Francisco and try to use your model for your whole user base).\n",
    "    * High bias can also come from underfitting, i.e., not fully representing the data you are given.\n",
    "\n",
    "* Variance: Are all the models close together?\n",
    "\n",
    "    * The main contributor to high variance is insufficient data or that what you're trying to predict isn't actually correlated to your features.\n",
    "    * High variance is also a result of overfitting to your sample dataset.\n",
    "    \n",
    "Note that both high bias or high variance are bad. Note that high variance is worse than it sounds since you will only be constructing the model once, so with high variance there's a low probability that your model will be near the optimal one.\n",
    "\n",
    "Looking at this from a number of feature perspective:\n",
    "\n",
    "* Increasing the number of features means:\n",
    "    * Increase in variance\n",
    "    * Decrease in bias\n",
    "    \n",
    "A graph can make this more clear:\n",
    "    \n",
    "![bias variance](images/bias_variance_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
