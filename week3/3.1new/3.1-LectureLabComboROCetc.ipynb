{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Classifiers\n",
    "\n",
    "There are two reasons why you might want to evaluate the performance of a classifier.  One is that you need to characterize performance as part of optimizing model parameters (also known as \"training\" the model).  The other is that you need to guage the performance of a classifier in terms of the objectives of the problem you're trying to solve.  \n",
    "\n",
    "## Q\n",
    "Would the same method of performance evaluation work for both of these objectives?  \n",
    "\n",
    "## Goals\n",
    "\n",
    "* Given two ROC curves, pick the best curve/threshold for the problem at hand\n",
    "* When would you use F_beta vs. AUC\n",
    "* State one reason why the Youden index is useful and one reason it can be misleading\n",
    "* __Exercise (next week):__ Construct a Profit curve to evaluate the precision/recall trade-off\n",
    "\n",
    "## Review \n",
    "\n",
    "A classification problem is when we're trying to predict a discrete (categorical) outcome. We'll start with binary classification (i.e., yes/no questions).\n",
    "\n",
    "Here are some example questions:\n",
    "\n",
    "* Does a patient have cancer?\n",
    "* Will a team win the next game?\n",
    "* Will the customer buy my product?\n",
    "* Will I get the loan?\n",
    "\n",
    "In binary classification, we assign labels of 0 and 1 to our data.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Let's start by looking at an example. We're going to be using some NFL data. The x axis is the number of touchdowns scored by team over a season and the y axis is whether they lost or won the game indicated by a value of 0 or 1 respectively.\n",
    "\n",
    "![NFL data](images/nfl.png)\n",
    "\n",
    "So, how do we predict whether we have a win or a loss if we are given a score? Note that we are going to be predicting values between 0 and 1. Close to 0 means we're sure it's in class 0, close to 1 means we're sure it's in class 1, and closer to 0.5 means we don't know.\n",
    "\n",
    "If we use linear regression, we will certainly do better than randomly guessing, but it doesn't accurately represent the data:\n",
    "\n",
    "![NFL linear regression](images/linefit.png)\n",
    "\n",
    "So clearly a line is not the best way to model this data. So we need to find a better curve.\n",
    "\n",
    "## Measuring success \n",
    "\n",
    "So how do we measure how well our model does? Just like with regression, we need to split the data in a training set and a test set and measure our success based on how well it does on the test set.\n",
    "\n",
    "### Accuracy\n",
    "The simplest measure is **accuracy**. This is the number of correct predictions over the total number of predictions. It's the percent you predicted correctly. In `sklearn`, this is what the `score` method calculates.\n",
    "\n",
    "### Shortcomings of Accuracy\n",
    "Accuracy is often a good first glance measure, but it has many shortcomings. If the classes are unbalanced, accuracy will not measure how well you did at predicting. Say you are trying to predict whether or not an email is spam. Only 2% of emails are in fact spam emails. You could get 98% accuracy by always predicting not spam. This is a great accuracy but a horrible model!\n",
    "\n",
    "### Confusion Matrix\n",
    "We can get a better picture our model but looking at the confusion matrix. We get the following four metrics:\n",
    "\n",
    "* **True Positives (TP)**: Correct positive predictions\n",
    "* **False Positives (FP)**: Incorrect positive predictions (false alarm)\n",
    "* **True Negatives (TN)**: Correct negative predictions\n",
    "* **False Negatives (FN)**: Incorrect negative predictions (a miss)\n",
    "\n",
    "|            | Predicted Yes  | Predicted No   |\n",
    "| ---------- | -------------- | -------------- |\n",
    "| Actual Yes | True positive  | False negative |\n",
    "| Actual No  | False positive | True negative  |\n",
    "\n",
    "With logistic regression, we can visualize it as follows:\n",
    "\n",
    "![logistic confusion matrix](images/logistic.png)\n",
    "\n",
    "### Precision, Recall and F1\n",
    "\n",
    "![](http://i.stack.imgur.com/ysM0Z.png)\n",
    "Instead of accuracy, there are some other scores we can calculate:\n",
    "\n",
    "* **Precision**: A measure of how good your positive predictions are\n",
    "    ```\n",
    "    Precison = TP / (TP + FP)\n",
    "             = TP / (predicted yes)\n",
    "    ```\n",
    "* **Recall**: A measure of how well you predict positive cases. Aka *sensitivity*.\n",
    "    ```\n",
    "    Recall = TP / (TP + FN) \n",
    "           = TP / (actual yes)\n",
    "    ```\n",
    "* **F1 Score**: The harmonic mean of Precision and Recall\n",
    "    ```\n",
    "    F1 = 2 / (1/Precision + 1/Recall)\n",
    "       = 2 * Precision * Recall / (Precision + Recall)\n",
    "       = 2TP / (2TP + FN + FP)\n",
    "    ```\n",
    "\n",
    "Accuracy can also be written in this notation:\n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "```\n",
    "![](http://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q\n",
    "1.  What tools do you have at your disposal to change TP, FP, TN and FN?  \n",
    "2.  Some of you used linear regression to predict the 2-4 labels on the breast cancer data set.  What could you do with the output from that model to get your classifier to correctly classify every positive exaple?\n",
    "3.  In what ways would performance suffer?\n",
    "\n",
    "## Example 1.  - Calculating confusion matrix quantities\n",
    "Suppose that you're using linear regression to predict 0-1 labels.  You train your model and on the test data you get the following results.  Calculate the  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9d1ce6e64a9b>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9d1ce6e64a9b>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    threshold = ???\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Here are the results in list form, so it's easier to do some calculations\n",
    "labels = [0,0,0,0,0,1,1,1,1,1]\n",
    "predictions = [-0.8, -0.4, 0.0, 0.4, 0.8, 0.2, 0.6, 1.0, 1.4, 1.8]\n",
    "threshold = ???\n",
    "\n",
    "#calculate #'s for TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. - Suppose some mistakes are more expensive than others\n",
    "Now suppose that the cost of a false positive is 1 and the cost of a false negative is 10, while true positive and false positive cost zero.  How much does your predictor cost with a threshold value of 0.5?  Generate costs for threshold values of 0.0, 0.25, 0.5, 0.75 and 1.0.  Which one yields the minimum cost?  Explain any shift in the threshold from 0.5.  \n",
    "\n",
    "\n",
    "## Lab Exercise 1.  \n",
    "Use one of the KNN model that you trained on breast cancer data.  That model is trying to predict the numbers 2 or 4 corresponding to no cancer or cancer.  Suppose you used the number 3.0 to form no/yes prediction based on your numerical prediction.  \n",
    "1.  Use a threshold value of 3.0 on the breast cancer knn model you built.  Compare the predictions to the actual labels and generate 2x2 matrix of predictions versus actual - TP, FP, TN, FN.  \n",
    "2.  Suppose there's a cost of $200k for false negative and a cost of $2k for false positive.  Calculate costs associated with your classifier and experiment with some values to get a feel for the best threshold value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves \n",
    "\n",
    "One of the best ways to evaluate how a classifier performs is an ROC curve. (http://en.wikipedia.org/wiki/Receiver_operating_characteristic) \n",
    "\n",
    "![](images/roc_curve.png)\n",
    "\n",
    "ROC curve plots true positive rate (TPR) versus false positive rate (FPR).  TPR and FPR are defined in terms of quantities that you can read out of the confusion matrix.  Here are the definitions.   to plot it.\n",
    "\n",
    "\n",
    "Recall that the true positive **rate** is\n",
    "\n",
    "```\n",
    " number of true positives     number correctly predicted positive\n",
    "-------------------------- = -------------------------------------\n",
    " number of positive cases           number of positive cases\n",
    "```\n",
    "\n",
    "and the false positive **rate** is\n",
    "\n",
    "```\n",
    " number of false positives     number incorrectly predicted positive\n",
    "--------------------------- = ---------------------------------------\n",
    "  number of negative cases           number of negative cases\n",
    "```\n",
    "\n",
    "\n",
    "### Example 3.  \n",
    "1. Write an ROC curve function to compute several points on the ROC curve for the toy problem above. Then plot the result (TPR versus FPR).  \n",
    "2.  What happens if you choose a threshold value and generate hard 0-1 labels before calculating the ROC curve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are the results in list form, so it's easier to do some calculations\n",
    "labels = [0,0,0,0,0,1,1,1,1,1]\n",
    "predictions = [-0.8, -0.4, 0.0, 0.4, 0.8, 0.2, 0.6, 1.0, 1.4, 1.8]\n",
    "threshold = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Thought Lab \n",
    "When you use KNN as your prediction algorithm you have two choices on this binary classification problem.  You can use regression version of KNN or classification version of KNN.  \n",
    "1.  What happens if you take as output from KNN classifier, the majority class?  What your alternative?\n",
    "2.  What is the difference in the way the labels are calculated?\n",
    "2.  What is the difference in the ROC curve?\n",
    "\n",
    "## Lab Exercise 2.  \n",
    "1.  Plot a ROC curve for the breast cancer data using whatever predictions are handy for you.  \n",
    "\n",
    "\n",
    "### Youden Index\n",
    "\n",
    "Youden's Index (sometimes called J statistic) is similar to the F1 score in that it is a single number that describes the performance of a classifier.\n",
    "\n",
    "$$J = Sensitivity + Specificity - 1$$\n",
    "\n",
    "$$where$$\n",
    "\n",
    "$$Sensitivity = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$Specificity = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "![](http://i.stack.imgur.com/ysM0Z.png)\n",
    "\n",
    "The J statistic ranges from 0 to 1:\n",
    "* 0 indicating that the classifier does no better than random\n",
    "* 1 indicating that the test performed perfectly\n",
    "\n",
    "It can be thought of as an improvement on the F1 score since it takes into account all of the cells in a confusion matrix.  It can also be used to find the optimal threshold for a given ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
