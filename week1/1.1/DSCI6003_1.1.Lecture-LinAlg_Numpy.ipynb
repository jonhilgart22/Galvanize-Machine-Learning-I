{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra and Machine Learning\n",
    "* Ranking web pages in order of importance\n",
    "    * Solved as the problem of finding the eigenvector of the page score matrix\n",
    "* Dimensionality reduction - Principal Component Analysis\n",
    "* Movie recommendation\n",
    "    * Use singular value decomposition (SVD) to break down user-movie into user-feature and movie-feature matrices, keeping only the top $k$-ranks to identify the best matches\n",
    "* Topic modeling\n",
    "    * Extensive use of SVD and matrix factorization can be found in Natural Language Processing, specifically in topic modeling and semantic analysis\n",
    "*All of the compute-intensive operations in deep learning are matrix manipulation\n",
    "    *Forward inference and backward propagation of gradients\n",
    "    *Convolution and de-convolution can be understood as matrix ops\n",
    "    *Recurrance function in RNN is non-linearity applied element-by-element to matrix-vector op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector can be represented by an array of real numbers\n",
    "\n",
    "$$\\boldsymbol{x} = [x_1, x_2, \\ldots, x_n]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, a vector specifies the coordinates of the tip of the vector if the tail were placed at the origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The norm of a vector $\\boldsymbol{x}$ is defined by\n",
    "\n",
    "$$||\\boldsymbol{x}|| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.47722557505\n",
      "5.47722557505\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "print np.sqrt(np.sum(x**2))\n",
    "print np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$ of the same length $(n)$, then the _dot product_ is give by\n",
    "\n",
    "$$\\boldsymbol{x} \\cdot \\boldsymbol{y} = x_1y_1 + x_2y_2 + \\cdots + x_ny_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathbf{x} \\cdot \\mathbf{y} = 0$ then $x$ and $y$ are *orthogonal* (aligns with the intuitive notion of perpindicular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1, 2])\n",
    "v = np.array([-2, 1])\n",
    "np.dot(w,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The norm squared of a vector is just the vector dot product with itself\n",
    "$$\n",
    "||x||^2 = x \\cdot x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.norm(x)**2\n",
    "print np.dot(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\boldsymbol{x}$ is centered at zero, $\\frac{||\\boldsymbol{x}||^2}{n}$ is the _variance_ of $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25\n",
      "1.25\n"
     ]
    }
   ],
   "source": [
    "x_centered = x - np.mean(x)\n",
    "print np.linalg.norm(x_centered)**2/len(x_centered)\n",
    "print np.var(x_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between two vectors is the norm of the difference.\n",
    "$$\n",
    "d(x,y) = ||x-y||\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "y = np.array([4,5,6,7])\n",
    "print np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cosine Similarity_ is the cosine of the angle between the two vectors give by\n",
    "\n",
    "$$cos(\\theta) = \\frac{\\boldsymbol{x} \\cdot \\boldsymbol{y}}{||\\boldsymbol{x}|| \\text{ } ||\\boldsymbol{y}||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96886393162696616"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "y = np.array([5,6,7,8])\n",
    "np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both $\\boldsymbol{x}$ and $\\boldsymbol{y}$ are zero-centered, this calculation is the _correlation_ between $\\boldsymbol{x}$ and $\\boldsymbol{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5 -0.5  0.5  1.5]\n",
      "[-1.5 -0.5  0.5  1.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99999999999999978"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_centered = x - np.mean(x)\n",
    "print x_centered\n",
    "y_centered = y - np.mean(y)\n",
    "print y_centered\n",
    "np.dot(x_centered,y_centered)/(np.linalg.norm(x_centered)*np.linalg.norm(y_centered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combinations of Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _linear combination_ of a collection of vectors $(\\boldsymbol{x}_1,\n",
    "                                                    \\boldsymbol{x}_2, \\ldots,\n",
    "                                                    \\boldsymbol{x}_m)$ \n",
    "is a vector of the form\n",
    "\n",
    "$$a_1 \\cdot \\boldsymbol{x}_1 + a_2 \\cdot \\boldsymbol{x}_2 + \n",
    "\\cdots + a_m \\cdot \\boldsymbol{x}_m$$\n",
    "                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $n \\times p$ matrix is an array of numbers with $n$ rows and $p$ columns:\n",
    "\n",
    "$$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\cdots & x_{np} \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$n$ = the number of subjects  \n",
    "$p$ = the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "\n",
    "In order to multiply two matrices, they must be _conformable_ such that the number of columns of the first matrix must be the same as the number of rows of the second matrix.\n",
    "\n",
    "Let $X$ be a matrix of dimension $n \\times k$ and let $Y$ be a matrix of dimension $k \\times p$, then the product $XY$ will be a matrix of dimension $n \\times p$ whose $(i,j)^{th}$ element is given by the dot product of the $i^{th}$ row of $X$ and the $j^{th}$ column of $Y$\n",
    "\n",
    "$$\\sum_{s=1}^k x_{is}y_{sj} = x_{i1}y_{1j} + \\cdots + x_{ik}y_{kj}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "$$XY \\neq YX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X$ and $Y$ are square matrices of the same dimension, then the both the product $XY$ and $YX$ exist; however, there is no guarantee the two products will be the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Properties of Matrices\n",
    "1. If $X$ and $Y$ are both $n \\times p$ matrices,\n",
    "then $$X+Y = Y+X$$\n",
    "2. If $X$, $Y$, and $Z$ are all $n \\times p$ matrices,\n",
    "then $$X+(Y+Z) = (X+Y)+Z$$\n",
    "3. If $X$, $Y$, and $Z$ are all conformable,\n",
    "then $$X(YZ) = (XY)Z$$\n",
    "4. If $X$ is of dimension $n \\times k$ and $Y$ and $Z$ are of dimension $k \\times p$, then $$X(Y+Z) = XY + XZ$$\n",
    "5. If $X$ is of dimension $p \\times n$ and $Y$ and $Z$ are of dimension $k \\times p$, then $$(Y+Z)X = YX + ZX$$\n",
    "6. If $a$ and $b$ are real numbers, and $X$ is an $n \\times p$ matrix,\n",
    "then $$(a+b)X = aX+bX$$\n",
    "7. If $a$ is a real number, and $X$ and $Y$ are both $n \\times p$ matrices,\n",
    "then $$a(X+Y) = aX+aY$$\n",
    "8. If $a$ is a real number, and $X$ and $Y$ are conformable, then\n",
    "$$X(aY) = a(XY)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Transpose\n",
    "\n",
    "The transpose of an $n \\times p$ matrix is a $p \\times n$ matrix with rows and columns interchanged\n",
    "\n",
    "$$\n",
    "X^T =\n",
    "  \\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{p1} & x_{p2} & \\cdots & x_{pn} \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Transpose\n",
    "1. Let $X$ be an $n \\times p$ matrix and $a$ a real number, then \n",
    "$$(aX)^T = aX^T$$\n",
    "2. Let $X$ and $Y$ be $n \\times p$ matrices, then\n",
    "$$(X \\pm Y)^T = X^T \\pm Y^T$$\n",
    "3. Let $X$ be an $n \\times k$ matrix and $Y$ be a $k \\times p$ matrix, then\n",
    "$$(XY)^T = Y^TX^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector in Matrix Form\n",
    "A column vector is a matrix with $n$ rows and 1 column and to differentiate from a standard matrix $X$ of higher dimensions can be denoted as a bold lower case $\\boldsymbol{x}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} =\n",
    "  \\begin{bmatrix}\n",
    "    x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy, when we enter a vector, it will not normally have the second dimension, so we can reshape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "(4,)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "print x\n",
    "print x.shape\n",
    "x = x.reshape(4,1)\n",
    "print x\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a row vector is generally written as the transpose\n",
    "\n",
    "$$\\boldsymbol{x}^T = [x_1, x_2, \\ldots, x_n]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]]\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "x_T = x.transpose()\n",
    "print x_T\n",
    "print x_T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$ of the same length $(n)$, then the _dot product_ is give by matrix multiplication\n",
    "\n",
    "$$\\boldsymbol{x}^T \\boldsymbol{y} =   \n",
    "    \\begin{bmatrix} x_1& x_2 & \\ldots & x_n \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    y_{1}\\\\\n",
    "    y_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    y_{n}\n",
    "  \\end{bmatrix}  =\n",
    "  x_1y_1 + x_2y_2 + \\cdots + x_ny_n$$\n",
    "  \n",
    "## In-class exercise - Differences between numpy vectors and arrays\n",
    "The distinction between a vector (aka oriented vector) and an array (aka unoriented vector) can be important.  Numpy will let you get away with using arrays and will try to determine what you intended.  Most of the time it will do what you intended, sometimes it won't.  Tensorflow will not let you use arrays, but will demand that you specify two (or more) indices for vectors.  The following sequence of exercise will help you understand why these distinctions are important.  \n",
    "1.  Form two arrays or unoriented vectors.  These are numpy arrays whose shape is something like (n,).  Take their dot product.  \n",
    "2.  Form two vectors or oriented vectors.  These will have shapes like (n,1) or (1,n).  Take their dot product.  If you used the same numeric values in as in 1, the answers will be the same.  \n",
    "3.  Reverse the order of the multiplication in 2 (reverse the order of the arguments in the dot() function).  Are the two vectors still conformable?  What shape do you expect the answer to have?  What shape does it have?  \n",
    "4.  Generate the same result as in 3, using unoriended vectors (arrays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse of a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of a square $n \\times n$ matrix $X$ is an $n \\times n$ matrix $X^{-1}$ such that \n",
    "\n",
    "$$X^{-1}X = XX^{-1} = I$$\n",
    "\n",
    "Where $I$ is the identity matrix, an $n \\times n$ diagonal matrix with 1's along the diagonal. \n",
    "\n",
    "If such a matrix exists, then $X$ is said to be _invertible_ or _nonsingular_, otherwise $X$ is said to be _noninvertible_ or _singular_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 1 6]\n",
      " [7 3 3]\n",
      " [1 5 0]]\n",
      "[[-0.16666667  0.33333333 -0.16666667]\n",
      " [ 0.03333333 -0.06666667  0.23333333]\n",
      " [ 0.35555556 -0.37777778  0.15555556]]\n",
      "[[  1.00000000e+00   0.00000000e+00  -5.55111512e-17]\n",
      " [  5.55111512e-17   1.00000000e+00  -1.94289029e-16]\n",
      " [  0.00000000e+00   5.55111512e-17   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randint(0, 10, size=(3, 3))\n",
    "A_inv = np.linalg.inv(A)\n",
    "print A\n",
    "print A_inv\n",
    "print A.dot(A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Inverse\n",
    "1. If $X$ is invertible, then $X^{-1}$ is invertible and\n",
    "$$(X^{-1})^{-1} = X$$\n",
    "2. If $X$ and $Y$ are both $n \\times n$ invertible matrices, then $XY$ is invertible and\n",
    "$$(XY)^{-1} = Y^{-1}X^{-1}$$\n",
    "3. If $X$ is invertible, then $X^T$ is invertible and\n",
    "$$(X^T)^{-1} = (X^{-1})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Matrices\n",
    "\n",
    "Let $X$ be an $n \\times n$ matrix such than $X^TX = I$, then $X$ is said to be orthogonal which implies that $X^T=X^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, two $n \\times 1$ vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$ are said to be orthogonal if\n",
    "\n",
    "$$\\boldsymbol{x}^T \\boldsymbol{y} = 0 $$\n",
    "\n",
    "An orthogonal matrix does not change the length of a vector that it multiplies.  It only changes the orientation of the vector.  It rotates the vector.  The proof is simple:  http://www.cse.psu.edu/~b58/cse456/lecture3.pdf \n",
    "\n",
    "### Exercise: Mental floss 1:  \n",
    "Prove that the rows (or columns) of an orthogonal matrix are orthogonal to one another. \n",
    "\n",
    "### Symmetric and Anti-Symmetric Matrices\n",
    "\n",
    "A matrix is called symmetric if $X^T = X$ and it's called anti-symmetric if $X^T = -X$.  \n",
    "\n",
    "### Exercise: Mental floss 2\n",
    "Prove that every matrix can be composed as the sum of a symmetric and an anti-symmetric matrix.  Ask for hint, if you need.\n",
    "\n",
    "### Positive (and negative) definite (and semi-definite) matrices (and quadratic forms).\n",
    "These concepts and properties are important in optimization problems.  You'll learn that much of machine learning is about optimization.  With deep networks the optimization problems become wildly more intense and reinforcement learning dials the problems up several notches from there.  \n",
    "\n",
    "A quadratic form is an expression of form $v^TXv$ where v is a column vector and X is a square matrix.  A matrix X is positive definite if $v^TXv\\, >\\,0 \\,\\forall v\\ne0$.  X is negative definite if -X is positive definite.  X is positive semi-definite if $v^TXv\\, \\geq\\,0 \\,\\forall v\\ne0$.  Sometimes you will see notation $X\\, >\\, 0$ with X a matrix.  That is a common shorthand for saying that X is positive definite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A system of equations of the form:\n",
    "\\begin{align*}\n",
    "    a_{11}x_1 + \\cdots + a_{1n}x_n &= b_1 \\\\\n",
    "    \\vdots \\hspace{1in} \\vdots \\\\\n",
    "    a_{m1}x_1 + \\cdots + a_{mn}x_n &= b_m \n",
    "\\end{align*}\n",
    "can be written as a matrix equation:\n",
    "$$\n",
    "A\\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "and hence, has solution\n",
    "$$\n",
    "\\mathbf{x} = A^{-1}\\mathbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue and Eigenvectors\n",
    "\n",
    "\n",
    "Let $\\bf A$ be a given nonzero square matrix of dimension $n \\times n$. Consider the following equation:\n",
    "\n",
    "$${\\bf A}{\\bf x} = \\lambda {\\bf x}$$\n",
    "\n",
    "This equation is called an _eigenvalue equation_. Here $\\bf A$ is a given square matrix, $\\bf x$ is an unknown vector, and $\\lambda$ is an unknown scalar. The problem of finding  $\\lambda$'s and  nonzero ${\\bf x}$'s that satisfy the eigenvalue equation is called the _eigenvalue problem_.\n",
    "\n",
    "### Eigenvalues of symmetric matrices\n",
    "Symmetric matrices have a relatively simple structure that can sometimes be useful for doing calculations and for visualizing the properties and effects of operations and equations.  The baasic notion is that by changing your coordinate frame you can view a symmetric matrix as being diagonal.  Multiplication by a diagonal matrix is much easier to contemplate than by a more general matrix.  In mathematical terms every square symmetric matrix matrix X can be decomposed as follows $X\\, =\\, U^T\\Lambda U$ where U and $\\Lambda$ square and the same dimension as X, U is orthogonal and $\\Lambda$ is diagonal with the eigenvalues of X along the diagonal.  \n",
    "\n",
    "As discussed above, an orthogonal matrix rotates vectors.  A diagonal matrix stretches or contracts the elements of a vector independently of one another. \n",
    "\n",
    "### Exercises Mental floss 3\n",
    "1.  Suppose you're given a diagonal matrix D.  Write out the expression for the quadratic form $v^TDv$ in terms of the componenets of v $v_1,\\,v_2,\\,etc$, the diagonal elements of D, $d_{1,1},\\, d_{2,2},\\,etc$.  What is the shape of the curve satisfying $v^TDv = c$ for some positive number c? \n",
    "2.  (More difficult) Suppose you're given a symmetric matrix X.  What requirements on the eigenvalues of X are necessary and sufficient for X > 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.38196601  2.61803399]\n",
      "[[-0.85065081 -0.52573111]\n",
      " [ 0.52573111 -0.85065081]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 1], [1, 2]])\n",
    "vals, vecs = np.linalg.eig(A)\n",
    "print vals\n",
    "print vecs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3249197   0.20081142]\n",
      "[-0.3249197   0.20081142]\n"
     ]
    }
   ],
   "source": [
    "lam = vals[0]\n",
    "vec = vecs[:,0]\n",
    "print A.dot(vec)\n",
    "print lam * vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis\n",
    "\n",
    "If a set of linearly independent vectors $B = \\{{\\bf b}_1, {\\bf b}_2, .... {\\bf b}_n\\}$ span $V$ then the set $B$ is said to be a __basis set__, or __basis vectors__, or simply __basis__, for the space. Thus, any vector $\\bf v$ in $V$ can be expressed as a linear combination of the elements of $B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Space (Kernel)\n",
    "\n",
    "The **Null Space** or more often **Kernel** of a $m \\times n$ matrix $A$ is the set of all $n$ dimensional vectors $\\vec{x}$ such that:\n",
    "\n",
    "$A\\vec{x} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of a few numpy matrix and array functions.  \n",
    "\n",
    "### Summary functions\n",
    "Calculate row-wise (or column-wise) means for a matrix (or higher rank tensor).  \n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\n",
    "\n",
    "### Scalar functions applied to arrays (vector, matrix, tensor)\n",
    "Scalar functions are functions that take a single real number as input and give a single real number as output.  There are several things you might have in mind by an expression like $X^2$ where X is a matrix.  Supposing X is square, you might mean XX where the two matrices are multiplied as matrices.  In this class an other machine learning classes $X^2$ means a matrix where each i,j element is the square of the corresponding element of X.  Something like $X^2_{i,j}\\,=\\,X_{i,j}*X_{i,j}$.  You'll see this notation used very frequently in deep learning literature.  A very common operation in a neural net is something like $tahn(W_xX + W_hh)$ where $W_x$ and $W_h$ are weight matrices, X is a vector of inputs and h is the output of a previous layer in the network.  In this example the hyperbolic tangent function is applied element by element to the vectors that result from the matrix multiplications.  \n",
    "\n",
    "You can use ordinary python math library functions like math.tanh() to do these calculations, but you'd have to build a double for loop to do it, making your code uglier and wasting compute cycles.  Instead there are numpy versions of household functions that you can use instead.  Heres exponential for example.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\n",
    "\n",
    "Not coincidentally there are also tensor versions of household scalar functions in neural net languages like Theano and Tensorflow.  H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
