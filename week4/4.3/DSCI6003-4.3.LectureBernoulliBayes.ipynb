{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 4.3 Lecture\n",
    "\n",
    "## Review of Naive Bayes, Bayesian Updates and Bernoulli Bayes\n",
    "\n",
    "### By the End of This Lecture You Will\n",
    "\n",
    "1. Be more familiar with the theory behind Naive Bayes\n",
    "2. Be comfortable with Bayesian Updates and be able to develop your own concept of them\n",
    "\n",
    "## References\n",
    "Stanford nlp class has some nice references for naive Bayes and related techniques.  Here are some that relate directly.  \n",
    "http://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html  \n",
    "http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html  \n",
    "http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html  \n",
    "\n",
    "Other refs (including video)\n",
    "http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/  - blog post  \n",
    "http://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf  - comparison paper  \n",
    "https://www.youtube.com/watch?v=pc36aYTP44o - Jurafsky video  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes: A Review\n",
    "\n",
    "### Why is Naive Bayes \"Naive?\"\n",
    "1. Assumes that all conditional distributions $P(x_{i}|c_{k})$ are independent. Please see chain rule.\n",
    "2. We assume that we have a perfect sample of the input distributions.\n",
    "3. We ignore the effects of the joint.\n",
    "\n",
    "### A slightly more precise description of Multinomial NB:\n",
    "\n",
    "The total likelihood of observing a given histogram (distribution) is the joint probability of every bin for a given event $x_i$ found $n_i$ times in the histogram:\n",
    "\n",
    "$P(C_{k}|{\\bf{x}}) = \\dfrac{(\\sum_{i}n_{i})!}{\\prod_{i}n_{i}!}\\prod_{i}P(x_{i}|C_{k})^{n_i}P(C_{k})$\n",
    "\n",
    "To use it as a NB maximum-likelihood method, we just dump the whole coefficient in front and take the log-likelihood:\n",
    "\n",
    "$$P(C_{k}|{\\bf{x}}) = log(P(C_{k})) + \\sum_{i}n_{i}log(P(x_{i}|C_{k}))$$\n",
    "\n",
    "Note that this is another linear equation:\n",
    "\n",
    "$$ log(P(C_{k})) + \\sum_{i}n_{i}log(P(x_{i}|C_{k})) = b + w^{T}\\bf{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Bayesian Updating\n",
    "\n",
    "Note that the way we have formulated Naive Bayes assumes that we have a perfectly complete sample, meaning that we do not update our prior beliefs. What does it mean if we think that new data (our test data) should affect our predictions? This falls within the realm of *bayesian updating*. The mathematics of bayesian updating is described directly from Bayes' law, and only requires that we can recognize it and execute it where needed.\n",
    "\n",
    "### Formulation:\n",
    "\n",
    "Suppose that a process generates a set of observations (events) $E_{m}$ and that we have a set of **models** $M_{k}$ describing this process. Each model is represented (defined) by the conditional probability tables $P(E_{m}|M_{k})$ and our **belief that this model is correct** is represented by the prior $P(M_{k})$.\n",
    "\n",
    "**Before** the first inference step, the prior distribution of our set of models $M_{k}$, $\\{P(M_m)\\}$ is a set of initial prior probabilities, obtained perhaps from expert knowledge. These must sum to 1, but are **otherwise arbitrary.**\n",
    "\n",
    "**After** each inference step $t+1$, we update the conditional probability tables $P(E_{m}|M_{k})$ by *multiplying them by our current belief in that model* $P(M_{k,t+1})$:\n",
    "\n",
    "$$P(M_{k,t+1}|E_{m,t+1}) = \\dfrac{P(E_{m,t}|M_{k,t})}{\\sum_{k=0}^{K}P(E_{m,t}|M_{k,t})}\\ P(M_{k,t+1})$$\n",
    "\n",
    "Please note that the denominator is simply a normalization factor to ensure that we have a distribution over each of the $K$ models.\n",
    "\n",
    "Okay, so how do we go about getting this number $P(M_{k,t+1})$? Precisely, it is this prior that is allowed to change with respect to each new event $E_{m,t+1}$. We get this update from the way we formulate the problem. After multiplication by the proper likelihood (comes from a table worked out beforehand), the posterior becomes a new prior.\n",
    "\n",
    "This process is called **Bayesian updating.**\n",
    "\n",
    "\n",
    "Although obtaining the update formula can be very complex indeed, but doesn't need to be for you to understand how it works conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Cookies in Bowls\n",
    "\n",
    "We have two bowls with cookies in them. One bowl (B1) has 5 chocolate chip cookies and 15 plain cookies in it, while the other bowl (B2) has 10 of each. We randomly draw a cookie from one of the bowls and we see that it is a plain cookie. How likely is it that the cookie came from the first bowl? \n",
    "\n",
    "#### Formulation: \n",
    "\n",
    "We seek $P(B1|plain)$. Our hypothesis priors on the bowls before pulling cookies are:\n",
    "\n",
    "$$P(B1) == P(B2) == 0.5$$\n",
    "\n",
    "We also have knowledge beforehand that the likelihood of it being a plain cookie, given bowl 1 is $$P(plain|B1) = \\dfrac{15}{20} = \\dfrac{3}{4} = 0.75$$\n",
    "and the likelihood of it being a chocolate cookie given bowl 1 is $$P(choc|B1) = \\dfrac{5}{20} = \\dfrac{1}{4} = =0.25$$\n",
    "Conversely, $$P(choc|B2) = P(plain|B2) = \\dfrac{1}{2} = 0.5$$\n",
    "\n",
    "\n",
    "Using Bayes' Law upon the first draw $t=1$: \n",
    "\n",
    "$$P_{t=1}(B1|plain) = \\dfrac{\\dfrac{3}{4} \\times \\dfrac{1}{2}}{\\dfrac{3}{4} \\times \\dfrac{1}{2}+\\dfrac{1}{2} \\times \\dfrac{1}{2}} = \\dfrac{\\dfrac{3}{8}}{\\dfrac{5}{8}}  = \\dfrac{3}{5} = 0.6$$\n",
    "\n",
    "So now our model that $B1$ is the bowl from which we are pulling cookies becomes this value! Thus $P(B1) = 0.6$ This is our belief that this bowl is the origin of the cookie. \n",
    "\n",
    "Let's continue the experiment to $t=2$. Now we reach over and grab another cookie. Again it is plain. Therefore we do the update again, based on the fact that we found the posterior for P(B1|plain):\n",
    "\n",
<<<<<<< HEAD
    "$P_{t=2}(B1|plain) = \\dfrac{0.75 \\times 0.6}{0.75 \\times 0.6 + 0.5 \\times .4} = 0.69$\n",
=======
    "$P_{t=2}(B1|plain) = \\dfrac{0.75 \\times 0.6}{0.75 \\times 0.6 + 0.5 \\times 0.4} = 0.69$\n",
>>>>>>> upstream/master
    "\n",
    "Again, this posterior becomes the new prior for the next update $t=3$. **Note that the likelihoods are NOT probability functions and they do not change between steps!!**\n",
    "\n",
    "### QUIZ (important):\n",
    "\n",
    "What would the update be if we pulled a chocolate cookie? Recall that we are updating $P(B1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{t=3}(B1|choc) = \\dfrac{P(choc|B1)P_{t=2}(B1)}{P(choc)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Coin Flips - Is the coin fair?\n",
    "\n",
    "Here I have constructed for you a simulation of coin flips, representing the prior in full as a distribution over the entire probability space $0 < P(heads) < 1$. Thus in this case, each subdivision $i$ of the probability space counts as a single model of $M_{i} = P(heads) = p_{i}$.\n",
    "\n",
    "Suppose we don't know if the coin we are flipping is fair. Say we know nothing about it. We can construct a probability table that reflects a continuum of hypotheses about the probability $p(H)$, ranging from 0 to 1. Note that the probability we will give tails is $1-p(H)$. This probability table then represents the probability distribution of the prior hypothesis, and we update it as above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def coin_study(divs, flip_sequence = ['T','F']):\n",
    "    \n",
    "    # generate the hypothesis function with a given set of subdivisions for confidence\n",
    "    \n",
    "    hypothesis = [1.0]*divs\n",
    "    \n",
    "    def normalize(hypothesis):\n",
    "        p_sum = sum(hypothesis)\n",
    "        for confidence in xrange(divs):\n",
    "            pos = hypothesis[confidence]\n",
    "            hypothesis[confidence] = pos/p_sum\n",
    "\n",
    "    def update_success(hypothesis):\n",
    "        # This function hypothesizes that the coin gives only heads p = 'H'\n",
    "        for confidence in xrange(divs):\n",
    "            hypothesis[confidence] = hypothesis[confidence]*(confidence/float(divs)) # h(p,t+1) = p(d)*p(H)\n",
    "        normalize(hypothesis)\n",
    "\n",
    "    def update_failure(hypothesis):\n",
    "        # This function hypothesizes that the coin gives only tails: 1-p = 'T'\n",
    "        for confidence in xrange(divs):\n",
    "            hypothesis[confidence] = hypothesis[confidence]*(1.0-(confidence/float(divs))) # h(p,t+1) = (1-p(d))*p(H)\n",
    "        normalize(hypothesis)\n",
    "\n",
    "    for flip in flip_sequence:\n",
    "        if flip == \"H\":\n",
    "            update_success(hypothesis)\n",
    "        else:\n",
    "            update_failure(hypothesis)\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(np.arange(divs),hypothesis)\n",
    "    ax.set_xlim([0,divs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFUBJREFUeJzt3X+Mndld3/H3J95sYUOLRWk3xB7kCDuVl5bGqHWskpRB\npJUzBbu0Uher1SJLzVoCky1SqyX/sO4/Rf2DdmtZstziRIaGuG0gwagGJ0GZNhKSNy67ziZrpx5a\nC9tLnIiwAbZCssm3f9zHm7vXnrk/5s7emTnvlzSyn+c559zzXF9/7rnnzvOcVBWSpM3tTbPugCRp\n7Rn2ktQAw16SGmDYS1IDDHtJaoBhL0kNGBr2SfYnuZrkWpKnlylzvDt+Ocmevv1PJXkxyReSPDXN\njkuSRrdi2CfZApwA9gOPAYeS7B4oswDsrKpdwJPAyW7/Xwf+OfC3gb8J/EiS75n6GUiShho2st8L\nLFXV9aq6A5wFDg6UOQCcAaiqi8DWJG8FdgMXq+rPqurPgf8B/KOp9l6SNJJhYb8NuNG3fbPbN6zM\n24AXgfck+Y4kjwD/ANi+uu5Kkibx0JDjo95LIfdVrLqa5N8CnwReBZ4HvjFe9yRJ0zAs7G8Bc33b\nc/RG7iuV2d7to6o+BHwIIMm/AX5/8AGSeHMeSZpAVd030F7OsLC/BOxKsgN4GXgcODRQ5hxwFDib\nZB/wSlXdBkjyV6vqK0m+G/gx4F2r7bBWluRYVR2bdT82C5/P6fG5nK5xB8orhn1V3U1yFLgAbAFO\nV9WVJEe646eq6nyShSRL9KZrDvc18bEkfxm4A/xkVf3xOJ3TxjXpJzbf+KW1MWxkT1X9JvCbA/tO\nDWwfXabu311V77TBjZv35ry0VoaGvTacxdU24Kj8dRZn3YFNZHHWHWhZZr14SZLapCGxYfXCfvxR\nef+/4zTakLS8cbPTe+NIUgMMe0lqgHP2WpdWc/2FU0HS/Qx7rWOT5L05Lz2I0ziS1ADDXpIaYNhL\nUgOcs99kvCBK0oMY9puStymQ9HpO40hSAwx7SWqAYS9JDTDsJakBhr0kNWBo2CfZn+RqkmtJnl6m\nzPHu+OUke/r2fzDJF5O8mORXkvyFaXZeWkmSmuRn1v2W1sKKYZ9kC3AC2A88BhxKsnugzAKws6p2\nAU8CJ7v9O4D3A99fVX+D3rKGPz7l/ktD1Jg/0uY0bGS/F1iqqutVdQc4CxwcKHMAOANQVReBrUke\nBf6Y3tqzjyR5CHgEuDXNzkuSRjMs7LcBN/q2b3b7hpapqq8BvwD8PvAy8EpVfXp13ZUkTWLYFbSj\nfq697xLMJN8D/AtgB/B14L8l+adV9ZEHlD3Wt7lYVYsjPq4kNSHJPDA/af1hYX8LmOvbnqM3cl+p\nzPZu3zzwO1X1h11Hfw34O8B9YV9Vx8bptCS1phsEL97bTvLMOPWHTeNcAnYl2ZHkYeBx4NxAmXPA\nE92D76M3XXMb+BKwL8m3JgnwXuClcTonSZqOFUf2VXU3yVHgAr3fpjldVVeSHOmOn6qq80kWkiwB\nrwKHu2MvJPklem8Y3wB+F/iPa3gukqRlpGq2v26WpLy97vT0fk98/Lte9v8brIc2Jqs/jTbi7Z61\nIYybnV5BK0kNMOwlqQGGvSQ1wJWq1pHV3JfFeWZJKzHs153JvpSUpJU4jSNJDTDsJakBTuNIK5j0\nexS/Q9F6Y9hLQ41/YZa03jiNI0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSA4aGfZL9Sa4m\nuZbk6WXKHO+OX06yp9v315I83/fz9SQfmPYJSJKGW/GiqiRbgBP01o+9BXwuybmqutJXZgHYWVW7\nkrwLOAnsq6ovAfeC/01d/Y+vzWlIklYybGS/F1iqqutVdQc4CxwcKHMAOANQVReBrUkeHSjzXuD3\nqurGFPosSRrTsLDfBvQH9M1u37Ay2wfK/DjwK5N0UJK0esPujTPqTUEGbwbyWr0kDwM/Cjxwvr8r\nc6xvc7GqFkd8XElqQpJ5YH7S+sPC/hYw17c9R2/kvlKZ7d2+e94H/K+q+upyD1JVx4b2VJIa1g2C\nF+9tJ3lmnPrDpnEuAbuS7OhG6I8D5wbKnAOe6B58H/BKVd3uO34I+Og4nZIkTdeKI/uqupvkKHAB\n2AKcrqorSY50x09V1fkkC0mWgFeBw/fqJ3kLvS9n379mZyBJGipVE69xPZ0OJOVCDz29hTImW4P2\n3nM4WRt53WIb66GNzfRcSGth3Oz0ClpJaoBhL0kNMOwlqQGuQSutoUkXLAcXLdd0GfbSmpvsi2Zp\nmpzGkaQGGPaS1ACncaZo0vlZ52YlrTXDfurGvwBHktaa0ziS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w\n7CWpAYa9JDVgaNgn2Z/kapJrSR64aHiS493xy0n29O3fmuRjSa4kealbtlCS9AZbMeyTbAFOAPuB\nx4BDSXYPlFkAdlbVLuBJ4GTf4f8AnK+q3cD3AVem2HdJ0oiGjez3AktVdb2q7gBngYMDZQ4AZwCq\n6iKwNcmjSb4deE9Vfag7dreqvj7d7kuSRjEs7LcBN/q2b3b7hpXZDrwd+GqSDyf53ST/Kckjq+2w\nJGl8w+6NM+qNXgZv8FJd298PHK2qzyV5FvhZ4Ofuq5wc69tcrKrFER9XkpqQZB6Yn7T+sLC/Bcz1\nbc/RG7mvVGZ7ty/Azar6XLf/Y/TC/j5VdWzE/kpSk7pB8OK97STPjFN/2DTOJWBXkh1JHgYeB84N\nlDkHPNE9+D7glaq6XVVfBm4keUdX7r3AF8fpnCRpOlYc2VfV3SRHgQvAFuB0VV1JcqQ7fqqqzidZ\nSLIEvAoc7mvip4GPdG8UvzdwTJL0BknVxOshT6cDSW2WxTt6i5eMfz/7e+c/Wf1ptJHXLaCyHtrY\nLM/FNM5DepBxs9MraCWpAa5UJa1zky53CS55qW8y7KUNYbKpIOkep3EkqQGGvSQ1wLCXpAYY9pLU\nAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1IChYZ9kf5KrSa4leXqZMse745eT\n7Onbfz3J55M8n+S5aXZckjS6Fe96mWQLcILekoK3gM8lOVdVV/rKLAA7q2pXkncBJ4F93eEC5qvq\na2vSe0nSSIaN7PcCS1V1varuAGeBgwNlDgBnAKrqIrA1yaN9x73PqiTN2LCw3wbc6Nu+2e0btUwB\nn05yKcn7V9NRSdLkhi1eMuqKCcuN3t9dVS8n+SvAp5JcrarPjt49SdI0DAv7W8Bc3/YcvZH7SmW2\nd/uoqpe7P7+a5OP0poXuC/skx/o2F6tqcYS+S1IzkswD8xPXr1p+8J7kIeBLwA8DLwPPAYce8AXt\n0apaSLIPeLaq9iV5BNhSVX+S5C3AJ4F/XVWfHHiMsVZIX896a4WOu3xcXlsndLL602gjr1urdD20\nsVmei9mdx+vb0OYzbnauOLKvqrtJjgIXgC3A6aq6kuRId/xUVZ1PspBkCXgVONxVfyvwa0nuPc5H\nBoNekvTGWHFk/4Z0YJ2M7Hujp/FtjlGgI/sH1Z9GG47stVamOrJvz/j/qSVpIzDspQZM45OrNjbD\nXmqGn1xb5o3QJKkBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJek\nBhj2ktQAw16SGjA07JPsT3I1ybUkTy9T5nh3/HKSPQPHtiR5PslvTKvTkqTxrBj2SbYAJ4D9wGPA\noSS7B8osADurahfwJHByoJmngJeYbKkdSdIUDBvZ7wWWqup6Vd0BzgIHB8ocAM4AVNVFYGuSRwGS\nbAcWgF/Em2NL0swMC/ttwI2+7ZvdvlHL/HvgXwHfWEUfJUmrNCzsR516GRy1J8mPAF+pqucfcFyS\n9AYatizhLWCub3uO3sh9pTLbu33/GDjQzel/C/CXkvxSVT0x+CBJjvVtLlbV4ki9l6RGJJkH5ieu\nX7X84D3JQ8CXgB8GXgaeAw5V1ZW+MgvA0apaSLIPeLaq9g2084PAv6yqH33AY9R6WNS4tyDz+Gt0\n9vd9tW1MVn8abUz3PKbRxmZ5LmZ3HtNoIy44vo6Nm50rjuyr6m6So8AFYAtwuqquJDnSHT9VVeeT\nLCRZAl4FDi/X3KidkiRN14oj+zekA47s190Ibj20sVmeC0f2WivjZqdX0EpSA4Z9QStJwL1PB+Pz\n08H6YNhLGsP4U0FaH5zGkaQGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2\nktQAw16SGmDYS1IDDHtJasDQsE+yP8nVJNeSPL1MmePd8ctJ9nT7viXJxSQvJHkpyc9Pu/OSpNGs\nGPZJtgAngP3AY8ChJLsHyiwAO6tqF/AkcBKgqv4M+KGqeifwfcAPJXn39E9BkjTMsJH9XmCpqq5X\n1R3gLHBwoMwB4AxAVV0EtiZ5tNv+f12Zh+mtYfu1aXVckjS6YWG/DbjRt32z2zeszHbofTJI8gJw\nG/hMVb20uu5KkiYxbKWqUZelGVyOpgCq6s+Bdyb5duBCkvmqWryvcnKsb3PxQWUkqWVJ5oH5SesP\nC/tbwFzf9hy9kftKZbZ3+15TVV9P8t+BvwUsDj5IVR0brbuS1KZuELx4bzvJM+PUHzaNcwnYlWRH\nkoeBx4FzA2XOAU90D74PeKWqbif5ziRbu/3fCvw94PlxOidJmo4VR/ZVdTfJUeACvS9YT1fVlSRH\nuuOnqup8koUkS8CrwOGu+ncBZ5K8id6byi9X1W+v2ZlIkpaVqnFXi59yB5KqqpkvQZ+kRv+K4rVa\n9Pd9tW1MVn8abUz3PKbRxmZ5LmZ3HtNoY/qvC03PuNk5bM5ekqai92YxGd8wVs+wl/QGmuwTilZv\nU4T9pCMGRwuSWrEpwr5n/LlESWqFd72UpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0k\nNcCwl6QGGPaS1ADDXpIaYNhLUgNGCvsk+5NcTXItydPLlDneHb+cZE+3by7JZ5J8MckXknxgmp2X\nJI1maNgn2QKcAPYDjwGHkuweKLMA7KyqXcCTwMnu0B3gZ6rqe4F9wE8N1pUkrb1RRvZ7gaWqul5V\nd4CzwMGBMgeAMwBVdRHYmuTRqvpyVb3Q7f9T4Arwtqn1XpI0klHCfhtwo2/7ZrdvWJnt/QWS7AD2\nABfH7aQkaXVGWbxk1FVBBlcDea1ekm8DPgY81Y3wX18xOda3uVhViyM+piQ1Ick8MD9p/VHC/hYw\n17c9R2/kvlKZ7d0+krwZ+FXgP1fVJx70AFV1bMT+SlKTukHw4r3tJM+MU3+UaZxLwK4kO5I8DDwO\nnBsocw54ouvAPuCVqrqdJMBp4KWqenacjknSoCQ16c+s+z5rQ0f2VXU3yVHgArAFOF1VV5Ic6Y6f\nqqrzSRaSLAGvAoe76j8A/DPg80me7/Z9sKp+a+pnIqkRk+S2a06narZveEmqqlb1L9F71x5/wfH+\nx10PbUxWfxpt+Fw8qP402pjdeUyjjc35XGwW42anV9BKUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJek\nBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhowUtgn2Z/kapJr\nSZ5epszx7vjlJHv69n8oye0kL06r05I0qVaXNRwa9km2ACeA/cBjwKEkuwfKLAA7q2oX8CRwsu/w\nh7u6krRO1Jg/G98oI/u9wFJVXa+qO8BZ4OBAmQPAGYCqughsTfLWbvuzwB9Nr8uSpHGNEvbbgBt9\n2ze7feOWkSTNyEMjlBn1M8zgwrcjf/ZJcqxvc7GqFketK0ktSDIPzE9af5SwvwXM9W3P0Ru5r1Rm\ne7dvJFV1bNSyktSibhC8eG87yTPj1B9lGucSsCvJjiQPA48D5wbKnAOe6DqwD3ilqm6P0xFJ0toZ\nGvZVdRc4ClwAXgL+S1VdSXIkyZGuzHng/yRZAk4BP3mvfpKPAr8DvCPJjSSH1+A8JEkrSNVsf60o\nSVXV4Hz/2G2M/+tRof9x10Mbk9WfRhs+Fw+qP402Znce02jD5+JB9deLcbPTK2glqQGGvSQ1wLCX\npAYY9pLUAMNekhpg2EtSA0a5glaS1GfSWx7P8tc3DXtJmsj4v6s/S07jSFIDDHtJaoBhL0kNMOwl\nqQGGvSQ1wLCXpAYY9pLUgKFhn2R/kqtJriV5epkyx7vjl5PsGaeuJGntrRj2SbYAJ4D9wGPAoSS7\nB8osADurahfwJHBy1LqS1KIkNenPpI85bGS/F1iqqutVdQc4CxwcKHMAOANQVReBrUneOmJdSWpU\nTfAzuWFhvw240bd9s9s3Spm3jVBXkvQGGBb2o76VrKu1GSVJrzfsRmi3gLm+7Tl6I/SVymzvyrx5\nhLrA5HeQG2hl/Br3Pe56aGOy983VtuFzsVz9abQxm/OYRhs+F8vVn0Yb0ziP0Q0L+0vAriQ7gJeB\nx4FDA2XOAUeBs0n2Aa9U1e0kfzhC3XW3YrskbUYrhn1V3U1yFLgAbAFOV9WVJEe646eq6nyShSRL\nwKvA4ZXqruXJSJIeLFVTmEGRJK1rM72C1ouupivJ9SSfT/J8kudm3Z+NJMmHktxO8mLfvu9I8qkk\n/zvJJ5NsnWUfN5Jlns9jSW52r8/nk+yfZR83kiRzST6T5ItJvpDkA93+kV+jMwt7L7paEwXMV9We\nqto7685sMB+m91rs97PAp6rqHcBvd9sazYOezwL+Xff63FNVvzWDfm1Ud4CfqarvBfYBP9Xl5civ\n0VmO7L3oam34hfcEquqzwB8N7H7tgsHuz3/4hnZqA1vm+QRfnxOpqi9X1Qvd3/8UuELvuqWRX6Oz\nDPtRLtjSeAr4dJJLSd4/685sAo9W1e3u77eBR2fZmU3ip7t7aJ12Wmwy3W847gEuMsZrdJZh7zfD\n0/cDVbUHeB+9j3nvmXWHNovq/SaDr9nVOQm8HXgn8AfAL8y2OxtPkm8DfhV4qqr+pP/YsNfoLMN+\nlAu2NIaq+oPuz68CH6c3VabJ3e7u80SS7wK+MuP+bGhV9ZXqAL+Ir8+xJHkzvaD/5ar6RLd75Nfo\nLMP+tQu2kjxM76KrczPsz4aW5JEkf7H7+1uAvw+8uHItDXEO+Inu7z8BfGKFshqiC6N7fgxfnyNL\nEuA08FJVPdt3aOTX6Ex/zz7J+4Bn+eZFVz8/s85scEneTm80D72L5T7i8zm6JB8FfhD4Tnpznz8H\n/DrwX4HvBq4D/6SqXplVHzeSBzyfzwDz9KZwCvi/wJG++WatIMm7gf8JfJ5vTtV8EHiOEV+jXlQl\nSQ1wWUJJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSA/4/SvfKi+sqNfAAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106d0ba10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coin_study(20,['H','T','T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beta Distribution\n",
    "\n",
    "As can be seen above, the flowing changes to the prior don't resemble any kind of normal distribution, however, they can be characterized with a different PDF, the **beta** distribution.\n",
    "\n",
    "$$\\beta_{PDF} = \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} = \\dfrac{1}{B(\\alpha+\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
    "\n",
    "This denominator is a normalization factor, and simply is a sum over the whole distribution, such that the probabilities sum to 1.\n",
    "\n",
    "The [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is used to characterize random variables bound to intervals. The distribution is not an exact description of any kind of probability, but rather a clean way to characterize a family of related distribution behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bayes\n",
    "\n",
    "As an analogy to the above example, we can use Bernoulli Naive Bayes if we are only concerned with whether or not a feature appears at all in the data (a binary variable), rather than the number of counts of each feature, in determining a class.( i.e. is or is not a customer, did or did not buy something, etc.)\n",
    "\n",
    "$$n_{i} \\in \\{0,1\\}$$\n",
    "\n",
    "$$P(C_{k}|{\\bf{x}}) = log(P(C_{k})) + \\sum_{i}n_{i}log(P(x_{i}|C_{k})) + (1-n_{i})(log(1 - P(x_{i}|C_{k})))$$\n",
    "\n",
    "Note that it has the special benefit of explicitly modeling the absence of terms. It is not the same as setting multinomial bayes with a binary variable.\n",
    "\n",
    "### Comparing Multinomial vs. Bernoulli Bayes:\n",
    "\n",
    "\n",
    "'''\n",
    "    class MultinomialBayes:\n",
    "\n",
    "        (...)\n",
    "\n",
    "        def fit(D, C): \n",
    "            # Count all the documents in D, save to N\n",
    "            # Extract the vocabulary in D (get corpus), save to V\n",
    "            # for each class c in C\n",
    "                # count all documents in D belonging to that class, Nc\n",
    "                # update the prior[c] with Nc\n",
    "                # store all text belonging to all documents in that class (bag of words) to T\n",
    "                # for each word v in V\n",
    "                    # count each occurrence of word in T (with Counter or equivalent) store to Tcv\n",
    "                # for each word v in V\n",
    "                    # add in the count to the conditional probability table P(v, c) +=$ (T_{cv}+ a)/sum(T_{cv'})+a*N$\n",
    "            # store V, N, P(v,c)\n",
    "\n",
    "         def predict(newDoc, C, V, P(v,c)):\n",
    "             # Extract vocab V from newDoc\n",
    "             # Extract word counts T from newDoc\n",
    "             # for each class c in C\n",
    "                 # initialize score[c] = log(prior[c])\n",
    "                 # for each occurrence of word t in T:\n",
    "                    # score[c] += log(P(v, c))\n",
    "             # return argmax(score[c])\n",
    "         \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "    class BernoulliBayes:\n",
    "\n",
    "        (...)\n",
    "\n",
    "        def fit(D, C): \n",
    "            # Count all the documents in D, save to N\n",
    "            # Extract the vocabulary in D (get corpus), save to V\n",
    "            # for each class c in C\n",
    "                # count all documents in D belonging to that class, Nc\n",
    "                # update the prior[c] with Nc\n",
    "                # store all text belonging to all documents in that class (bag of words) to T\n",
    "                # for each word v in V\n",
    "                    # count all docs in D containing v, Ncv\n",
    "                    # add in the count to the conditional probability table P(v, c) += (N_{cv} + 1)/(Nc + 2)\n",
    "            # store V, N, P(v,c)\n",
    "\n",
    "         def predict(newDoc, C, V, P(v,c)):\n",
    "             # Extract vocab V from newDoc\n",
    "             # Extract word counts T from newDoc\n",
    "             # for each class c in C\n",
    "                 # initialize score[c] = log(prior[c])\n",
    "                 # for all v in V:\n",
    "                     # if v is in T:\n",
    "                        # score[c] += log(P(v, c))\n",
    "                     # else:\n",
    "                        # score[c] += (log(1 - P(v, c))\n",
    "             # return argmax(score[c])\n",
    "         \n",
    "'''\n",
    "\n",
    "\n",
    "### QUIZ:\n",
    "Where might we use Bernoulli Bayes? As a **reach goal**, implement bernoulli bayes in your Naive Bayes code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
