{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Place import statements here\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent - Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will lay out the process here in detail. I did not lay out any pseudocode for the formulas. If you are struggling to program the formulas, please talk to me or Mike.\n",
    "If there are any issues or questions let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the hypothesis: \n",
    "\n",
    "$$ h(x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3})}} $$\n",
    "\n",
    "Below, you will find a function defining the hypothesis. Fill it out using the formula from above. To check if it is correct, use the test variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not change the function names if you want to use the assert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hypothesis(X,coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X : (numpy 2D array) containing the input data\n",
    "        -coeffs: (column vector) containing the coefficients being used to get a hypothesis\n",
    "    Output:\n",
    "        -Hypothesis: (column vector) containing hypothesis values\n",
    "    \n",
    "    You can generate hypothesis values using the sigmoid (i.e. the formula from above)\n",
    "    '''\n",
    "    return   1/(1+np.exp(-(np.dot(X,coeffs))))\n",
    "\n",
    "    \n",
    "\n",
    "#Remember that you can call this function in later parts of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95257413]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis(test_x,test_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to test your hypothesis function\n",
    "test_x,test_coeffs = np.array([1,1,1]).reshape(1,-1),np.array([1,1,1]).reshape(-1,1)\n",
    "assert_almost_equal(0.95257413,hypothesis(test_x,test_coeffs)[0][0],decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the cost function for logistic regression\n",
    "\n",
    "$$ J(\\beta) = - \\frac{1}{n} \\sum_{i = 1}^{n} \\left[ y_i log(h(x_i)) + (1 - y_i) log(1 - h(x_i)) \\right] $$  \n",
    "\n",
    "Don't forget to take the log of the prediction or else the assert statement will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(X,y,coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -y: (column vector) containing the labels for the input data\n",
    "        -coeffs: (column vector) containing the coefficient values\n",
    "    Ouput:\n",
    "        -Float Value: the cost associated with using certain coefficient values\n",
    "\n",
    "    Implement the cost function from above.\n",
    "    '''\n",
    "    \n",
    "    h = hypothesis(X,coeffs)\n",
    "\n",
    "\n",
    "    length = len(X)\n",
    "\n",
    "\n",
    "    cost = -(1/length)*sum(y*(np.log(h)) + (1-y)*np.log(1-h))\n",
    "\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69314718])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(test_x,test_y,test_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3]])\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to test your cost function\n",
    "test_x,test_coeffs, test_y = np.array([1,2,3]).reshape(-1,1),np.array([0]).reshape(-1,1), np.array([0,0,0]).reshape(-1,1)\n",
    "assert_almost_equal(cost(test_x,test_y,test_coeffs),0.69314718055994518, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the gradient formula:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_j} J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h(x_i) - y_i \\right) x_{ij}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = np.array([[1,2],\n",
    "                   [2,3],\n",
    "                   [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(X,y,coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -y: (column vector) containing the labels for the input data\n",
    "        -coeffs: (column vector) containing the coefficient values\n",
    "    Output:\n",
    "        -Gradient: the gradient for each of the coefficients\n",
    "    \n",
    "    Use the formula above to calculate gradient\n",
    "    '''\n",
    "\n",
    "\n",
    "    rows,columns = np.shape(X)\n",
    "\n",
    "    \n",
    "    h = hypothesis(X,coeffs)\n",
    "    print(h, 'h')\n",
    "\n",
    "    grad = []\n",
    "    column_total = np.zeros((np.shape(X)))\n",
    "    \n",
    "    for count_row, row in enumerate(range(rows)):\n",
    "        \n",
    "        inside = h[count_row]-y[count_row]\n",
    "\n",
    "        current_total=0\n",
    "        \n",
    "        for matrix_column,col in enumerate(range(columns)): ## go through the original matrix\n",
    "            for matrix_row,row in enumerate(range(rows)):\n",
    "\n",
    "                current_total +=inside*X[matrix_row,matrix_column] ###\n",
    "\n",
    "        grad.append(current_total/rows)\n",
    "\n",
    "    return np.array(grad)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]] h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(test_x,test_y,test_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to figure out if your implementation of the gradient is correct\n",
    "test_x,test_coeffs, test_y = np.array([1,2,3]).reshape(-1,1),np.array([0]).reshape(-1,1), np.array([0,0,0]).reshape(-1,1)\n",
    "assert_almost_equal(gradient(test_x,test_y,test_coeffs)[0][0],1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predict function which will predict any values strictly greater than 0.5 to be 1 when the hypothesis function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X, coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -coeffs: (column vector) containing the coefficient values\n",
    "    Output:\n",
    "        -Predictions: predicted values using a threshold of 0.5\n",
    "    '''\n",
    "    \n",
    "    threshold = .5\n",
    "    bool_t = hypothesis( X, coeffs)>threshold \n",
    "    bool_f = [1 for i in bool_t if i == True ]\n",
    "    return bool_f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to figure out if your implementation of the predict function is correct\n",
    "test_x,test_coeffs = np.array([1,1,1]).reshape(1,-1),np.array([1,1,1]).reshape(-1,1)\n",
    "assert_almost_equal(predict(test_x, test_coeffs),1,decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to implement gradient descent in the fit function (we are going to treat our class like an sklearn class). For now to see if everything is working we will just implement gradient descent (and tweak some of the code to take care of some functionality we might want). Below is the algorithm for gradient descent.\n",
    "\n",
    "    Gradient Descent:\n",
    "        input: J: optimization function (cost function)\n",
    "               alpha: learning rate\n",
    "               n: number of iterations\n",
    "        output: local minimum of optimization function J\n",
    "\n",
    "        initialize b (often as all 0's)\n",
    "        costs = []\n",
    "        repeat for n iterations:\n",
    "            update b as b - alpha * gradient(J)\n",
    "            append costs for updated b values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,alpha):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -y: (column vector) containing the labels for the input data\n",
    "        -alpha: (int) the step-size to multiply the gradient by\n",
    "    Output:\n",
    "        Coefficients - coefficients calculated by gradient descent\n",
    "        Costs - the cost associated with each update of the coefficients\n",
    "    \n",
    "    For this implementation just use 10,000 iterations. MAKE SURE YOU RETURN 2 VALUES!\n",
    "    '''\n",
    "\n",
    "    coeffs=np.zeros(np.shape(X)[1]).reshape(np.shape(X)[1],1)\n",
    "        # * Recall that there should be as many coefficients as features.\n",
    "\n",
    "        # I give you this line here. if fit_intercept = True, set the intercept to 0\n",
    "#         if self.fit_intercept:\n",
    "#             self.coeffs = np.insert(self.coeffs, 0, 0)\n",
    "\n",
    "        # * for each of the num_iterations, update self.coeffs at each step.\n",
    "    num_interations = 10\n",
    "    grad_cost= []\n",
    "    #def cost(X,y,coeffs):\n",
    "    #X = X.T\n",
    "    \n",
    "    for n in range(num_interations):\n",
    "        print(n, ' iteration')\n",
    "        #update b as b - alpha * gradient(J)\n",
    "        coeffs = coeffs- alpha*gradient(X,y,coeffs)\n",
    "        grad_cost.append(cost(X,y,coeffs))\n",
    "        \n",
    "    print(grad_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1/(1+np.exp(-(np.dot(X,coeffs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]] h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [ 6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333],\n",
       "       [-6.92733333]])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(gradient(test_x,test_y ,coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = np.zeros((np.shape(test_x)[1])).reshape(np.shape(test_x)[1],1)\n",
    "np.shape(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1/(1+np.exp(-(np.dot(X,coeffs))))\n",
    "\n",
    "# we want a (1,4) return from the hypothesis function\n",
    "# therefore, need to multiply matrices of size 4 x n by n X 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  iteration\n",
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]] h\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,1) (150,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-507-287b5b9f2509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# plt.plot(costs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-482-4a95030e9fac>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, alpha)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#update b as b - alpha * gradient(J)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mgrad_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,1) (150,1) "
     ]
    }
   ],
   "source": [
    "# Uncomment the code below to figure out if your implementation of gradient_descent is correct\n",
    "# The code depends on your functions from above so make sure those are correct\n",
    "# Use should be generating a plot of the costs it should look like the graph below.\n",
    "\n",
    "\n",
    "test_x, test_alpha, test_y = load_iris().data, 0.01, (load_iris().target >= 1).astype(int).reshape(-1,1)\n",
    "coeffs, costs = gradient_descent(test_x,test_y,test_alpha)\n",
    "assert_almost_equal(accuracy_score(predict(test_x,coeffs),test_y.flatten()),1.0, decimal = 3)\n",
    "# plt.plot(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts of gradient descent, let's put this in a class. PLEASE DO NOT JUST COPY AND PASTE YOUR FUNCTIONS FROM ABOVE. You should use them to ensure your implementation is correct, but the class should be set up differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun Fact: Notice there are functions that have 1 underscore before them (for example __hypothesis). In python programming, these are usually considered to be private functions. Unlike C/C++, these functions can be called by users (so they are not really private). The idea behind private functions is that a user would most likely never call this function, but you need it in order to perform operations for functions that users will call. For example, it can be argued that users will never care about what the gradient  or hypothesis for a given example will be. Thus, you can keep these hidden, as users will most likely only care about fitting the model and predicting on new values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \n",
    "    def __init__(self, alpha = 0.01,num_iterations = 10000, fit_intercept = True, normalize = False):\n",
    "        # I have given the inputs that this class should take. Please set them as class \n",
    "        # attributes so that you can call them throughout the class\n",
    "        \n",
    "        \n",
    "        # Now add a coefficients variable so it does not need to be passed from function to function\n",
    "        \n",
    "        \n",
    "        # Also, add a scores variable which is a list that will hold the costs associated with\n",
    "        # each update of the coefficients\n",
    "        \n",
    "        # You might want to store mu and sigma if normalize is true. For now leave them as None\n",
    "        \n",
    "        # Another thing you can do is store n and m (number of rows and columns in X). However,\n",
    "        # be careful! If fit intercept is true then the value m will change. Store them as None for now. \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "            -y: (column vector) containing the labels for the input data\n",
    "        Output:\n",
    "            None\n",
    "\n",
    "        This is where gradient descent will be happening. However, notice we are just inputting \n",
    "        X and Y. Thus the other variables you will have to get from the attributes of the class.\n",
    "        '''\n",
    "        # If fit_intercept is true add a column of ones before the data. I will give you this code below.\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        # If normalize is true, normalize the data.\n",
    "        \n",
    "        \n",
    "        # Now that we are done with transformations, save the n and m values.\n",
    "        \n",
    "        # Also, initialize coefficients to be 0\n",
    "        \n",
    "        # Finally, perform gradient descent. Get num_iterations and alpha from the atrributes\n",
    "        # Don't forget to store the costs for the updated coefficients\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _hypothesis(self,X):\n",
    "        '''\n",
    "        Input:\n",
    "            -X : (numpy 2D array) containing the input data\n",
    "        Output:\n",
    "            -Hypothesis: (column vector) containing hypothesis values\n",
    "        '''            \n",
    "        pass\n",
    "\n",
    "    def _gradient(self,X,y):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "            -y: (column vector) containing the labels for the input data\n",
    "        Output:\n",
    "            -Gradient: the gradient for each of the coefficients\n",
    "        '''        \n",
    "        pass\n",
    "    \n",
    "    def _cost(self,X,y):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "            -y: (column vector) containing the labels for the input data\n",
    "        Ouput:\n",
    "            -Float Value: the cost associated with using certain coefficient values\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "        Output:\n",
    "            -Predictions: predicted values using a threshold of 0.5\n",
    "        '''        \n",
    "        pass      \n",
    "    \n",
    "    def plot_scores(self):\n",
    "        '''\n",
    "        Input:\n",
    "            -None\n",
    "        Ouput:\n",
    "            -Line plot showing how scores decrease\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XucXWV97/HPLxcIuZBAAhmQnAKKEIpAMkCNilhDxUvR\noq04ypGGSkU5VadqW496VHoQiwUrxRTqhYDgHNG+jqKnfUWD9JxSTJAZkqoNoBBuCgkJmgAh5Pac\nP549ZrKZ295ZM2uvmc/79Vqvvfezn7XWbx5C5ptn3SKlhCRJUhEmlF2AJEkaOwwWkiSpMAYLSZJU\nGIOFJEkqjMFCkiQVxmAhSZIKY7CQJEmFMVhIkqTCGCwkSVJhDBaSJKkwTQWLiLg4ItZFxLMRsTIi\nTh2k73URsTsidtVee5cfN1+2JElqRQ0Hi4g4F7gC+ASwAFgDLI+IOQOs8j6gDTis9noE8CRwczMF\nS5Kk1hWNPoQsIlYCq1JK7699DuAR4KqU0uXDWP8PgG8CR6WUHmm8ZEmS1KoamrGIiMlAO3Brb1vK\nyWQFsGiYm7kAWGGokCRp7JnUYP85wERgfV37euDYoVaOiMOA1wFvG6LfbOAs4EFgW4M1SpI0nk0B\njgSWp5Q2jfbOGw0W++qPgV8B3x6i31nATSNejSRJY9c7gK+N9k4bDRYbgV3A3Lr2ucDjw1h/CXBD\nSmnnEP0eBLjxxhuZP39+gyWqWZ2dnXzuc58ru4xxxTEffY756HPMR9fatWs577zzoPa7dLQ1FCxS\nSjsiohtYDNwCvzl5czFw1WDrRsSrgBcCXx7GrrYBzJ8/n4ULFzZSovbBzJkzHe9R5piPPsd89Dnm\npSnlVIJmDoVcCSyrBYw7gU5gKrAMICIuAw5PKZ1ft96fkK8mWdt8uZIkqZU1HCxSSjfX7llxCfkQ\nyGrgrJTSE7UubcC8vutExIHAOeR7WkiSpDGqqZM3U0pLgaUDfLekn7YtwPRm9iVJkqrDZ4XoNzo6\nOsouYdxxzEefYz76HPPxpeE7b46GiFgIdHd3d3vCjyRJDejp6aG9vR2gPaXUM9r7d8ZCkiQVxmAh\nSZIKY7CQJEmFMVhIkqTCGCwkSVJhDBaSJKkwBgtJklQYg4UkSSqMwUKSJBXGYCFJkgpjsJAkSYUx\nWEiSpMIYLCRJUmEMFpIkqTAGC0mSVBiDhSRJKozBQpIkFcZgIUmSCmOwkCRJhTFYSJKkwhgsJElS\nYQwWkiSpMAYLSZJUGIOFJEkqjMFCkiQVxmAhSZIKY7CQJEmFMVhIkqTCGCwkSVJhDBaSJKkwBgtJ\nklSYpoJFRFwcEesi4tmIWBkRpw7Rf7+IuDQiHoyIbRHxQET8cVMVS5KkljWp0RUi4lzgCuBPgTuB\nTmB5RLw4pbRxgNW+ARwCLAHuBw7D2RJJksachoMFOUhcm1K6ASAiLgLeAFwAXF7fOSJeC5wOHJ1S\n+nWt+eHmypUkSa2soVmDiJgMtAO39rallBKwAlg0wGpnA3cBfxkRj0bEvRHx2YiY0mTNkiSpRTU6\nYzEHmAisr2tfDxw7wDpHk2cstgF/UNvGPwAHA3/S4P4lSVILa+ZQSKMmALuBt6eUngaIiD8HvhER\n700pPTcKNUiSpFHQaLDYCOwC5ta1zwUeH2Cdx4Bf9IaKmrVAAEeQT+bsV2dnJzNnztyrraOjg46O\njgbLliRp7Onq6qKrq2uvts2bN5dUTRb5FIkGVohYCaxKKb2/9jnIJ2NelVL6bD/9LwQ+BxyaUtpa\na3sT8E1gen8zFhGxEOju7u5m4cKFDf5IkiSNXz09PbS3twO0p5R6Rnv/zVzyeSVwYUS8MyKOA64B\npgLLACLisoi4vk//rwGbgOsiYn5EvJJ89ciXPQwiSdLY0vA5FimlmyNiDnAJ+RDIauCslNITtS5t\nwLw+/Z+JiN8D/h74ETlkfB34+D7WLkmSWkxTJ2+mlJYCSwf4bkk/bfcBZzWzL0mSVB3e/VKSJBXG\nYCFJkgpjsJAkSYUxWEiSpMIYLCRJUmEMFpIkqTAGC0mSVBiDhSRJKozBQpIkFcZgIUmSCmOwkCRJ\nhTFYSJKkwhgsJElSYQwWkiSpMAYLSZJUGIOFJEkqjMFCkiQVxmAhSZIKY7CQJEmFMVhIkqTCGCwk\nSVJhDBaSJKkwBgtJklQYg4UkSSqMwUKSJBXGYCFJkgpjsJAkSYVp6WCRUtkVSJKkRrR0sNi5s+wK\nJElSI1o6WDz3XNkVSJKkRrR0sNi+vewKJElSI1o6WDhjIUlStbR0sHDGQpKkajFYSJKkwjQVLCLi\n4ohYFxHPRsTKiDh1kL5nRMTuumVXRBw61H48FCJJUrU0HCwi4lzgCuATwAJgDbA8IuYMsloCjgHa\nasthKaUNQ+3LGQtJkqqlmRmLTuDalNINKaV7gIuArcAFQ6z3REppQ+8ynB05YyFJUrU0FCwiYjLQ\nDtza25ZSSsAKYNFgqwKrI+KXEfG9iHjZcPbnjIUkSdXS6IzFHGAisL6ufT35EEd/HgPeDbwFeDPw\nCPCvEXHyUDtzxkKSpGqZNNI7SCndB9zXp2llRLyQfEjl/MHWXbaskzvumLlXW0dHBx0dHYXXKUlS\n1XR1ddHV1bVX2+bNm0uqJms0WGwEdgFz69rnAo83sJ07gZcP1ektb/kcl166sIHNSpI0fvT3j+2e\nnh7a29tLqqjBQyEppR1AN7C4ty0iovb5jgY2dTL5EMmgPBQiSVK1NHMo5EpgWUR0k2ceOoGpwDKA\niLgMODyldH7t8/uBdcBPgSnAhcDvAr831I48eVOSpGppOFiklG6u3bPiEvIhkNXAWSmlJ2pd2oB5\nfVbZj3zfi8PJl6X+B7A4pfT/htqXMxaSJFVLUydvppSWAksH+G5J3efPAp9tZj/OWEiSVC0+K0SS\nJBWmpYPFtm1lVyBJkhphsJAkSYUxWEiSpMIYLCRJUmEMFpIkqTAGC0mSVBiDhSRJKozBQpIkFcZg\nIUmSCtPSweLZZ8uuQJIkNaKlg8W2bZBS2VVIkqThaulgkZLPC5EkqUpaOlgAbN1adgWSJGm4DBaS\nJKkwBgtJklQYg4UkSSqMwUKSJBWm5YPFM8+UXYEkSRqulg8WzlhIklQdBgtJklQYg4UkSSpMSweL\nyZMNFpIkVUlLB4spUwwWkiRVSUsHiwMOMFhIklQlLR0snLGQJKlaWj5YeB8LSZKqo+WDhTMWkiRV\nR8sHC2csJEmqjpYOFtOmwdNPl12FJEkarpYOFgccYLCQJKlKWjpYTJsGTz1VdhWSJGm4WjpYHHCA\nwUKSpCppKlhExMURsS4ino2IlRFx6jDXe3lE7IiInuH09xwLSZKqpeFgERHnAlcAnwAWAGuA5REx\nZ4j1ZgLXAyuGuy9nLCRJqpZmZiw6gWtTSjeklO4BLgK2AhcMsd41wE3AyuHuaNq0fB+LXbuaqFKS\nJI26hoJFREwG2oFbe9tSSok8C7FokPWWAEcBn2pkf1On5lfvZSFJUjU0OmMxB5gIrK9rXw+09bdC\nRBwDfBp4R0ppdyM76w0WnmchSVI1TBrJjUfEBPLhj0+klO7vbR7u+l/5Sicwk/POg+nTc1tHRwcd\nHR1FlypJUuV0dXXR1dW1V9vmzZtLqiaLfCRjmJ3zoZCtwFtSSrf0aV8GzEwpnVPXfybwK2AnewLF\nhNr7ncBrUkr/2s9+FgLdN93UzTvesZC77oL29kZ+LEmSxqeenh7a8y/N9pTSsK7CLFJDh0JSSjuA\nbmBxb1tERO3zHf2ssgU4ATgZOKm2XAPcU3u/arD99R4K8coQSZKqoZlDIVcCyyKiG7iTfJXIVGAZ\nQERcBhyeUjq/dmLnf/ZdOSI2ANtSSmuH2pHnWEiSVC0NB4uU0s21e1ZcAswFVgNnpZSeqHVpA+YV\nUZwzFpIkVUtTJ2+mlJYCSwf4bskQ636KYV52OmUKRDhjIUlSVbT0s0ImTPBBZJIkVUlLBwuAGTOc\nsZAkqSpaPlhMn+6MhSRJVdHywcIZC0mSqqPlg4UzFpIkVUfLBwtnLCRJqo6WDxbOWEiSVB0tHyxm\nzIAtW8quQpIkDUfLB4tZs6DkB7VJkqRhavlgMXOmwUKSpKpo+WDhjIUkSdXR8sFi5kx47jnYtq3s\nSiRJ0lAqESzAWQtJkqqg5YPFrFn51WAhSVLra/lg0Ttj8etfl1uHJEkaWssHC2csJEmqjpYPFp5j\nIUlSdbR8sJgxI796KESSpNbX8sFi4kQ48EBnLCRJqoKWDxbg3TclSaqKSgSLWbM8FCJJUhVUIlg4\nYyFJUjUYLCRJUmEqESw8FCJJUjVUIlg4YyFJUjVUIlg4YyFJUjVUIlgcdBA8+WTZVUiSpKFUIljM\nng1btsCOHWVXIkmSBlOZYAHOWkiS1OoMFpIkqTCVCBYHH5xfN20qtw5JkjS4SgSL3hkLg4UkSa2t\nEsHCGQtJkqqhqWARERdHxLqIeDYiVkbEqYP0fXlE3B4RGyNia0SsjYgPNLK/SZPyTbIMFpIktbZJ\nja4QEecCVwB/CtwJdALLI+LFKaWN/azyDPD3wH/U3r8C+MeIeDql9KXh7nf2bIOFJEmtrpkZi07g\n2pTSDSmle4CLgK3ABf11TimtTil9PaW0NqX0cErpa8By4PRGdnrwwQYLSZJaXUPBIiImA+3Arb1t\nKaUErAAWDXMbC2p9/7WRfTtjIUlS62t0xmIOMBFYX9e+HmgbbMWIeCQitpEPn3whpXRdIzs2WEiS\n1PpG86qQV5BnOy4COmvnagybwUKSpNbX6MmbG4FdwNy69rnA44OtmFJ6qPb2pxHRBnwS+Ppg63R2\ndjJz5kwA7r0XHnoIuro66OjoaLBsSZLGnq6uLrq6uvZq27x5c0nVZJFPkWhghYiVwKqU0vtrnwN4\nGLgqpfTZYW7jfwB/nFI6eoDvFwLd3d3dLFy4EICrr4YPfhC2bYOIhkqWJGnc6Onpob29HaA9pdQz\n2vtv+HJT4EpgWUR0s+dy06nAMoCIuAw4PKV0fu3ze8nB457a+mcAHwT+rpGdzp4N27fD00/DjBlN\nVC1JkkZcw8EipXRzRMwBLiEfAlkNnJVSeqLWpQ2Y12eVCcBlwJHATuB+4MMppX9sZL+HHppfN2ww\nWEiS1KqambEgpbQUWDrAd0vqPl8NXN3MfvqaWzur4/HH4YUv3NetSZKkkVCJZ4XAnmCxvv5CV0mS\n1DIqEyxmz4aJEw0WkiS1ssoEiwkT4JBDDBaSJLWyygQLyIdDDBaSJLUug4UkSSqMwUKSJBXGYCFJ\nkgpTqWDR1mawkCSplVUqWMydm2/pvXVr2ZVIkqT+VC5YgLMWkiS1KoOFJEkqTKWCxeGH59fHHiu3\nDkmS1L9KBYvZs2H//eGRR8quRJIk9adSwSICjjgCHn207EokSVJ/KhUswGAhSVIrM1hIkqTCGCwk\nSVJhKhksfvEL2L277EokSVK9SgaL7dth48ayK5EkSfUqGSzAwyGSJLUig4UkSSpM5YLFoYfCpEkG\nC0mSWlHlgsWECXnW4uGHy65EkiTVq1ywADjqKFi3ruwqJElSvUoGi6OPhgceKLsKSZJUr7LBwhkL\nSZJaT2WDxaZNsHlz2ZVIkqS+KhksjjoqvzprIUlSa6lksDj66PzqeRaSJLWWSgaLOXNg+nSDhSRJ\nraaSwSLCK0MkSWpFlQwWYLCQJKkVVTZYvPCF8LOflV2FJEnqq6lgEREXR8S6iHg2IlZGxKmD9D0n\nIr4XERsiYnNE3BERr2m+5Oy44+DBB2Hbtn3dkiRJKkrDwSIizgWuAD4BLADWAMsjYs4Aq7wS+B7w\nOmAhcBvwnYg4qamKa+bPh9274b779mUrkiSpSM3MWHQC16aUbkgp3QNcBGwFLuivc0qpM6X0tyml\n7pTS/SmljwI/A85uumryjAXAPffsy1YkSVKRGgoWETEZaAdu7W1LKSVgBbBomNsIYAbwZCP7rjd7\nNhxyCKxduy9bkSRJRWp0xmIOMBFYX9e+Hmgb5jY+DEwDbm5w388zf77BQpKkVjKqV4VExNuBjwN/\nlFLauK/bmz/fQyGSJLWSSQ323wjsAubWtc8FHh9sxYh4G/CPwB+mlG4bzs46OzuZOXPmXm0dHR10\ndHQA+TyL66+HXbtg4sTh/QCSJI0VXV1ddHV17dW2ueQndEY+RaKBFSJWAqtSSu+vfQ7gYeCqlNJn\nB1inA/gScG5K6bvD2MdCoLu7u5uFCxcO2G/5cnjta+H++/c8P0SSpPGsp6eH9vZ2gPaUUs9o77+Z\nQyFXAhdGxDsj4jjgGmAqsAwgIi6LiOt7O9cOf1wPfBD4UUTMrS0H7mvxJ56YX9es2dctSZKkIjQc\nLFJKNwMfAi4B7gZOBM5KKT1R69IGzOuzyoXkEz6/APyyz/J3zZdd21EbHHoorF69r1uSJElFaPQc\nCwBSSkuBpQN8t6Tu8+82s4/hiIAFC+Duu0dqD5IkqRGVfVZIr5NPdsZCkqRWUflgsWABPPIIbNpU\ndiWSJKnyweLkk/OrsxaSJJWv8sHiRS+CadM8z0KSpFZQ+WAxcWI+HPKjH5VdiSRJqnywAFi0CO64\no+wqJEnSmAkWjz6aT+KUJEnlGTPBAuCHPyy3DkmSxrsxESza2uCoowwWkiSVbUwEC/A8C0mSWsGY\nCRYve1m+5PSZZ8quRJKk8WvMBItXvxp27IDbby+7EkmSxq8xEyyOOw4OOwxWrCi7EkmSxq8xEywi\n4Mwz4dZby65EkqTxa8wEC8jB4u67YePGsiuRJGl8GlPBYvHi/PqDH5RbhyRJ49WYChYveAEcfzz8\ny7+UXYkkSePTmAoWAG98I3znO7BzZ9mVSJI0/oy5YPGmN8GmTd4sS5KkMoy5YHHaafkW39/+dtmV\nSJI0/oy5YDFhApx9dg4WKZVdjSRJ48uYCxYAb34z3H9/vvRUkiSNnjEZLM48Ew49FG68sexKJEka\nX8ZksJg0CTo6oKvLq0MkSRpNYzJYAJx3Hjz+uDfLkiRpNI3ZYNHeDsceC9dfX3YlkiSNH2M2WETA\nhRfCN78JGzaUXY0kSePDmA0WAEuW5MtPv/zlsiuRJGl8GNPB4uCD80mc11wDu3aVXY0kSWPfmA4W\nABdfDA8/7J04JUkaDWM+WLS3wxlnwKc/7Z04JUkaaWM+WAB87GPQ3Q3f+17ZlUiSNLaNi2CxeHF+\nONmll5ZdiSRJY9u4CBYRedbi3/4Nli8vuxpJksaupoJFRFwcEesi4tmIWBkRpw7Sty0iboqIeyNi\nV0Rc2Xy5zfv934fTT4cPf9grRCRJGikNB4uIOBe4AvgEsABYAyyPiDkDrLI/sAH4a2B1k3Xuswj4\n27+FH/8Yli0rqwpJksa2ZmYsOoFrU0o3pJTuAS4CtgIX9Nc5pfRQSqkzpXQjsKX5Uvfdaafl+1p8\n7GPw61+XWYkkSWNTQ8EiIiYD7cCtvW0ppQSsABYVW9rIuPxyeOYZ+Ku/KrsSSZLGnkZnLOYAE4H1\nde3rgbZCKhphRxwBn/kMXHttPplTkiQVZ1LZBQyms7OTmTNn7tXW0dFBR0fHPm33oovgppvgXe+C\nnh6YNm2fNidJUim6urro6uraq23z5s0lVZNFauB2lLVDIVuBt6SUbunTvgyYmVI6Z4j1bwPuTin9\n+RD9FgLd3d3dLFy4cNj1NeK++2DhQnjrW+ErXxmRXUiSNOp6enpob28HaE8p9Yz2/hs6FJJS2gF0\nA4t72yIiap/vKLa0kfXiF8PVV8N118HXvlZ2NZIkjQ3NHAq5ElgWEd3AneSrRKYCywAi4jLg8JTS\n+b0rRMRJQADTgUNqn7enlNbuW/n75vzzYcUKePe74YQT4MQTy6xGkqTqazhYpJRurt2z4hJgLvne\nFGellJ6odWkD5tWtdjfQe8xlIfB24CHg6GaKLkpEfqT6T34CZ58Nd94Jc+eWWZEkSdXW1J03U0pL\nU0pHppQOSCktSind1ee7JSmlV9f1n5BSmli3lBoqek2fDt/5DmzfDuecA88+W3ZFkiRV17h4VshQ\n5s2Db38b1qyBN78Znnuu7IokSaomg0XNaafBLbfAbbflu3Pu2FF2RZIkVY/Boo/Fi+Gf/gm++134\noz/ysIgkSY0yWNR5wxvgW9+C730PXvtanykiSVIjDBb9eP3r82WoP/5xftT6Aw+UXZEkSdVgsBjA\ny14Gt9+eD4ecckqewZAkSYMzWAzi+OPhRz+Cl74UXvc6uOQS2Lmz7KokSWpdBoshHHRQvs/Fxz4G\nn/oUvPKV8POfl12VJEmtyWAxDBMn5lBx++2wYQOcfDJ8/vPOXkiSVM9g0YBFi2D1anjnO6GzE049\nFVauLLsqSZJah8GiQdOnw9KlsGpVnslYtCgHjXXryq5MkqTyGSyadOqpOVxccw18//tw7LHwvvfB\n+vVlVyZJUnkMFvtg4sT8yPWf/zyfg3HDDXDkkXDxxd77QpI0PhksCjBtGnzkIzlMfPSj8I1vwDHH\nwNveBj/8IaQ09DYkSRoLDBYFOvjgfFnqQw/B1VfDXXflG22ddFL+7O3BJUljncFiBBxwALznPXDf\nfbB8eZ69+MAH4PDD4R3vyA8527697ColSSqewWIETZgAr3lNfmLqI4/Axz8Oa9bA2WdDWxu86135\nxE9DhiRprDBYjJLDDsvnYfzkJ/nhZu95D9x2Ww4ehxwCb31rPvnziSfKrlSSpOYZLEpwwglw6aX5\napKeHvjQh/J5GeefD3Pn5ntjfPSjcOut+SFokiRVhcGiRBGwYEE+RLJqFTz2GHz5yzBvHnzxi3Dm\nmTBrFpxxRr6c9dZbYcuWsquWJGlgk8ouQHu0tcGSJXlJCX7603y45Ac/yM8m+eQncxg5/vj8xNXf\n+Z38evzx+Z4akiSVzWDRoiLyIZMTToA/+zPYvRvuvTc/m2TVqvx63XW5/YAD4CUvyZe19i4nnggH\nHlj2TyFJGm8MFhUxYQLMn5+XJUty29NP53tl9PTkq01WrYJly2DHjvz9UUflwHHccXuWY4/N99uQ\nJGkkGCwqbPp0eNWr8tJr+3ZYuzYHjTVr8uGUr389nxza65BD9oSMY46Bo4/OIeSoo+Cgg/JsiSRJ\nzTBYjDH77bfncEhfW7fCz34G99yTD6nccw90d+fQ8dRTe/odeODeQePoo+G3fguOOCIvs2cbPCRJ\nAzNYjBNTp/YfOFKCJ5/Mj31/4IG9X2+5BR58EHbu3NN///3hBS/IIaP3tff9C16QT0CdOzef9yFJ\nGn8MFuNcRJ6FmD0bTjnl+d/v2pUvg/3FL/Ly6KN56X1/55359bnn9l5vxgw49NAcMnqX+s9z5+b9\nzpqVzyGRJFWfwUKDmjhxz6zEQFKCTZty2Fi/Pi8bNuz9ftWqPe/7zoBADjcHHZRDxsEH7/06UNvM\nmfmwjZfZSlJrMVhon0XAnDl5Gcru3fkpr72h48kncyjZtGnv9w89lK926f1cH0Z6TZ+eQ0b9MmtW\n/+29y4EH5nWnT4cpUzxvRJKKYrDQqJowIc88HHxwvnR2OFLKl9b2DR+bNz9/+fWv8+uGDflE1b7f\n9V6CO1BNvSFjoGXGjIG/mzYtn8MydWo+t6T39YADYJL/h0kaZ/xrTy0vIv9inzEDjjyy8fVTgm3b\n9g4aW7bAM8/kwDLY8uST8PDDz2/ftm14+548+fmBo+/rYN/1vk6Zkk+arX8dqG3//Z2BkVQeg4XG\nvIg9MwhtbcVsc+fOPcHkqafyw+K2bm38devWPAMzUJ/t25urb7/9hg4gg7VNnpy3sd9+jb8fTj9P\n1pXGLoOFfqOrq4uOjo6yy6iESZP2nK+xL4Ya85078xU3zz2XZ0nq3w/WNtz+W7bs3bZ9e1527Hj+\n+4HOdWnUxIlDh5HeZdKkvAzn/XD6rV7dxUtf2rFP26j/PHHinqXvZwNU5t8t40tTwSIiLgY+BLQB\na4A/Syn9aJD+rwKuAH4beBi4NKV0fTP71sjxf/7RN9SY9/4CmzZtFIsaxO7dOWT0FzoGez/cfvXr\n7NqV23bu3LPs2JEDUO/7vu31/fp7v3VrFzfc0MHu3aMzZv0FjsHaWqVvbzDqfR3s/VD9rr66i3nz\nOhpap9l+ER4KLFvDwSIiziWHhD8F7gQ6geUR8eKU0sZ++h8JfBdYCrwdOBP4UkT8MqX0/eZLlzTa\nJkzYc9ikqt74xnzzt927nx9chhtO6t/v2rX3snPn8NqK6NsbwEZif7t35yWlfR/300/f920MV8S+\nB5X6deqXiME/D6dPM+sMp8/69aM31v1pZsaiE7g2pXQDQERcBLwBuAC4vJ/+7wEeSCn9Re3zvRHx\nitp2DBaSStH7l/DkyWVX0vpS2hMyegNZI+8vvBCWLm1+/eG8H+lt1Y9Bb+Aa7PNw+ozEdod7cvlI\naShYRMRkoB34dG9bSilFxApg0QCrvRRYUde2HPhcI/uWJJWjdwag2RvSTZ+eH3yo0dHTA+3t5e2/\n0RmLOcBEoH6iZT1w7ADrtA3Q/8CI2D+l9Fw/60wBWLt2bYPlaV9s3ryZnp6esssYVxzz0eeYjz7H\nfHT1+d05pYz9t+pVIUcCnHfeeSWXMf60lxlzxynHfPQ55qPPMS/FkcAdo73TRoPFRmAXMLeufS7w\n+ADrPD5A/y0DzFZAPlTyDuBBoOSjRZIkVcoUcqhYXsbOGwoWKaUdEdENLAZuAYiIqH2+aoDVfgi8\nrq7tNbX2gfazCfhaI7VJkqTfGPWZil7N3L7lSuDCiHhnRBwHXANMBZYBRMRlEdH3HhXXAEdHxN9E\nxLER8V7gD2vbkSRJY0jD51iklG6OiDnAJeRDGquBs1JKT9S6tAHz+vR/MCLeQL4K5H3Ao8CfpJTq\nrxSRJEkVF6mIO59IkiTR3KEQSZKkfhksJElSYVouWETExRGxLiKejYiVEXFq2TVVQUR8JCLujIgt\nEbE+Iv53RLy4n36XRMQvI2JrRHw/Il5U9/3+EfGFiNgYEU9FxDcj4tC6PgdFxE0RsTkifhURX4qI\nFnlMVnki4q8iYndEXFnX7pgXKCIOj4iv1sZra0SsiYiFdX0c84JExISI+OuIeKA2nj+PiI/1088x\nb1JEnB7QLNCvAAAFHklEQVQRt0TEL2p/h7yxnz6jMr4RMS8i/k9EPBMRj0fE5RHRWFZIKbXMApxL\nvm/FO4HjgGuBJ4E5ZdfW6gvwz8B/BeYDLyE/+O1B4IA+ff6yNp6/D5wAfAu4H9ivT59/qK13BrCA\nfMnSv9Xt61+AHuAU4GXAfcCNZY9ByeN/KvAAcDdwpWM+YuM8C1gHfIn8eIHfIj/Y8CjHfMTG/L8D\nG4DXAv8FeDOwBfhvjnlhY/xa8gURbyLfK+qNdd+PyviSJxt+TL7/xUuAs2r/7f9nQz9P2QNa90Ov\nBD7f53OQryL5i7Jrq9pCvv36buAVfdp+CXT2+Xwg8Czw1j6fnwPO6dPn2Np2Tqt9nl/7vKBPn7OA\nnUBb2T93SWM9HbgXeDVwG3sHC8e82LH+DPB/h+jjmBc75t8BvljX9k3gBsd8RMZ7N88PFqMyvuR7\nTu2gzz/mgXcDvwImDfdnaJlDIbHnAWe39ral/FMN9oAzDWwWkMgpl4g4inwpcN/x3QKsYs/4nkK+\nBLlvn3uBh/v0eSnwq5TS3X32taK2r98ZiR+kAr4AfCel9IO+jY75iDgbuCsibq4d8uuJiHf1fumY\nj4g7gMURcQxARJwEvJw8S+qYj7BRHt+XAj9OKW3s02c5MBP47eHW3ErPCmnmAWfqR0QE8HfA7Sml\n/6w1t5H/APU3vm2193OB7bU/tAP1aSNPjf1GSmlXRDzZp8+4ERFvA04m/49dzzEv3tHAe4ArgEuB\n04CrIuK5lNJXccxHwmfI/yK+JyJ2kafLP5pS+l+17x3zkTWa4zvQQ0N7v1sznIJbKVioOEuB48n/\nqtAIiYgjyAHuzJTSjrLrGScmAHemlD5e+7wmIk4ALgK+Wl5ZY9q5wNuBtwH/SQ7Sn4+IX9bCnLSX\nljkUQnMPOFOdiLgaeD3wqpTSY32+epx8zspg4/s4sF9EHDhEn/ozjScCBzP+/ju1A4cAPRGxIyJ2\nkE+cen9EbCcnfce8WI8Ba+va1pJPKgT/nI+Ey4HPpJS+kVL6aUrpJvKdlD9S+94xH1mjOb4DPTQU\nGvhv0DLBovYvvt4HnAF7PeCstIepVEktVLwJ+N2U0sN9v0sprSP/weg7vgeSj631jm83+USevn2O\nJf+l3fvQuB8CsyJiQZ/NLyb/wV9V5M9TASvIZ06fDJxUW+4CbgROSik9gGNetH/n+YdGjwUeAv+c\nj5Cp5H/09bWb2u8Px3xkjfL4/hB4SeTHdvR6DbCZPFs17KJbZgHeCmxl78tNNwGHlF1bqy/kwx+/\nAk4nJ8zeZUqfPn9RG8+zyb8QvwX8jL0vWVpKvpzvVeR/kf87z79k6Z/Jv0BPJR9uuRf4atlj0AoL\nz78qxDEvdnxPIZ/9/hHgheQp+qeAtznmIzbm15FPAnw9+fLec8jH6j/tmBc2xtPI/zA5mRzaPlD7\nPG80x5ccFteQL0s9kXzVyHrgrxv6ecoe0H4G+L3ka3GfJaenU8quqQpL7Q/jrn6Wd9b1+yT50qWt\n5LN9X1T3/f7A35MPTT0FfAM4tK7PLPK/yjeTw8wXgallj0ErLMAP6BMsHPMRGePXA/9RG8+fAhf0\n08cxL268p5GfRr0OeKb2C+1T1F1+6Jjv0xifMcDf4V8Z7fElP0T0u8DT5FDxN8CERn4eH0ImSZIK\n0zLnWEiSpOozWEiSpMIYLCRJUmEMFpIkqTAGC0mSVBiDhSRJKozBQpIkFcZgIUmSCmOwkCRJhTFY\nSJKkwhgsJElSYf4/arU0Wk08WvwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb38400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is how your class should be called.\n",
    "X = load_iris().data\n",
    "y = (load_iris().target >= 1).astype(int).reshape(-1,1)\n",
    "logreg = LogReg()\n",
    "logreg.fit(X,y)\n",
    "print(accuracy_score(logreg.predict(X),y.flatten()))\n",
    "logreg.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
