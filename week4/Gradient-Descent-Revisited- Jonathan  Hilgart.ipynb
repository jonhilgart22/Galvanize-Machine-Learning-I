{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Place import statements here\n",
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent - Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will lay out the process here in detail. I did not lay out any pseudocode for the formulas. If you are struggling to program the formulas, please talk to me or Mike.\n",
    "If there are any issues or questions let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the hypothesis: \n",
    "\n",
    "$$ h(x_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i3})}} $$\n",
    "\n",
    "Below, you will find a function defining the hypothesis. Fill it out using the formula from above. To check if it is correct, use the test variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not change the function names if you want to use the assert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hypothesis(X,coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X : (numpy 2D array) containing the input data\n",
    "        -coeffs: (column vector) containing the coefficients being used to get a hypothesis\n",
    "    Output:\n",
    "        -Hypothesis: (column vector) containing hypothesis values\n",
    "    \n",
    "    You can generate hypothesis values using the sigmoid (i.e. the formula from above)\n",
    "    '''\n",
    "    return   1/(1+np.exp(-(np.dot(X,coeffs))))\n",
    "\n",
    "    \n",
    "\n",
    "#Remember that you can call this function in later parts of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to test your hypothesis function\n",
    "test_x,test_coeffs = np.array([1,1,1]).reshape(1,-1),np.array([1,1,1]).reshape(-1,1)\n",
    "assert_almost_equal(0.95257413,hypothesis(test_x,test_coeffs)[0][0],decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the cost function for logistic regression\n",
    "\n",
    "$$ J(\\beta) = - \\frac{1}{n} \\sum_{i = 1}^{n} \\left[ y_i log(h(x_i)) + (1 - y_i) log(1 - h(x_i)) \\right] $$  \n",
    "\n",
    "Don't forget to take the log of the prediction or else the assert statement will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(X,y,coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -y: (column vector) containing the labels for the input data\n",
    "        -coeffs: (column vector) containing the coefficient values\n",
    "    Ouput:\n",
    "        -Float Value: the cost associated with using certain coefficient values\n",
    "\n",
    "    Implement the cost function from above.\n",
    "    '''\n",
    "    \n",
    "    h = hypothesis(X,coeffs)\n",
    "\n",
    "\n",
    "    length = len(X)\n",
    "\n",
    "\n",
    "    cost = -(1/length)*sum(y*(np.log(h)) + (1-y)*np.log(1-h))\n",
    "\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to test your cost function\n",
    "test_x,test_coeffs, test_y = np.array([1,2,3]).reshape(-1,1),np.array([0]).reshape(-1,1), np.array([0,0,0]).reshape(-1,1)\n",
    "assert_almost_equal(cost(test_x,test_y,test_coeffs),0.69314718055994518, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the gradient formula:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_j} J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h(x_i) - y_i \\right) x_{ij}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = np.array([[1,2],\n",
    "                   [2,3],\n",
    "                   [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(X,y,coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -y: (column vector) containing the labels for the input data\n",
    "        -coeffs: (column vector) containing the coefficient values\n",
    "    Output:\n",
    "        -Gradient: the gradient for each of the coefficients\n",
    "    \n",
    "    Use the formula above to calculate gradient\n",
    "    '''\n",
    "    # Some help from below\n",
    "    #https://bryantravissmith.com/2015/12/29/implementing-logistic-regression-from-scratch-part-2-python-code/\n",
    "    \n",
    "\n",
    "    rows,columns = np.shape(X)\n",
    "\n",
    "    \n",
    "    h = hypothesis(X,coeffs)\n",
    "\n",
    "\n",
    "    grad = []\n",
    "    error = h-y\n",
    "    product = error*X\n",
    "    \n",
    "#     print(np.array(sum(product).reshape(coeffs.shape)/rows))\n",
    "    return np.array(sum(product).reshape(coeffs.shape)/rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to figure out if your implementation of the gradient is correct\n",
    "test_x,test_coeffs, test_y = np.array([1,2,3]).reshape(-1,1),np.array([0]).reshape(-1,1), np.array([0,0,0]).reshape(-1,1)\n",
    "assert_almost_equal(gradient(test_x,test_y,test_coeffs)[0][0],1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(test_x,test_y,test_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predict function which will predict any values strictly greater than 0.5 to be 1 when the hypothesis function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X, coeffs):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -coeffs: (column vector) containing the coefficient values\n",
    "    Output:\n",
    "        -Predictions: predicted values using a threshold of 0.5\n",
    "    '''\n",
    "    \n",
    "    threshold = .5\n",
    "    bool_t = hypothesis( X, coeffs)>threshold \n",
    "    final=[]\n",
    "    for item in bool_t:\n",
    "        if item ==True:\n",
    "            final.append(1)\n",
    "        else:\n",
    "            final.append(0)\n",
    "            \n",
    "   \n",
    "    return np.array(final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the code below to figure out if your implementation of the predict function is correct\n",
    "test_x,test_coeffs = np.array([1,1,1]).reshape(1,-1),np.array([1,1,1]).reshape(-1,1)\n",
    "assert_almost_equal(predict(test_x, test_coeffs),1,decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to implement gradient descent in the fit function (we are going to treat our class like an sklearn class). For now to see if everything is working we will just implement gradient descent (and tweak some of the code to take care of some functionality we might want). Below is the algorithm for gradient descent.\n",
    "\n",
    "    Gradient Descent:\n",
    "        input: J: optimization function (cost function)\n",
    "               alpha: learning rate\n",
    "               n: number of iterations\n",
    "        output: local minimum of optimization function J\n",
    "\n",
    "        initialize b (often as all 0's)\n",
    "        costs = []\n",
    "        repeat for n iterations:\n",
    "            update b as b - alpha * gradient(J)\n",
    "            append costs for updated b values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,alpha):\n",
    "    '''\n",
    "    Input:\n",
    "        -X: (numpy 2D array) containing the input data\n",
    "        -y: (column vector) containing the labels for the input data\n",
    "        -alpha: (int) the step-size to multiply the gradient by\n",
    "    Output:\n",
    "        Coefficients - coefficients calculated by gradient descent\n",
    "        Costs - the cost associated with each update of the coefficients\n",
    "    \n",
    "    For this implementation just use 10,000 iterations. MAKE SURE YOU RETURN 2 VALUES!\n",
    "    '''\n",
    "\n",
    "    coeffs=np.zeros(np.shape(X)[1]).reshape(np.shape(X)[1],1)\n",
    "        # * Recall that there should be as many coefficients as features.\n",
    "\n",
    "        # I give you this line here. if fit_intercept = True, set the intercept to 0\n",
    "#         if self.fit_intercept:\n",
    "#             self.coeffs = np.insert(self.coeffs, 0, 0)\n",
    "\n",
    "        # * for each of the num_iterations, update self.coeffs at each step.\n",
    "    num_interations = 5000\n",
    "    grad_cost= []\n",
    "    #def cost(X,y,coeffs):\n",
    "    #X = X.T\n",
    "    \n",
    "    for n in range(num_interations):\n",
    "   \n",
    "        #update b as b - alpha * gradient(J)\n",
    "        coeffs = coeffs- alpha*gradient(X,y,coeffs)\n",
    "        grad_cost.append(cost(X,y,coeffs))\n",
    "    grad_cost = np.array(grad_cost)\n",
    "    #print(grad_cost,'grad cost')\n",
    "    #print(coeffs, ' coeffs')\n",
    "    \n",
    "    \n",
    "    return coeffs, grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1/(1+np.exp(-(np.dot(X,coeffs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x103339048>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYZiZWS5OGGZmlsv/Bwz0+VI2mg07AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c191978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment the code below to figure out if your implementation of gradient_descent is correct\n",
    "# The code depends on your functions from above so make sure those are correct\n",
    "# Use should be generating a plot of the costs it should look like the graph below.\n",
    "\n",
    "\n",
    "test_x, test_alpha, test_y = load_iris().data, 0.01, (load_iris().target >= 1).astype(int).reshape(-1,1)\n",
    "coeffs, costs = gradient_descent(test_x,test_y,test_alpha)\n",
    "assert_almost_equal(accuracy_score(predict(test_x,coeffs),test_y.flatten()),1.0, decimal = 3)\n",
    "plt.plot(costs)\n",
    "plt.xlabel('iteration number')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function over 5k iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts of gradient descent, let's put this in a class. PLEASE DO NOT JUST COPY AND PASTE YOUR FUNCTIONS FROM ABOVE. You should use them to ensure your implementation is correct, but the class should be set up differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun Fact: Notice there are functions that have 1 underscore before them (for example __hypothesis). In python programming, these are usually considered to be private functions. Unlike C/C++, these functions can be called by users (so they are not really private). The idea behind private functions is that a user would most likely never call this function, but you need it in order to perform operations for functions that users will call. For example, it can be argued that users will never care about what the gradient  or hypothesis for a given example will be. Thus, you can keep these hidden, as users will most likely only care about fitting the model and predicting on new values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2],\n",
    "             [1,2],\n",
    "             [0,1],\n",
    "             [2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[1 1 0 2] i\n",
      "[ 0.  0. -1.  1.] mean out\n",
      "[ 0.          0.         -1.41421356  1.41421356] calc\n",
      "[2 2 1 4] i\n",
      "[-0.25 -0.25 -1.25  1.75] mean out\n",
      "[-0.22941573 -0.22941573 -1.14707867  1.60591014] calc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.22941573],\n",
       "       [ 0.        , -0.22941573],\n",
       "       [-1.41421356, -1.14707867],\n",
       "       [ 1.41421356,  1.60591014]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.zeros((np.shape(x)))\n",
    "print(z)\n",
    "for col_idx,i in enumerate(x.T):\n",
    "    print(i,'i')\n",
    "    print(i-np.mean(i),'mean out')\n",
    "    print((i-np.mean(i))/np.std(i),'calc')\n",
    "    \n",
    "    z.T[col_idx] = np.array((i-np.mean(i))/np.std(i))\n",
    "    \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \n",
    "    def __init__(self, alpha = 0.01,num_iterations = 10000, fit_intercept = True, normalize = False):\n",
    "        # I have given the inputs that this class should take. Please set them as class \n",
    "        # attributes so that you can call them throughout the class\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        \n",
    "        # Now add a coefficients variable so it does not need to be passed from function to function\n",
    "        \n",
    "        self.coeffs = None\n",
    "        # Also, add a scores variable which is a list that will hold the costs associated with\n",
    "        # each update of the coefficients\n",
    "        \n",
    "        if self.normalize:\n",
    "            self.mu = None\n",
    "            self.sigma = None\n",
    "        if self.fit_intercept:\n",
    "            self.n=None\n",
    "            self.m=None\n",
    "        \n",
    "        # You might want to store mu and sigma if normalize is true. For now leave them as None\n",
    "        \n",
    "        # Another thing you can do is store n and m (number of rows and columns in X). However,\n",
    "        # be careful! If fit intercept is true then the value m will change. Store them as None for now. \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "            -y: (column vector) containing the labels for the input data\n",
    "        Output:\n",
    "            None\n",
    "\n",
    "        This is where gradient descent will be happening. However, notice we are just inputting \n",
    "        X and Y. Thus the other variables you will have to get from the attributes of the class.\n",
    "        '''\n",
    "        # If fit_intercept is true add a column of ones before the data. I will give you this code below.\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        \n",
    "        # If normalize is true, normalize the data.\n",
    "\n",
    "\n",
    "\n",
    "        if self.normalize:\n",
    "            norm_x = np.zeros((np.shape(X)))\n",
    "                \n",
    "            for col_idx,col in enumerate(X.T):\n",
    "                norm_x.T[col_idx]=(col-np.mean(col))/np.std(col)\n",
    "            \n",
    "        \n",
    "            X = norm_x\n",
    "        else:\n",
    "            pass\n",
    "        # Now that we are done with transformations, save the n and m values.\n",
    "        \n",
    "        # Also, initialize coefficients to be 0\n",
    "      \n",
    "        \n",
    "        # Finally, perform gradient descent. Get num_iterations and alpha from the atrributes\n",
    "        # Don't forget to store the costs for the updated coefficients\n",
    "        #############gradient descent\n",
    "        \n",
    "        self.coeffs=np.zeros(np.shape(X)[1]).reshape(np.shape(X)[1],1)\n",
    "        print(np.shape(self.coeffs),'shape of coefficients')\n",
    "\n",
    "        #self.num_iteration\n",
    "        self.grad_cost = []\n",
    "        #def cost(X,y,coeffs):\n",
    "        #X = X.T\n",
    "\n",
    "        for n in range(self.num_iterations):\n",
    "\n",
    "            #update b as b - alpha * gradient(J)\n",
    "            self.coeffs = self.coeffs- self.alpha*gradient(X,y,self.coeffs)\n",
    "            self.grad_cost.append(cost(X,y,self.coeffs))\n",
    "        self.grad_cost = np.array(self.grad_cost)\n",
    "        \n",
    "        #print(grad_cost,'grad cost')\n",
    "        #print(coeffs, ' coeffs')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def hypothesis(X,coeffs):\n",
    "        '''\n",
    "        Input:\n",
    "            -X : (numpy 2D array) containing the input data\n",
    "            -coeffs: (column vector) containing the coefficients being used to get a hypothesis\n",
    "        Output:\n",
    "            -Hypothesis: (column vector) containing hypothesis values\n",
    "\n",
    "        You can generate hypothesis values using the sigmoid (i.e. the formula from above)\n",
    "        '''\n",
    "        return   1/(1+np.exp(-(np.dot(X,coeffs))))\n",
    "\n",
    "\n",
    "    def _gradient(self,X,y):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "            -y: (column vector) containing the labels for the input data\n",
    "        Output:\n",
    "            -Gradient: the gradient for each of the coefficients\n",
    "        '''        \n",
    "        rows,columns = np.shape(X)\n",
    "\n",
    "    \n",
    "        h = hypothesis(X,coeffs)\n",
    "\n",
    "\n",
    "        grad = []\n",
    "        error = h-y\n",
    "        product = error*X\n",
    "\n",
    "        #     print(np.array(sum(product).reshape(coeffs.shape)/rows))\n",
    "        return np.array(sum(product).reshape(self.coeffs.shape)/rows)\n",
    "    \n",
    "    \n",
    "    def _cost(self,X,y):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "            -y: (column vector) containing the labels for the input data\n",
    "        Ouput:\n",
    "            -Float Value: the cost associated with using certain coefficient values\n",
    "        '''\n",
    "        h = hypothesis(X,self.coeffs)\n",
    "\n",
    "\n",
    "        length = len(X)\n",
    "\n",
    "\n",
    "        cost = -(1/length)*sum(y*(np.log(h)) + (1-y)*np.log(1-h))\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Input:\n",
    "            -X: (numpy 2D array) containing the input data\n",
    "        Output:\n",
    "            -Predictions: predicted values using a threshold of 0.5\n",
    "        '''        \n",
    "        threshold = .5\n",
    "        bool_t = hypothesis( X, coeffs)>threshold \n",
    "        final=[]\n",
    "        for item in bool_t:\n",
    "            if item ==True:\n",
    "                final.append(1)\n",
    "            else:\n",
    "                final.append(0)\n",
    "\n",
    "\n",
    "        return np.array(final)     \n",
    "    \n",
    "    def plot_scores(self):\n",
    "        '''\n",
    "        Input:\n",
    "            -None\n",
    "        Ouput:\n",
    "            -Line plot showing how scores decrease\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        plt.plot(self.grad_cost)\n",
    "        plt.xlabel('iteration number')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost Function over {} iterations'.format(self.num_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) shape of coefficients\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4gAxM7O6OEDMzKwuDhAzM6vL/wcMzayL6q+lrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103334048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is how your class should be called.\n",
    "X = load_iris().data\n",
    "y = (load_iris().target >= 1).astype(int).reshape(-1,1)\n",
    "logreg = LogReg()\n",
    "logreg.fit(X,y)\n",
    "print(accuracy_score(logreg.predict(X),y.flatten()))\n",
    "logreg.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
