{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RAT 4.2\n",
    "\n",
    "1 - \n",
    "\n",
    "A recap of regularization: Describe weaknesses and strengths of Lasso, Ridge and Elstic Net Regularization\n",
    "\n",
    "2 - \n",
    "\n",
    "Discuss in plain English the Regularization Paths (\"Squid\" plots) in terms of each of the above three regularization techniques. For reference, here is what a regularization path looks like: \n",
    "\n",
    "![Squid Plots](http://scikit-learn.sourceforge.net/0.6/_images/plot_lasso_coordinate_descent_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) Lasso (L1 penalty) helps  create sparsity in the matrix by only including the most important terms. In addition, if there are terms that are highly correlated, lasso will only pick one of these terms. On the plus side, it helps with shrinkage of the features\n",
    "\n",
    "2) Ridge regularization (L2 penalty) is good if you want to include ALL of the features in your matrix. In addition, it is generally faster to compute. However, it does not induce sparsity into the matri which might be a problem for high dimensional matrices (see Curse of DImensionality).\n",
    "\n",
    "3) Elastic Net regularization attempts to combine the best features from both Lasso and Ridge regression by determining what 'percent' of the regression should use a ridge penalty (l2) vs. a lasso penalty (l1). There is no limitation to the number of of selected varaibles (only the important ones will be included). The con is that the features undergo double shrinkage (becuase they are being penalized by both the L1 and L2 penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Regularization Path\n",
    "\n",
    "- A Regularization path shows the sequence of weights assigned to the features for different 'learning rates' (typically alpha) or 'percent of L1 and L2 penalty' (typicall Lambda and 1-Lambda). For each regression, we can see that lasso tends to have hard corners in the weights (due to the way penalization works), ridge will tend to have very smooth weights and elastic net will be a combination of both.\n",
    "\n",
    "- What we want to see is the weights that are constant after a certain lambda value. These are your strongest features.  In addition, if the features are normalized (0 mean and 1 variance), then the weights show you variable importance given a different lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
