{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 4.2 Practicum: Naive Bayes\n",
    "\n",
    "In this exercise you will implement Naive Bayes classification in Python. \n",
    "\n",
    "##Background\n",
    "\n",
    "- Naive Bayes primarily relies on the Bayes Theorem:\n",
    "\n",
    "  $$p(y|x) = \\frac{p(x|y) \\times p(y)}{p(x)}$$\n",
    "\n",
    "  <br>\n",
    "\n",
    "  where \n",
    "\n",
    "  - $p(y|x)$ is the probability of observing a particular label / class given the data (posterior)\n",
    "  - $p(x|y)$ is the probability of observing the data given a particular label / class (likelihood)\n",
    "  - $p(y)$ is the probability of observing the a particular label / class (prior)\n",
    "  - $p(x)$ is the probability of observing the data\n",
    "\n",
    "  <br>\n",
    "\n",
    "- It is assumed that $p(x)$ is constant, and therefore we can ignore the term and rewrite the formulation for Naive Bayes as:\n",
    "\n",
    "  $$p(y|x) \\propto p(x|y) \\times p(y)$$\n",
    "\n",
    "  <br>\n",
    "\n",
    "- In more concrete terms, we can express the likelihood of observing the data as the joint probability of observing all the features in the data:\n",
    "\n",
    "  $$p(x|y) = p(x_i|y) \\cdot p(x_{i+1}|y) \\cdot p(x_{i+2}|y) \\cdot \\text{...} \\cdot p(x_n|y)$$\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "- We would compute the likelihood based on exisiting data and set a prior based on the class distribution\n",
    "- Based on the likelihood and prior, we can then compute the probability observing a certain class given I have observed feature i two times and  feature i+1 3 times:\n",
    "\n",
    "  $$p(y|x) \\propto p(x_i|y)^2 \\times p(x_{i+1}|y)^3 \\times p(y)$$\n",
    "\n",
    "  <br>\n",
    "\n",
    "- To take the log form of the above formulation, we will get:\n",
    "\n",
    "  $$log(p(y|x)) \\propto 2log(p(x_i|y)) + 3log(p(x_{i+1}|y)) + log(p(y))$$\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "- The general form to compute the posterior would be:\n",
    "\n",
    "  $$log(p(y|x)) \\propto \\sum_{i=1}^n  x_i log(p(x_i|y)) + log(p(y))$$\n",
    "\n",
    "  <br>\n",
    "  \n",
    "- To compute the likelihood of observing a certain feature given a class, $p(x_i|y)$:\n",
    "\n",
    "  $$p(x_i|y) = \\frac{S_{y,i} + \\alpha}{S_y + \\alpha p}$$\n",
    "  \n",
    "  where \n",
    "  - p is the number of features\n",
    "  - $\\alpha$ is a smoothing terming which prevents undefined probability, usually set to 1\n",
    "  - $S_{y,i}$ is the sum of all of the $i^{th}$ features for all the datapoints in class $y$\n",
    "  - $S_y$ is the sum of all of the features for all the datapoints in class $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
