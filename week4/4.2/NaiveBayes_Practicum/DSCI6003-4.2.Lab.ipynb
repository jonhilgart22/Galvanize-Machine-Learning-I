{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 4.2 Practicum: Naive Bayes\n",
    "\n",
    "In this exercise you will implement Naive Bayes classification in Python. You should rely primarily on counters and dictionaries instead of numpy arrays for this implementation.\n",
    "\n",
    "Recall the formulas we use for Naive Bayes:\n",
    "\n",
    "![likelihood](images/likelihood2.png)\n",
    "\n",
    "Let's unpack this a bit.  The numerator is the number of times a word from the document in question appears in each class from the training set plus a Laplace smoother.  The denominator is the total number of words in each class from the training set with additional smoothing.\n",
    "\n",
    "Notice that these probabilities are simply the probability that the word you are investigating would be drawn at random from all of the documents in a given class (with smoothing).\n",
    "\n",
    "And here's how we calculate the probability that the document in question belongs to a class:\n",
    "\n",
    "![posterior](images/posterior.png)\n",
    "\n",
    "Here we determine the probability of a class given a document.  This probability is given by the frequency of each class in the training set `P(y)` times the sum of the probabilities that each word in the document would be drawn at random from the class you are investigating `sum(P(x_i|y))`.  You will need one of these probabilities for each class in your training set.  Choose the class with the largest probability.\n",
    "\n",
    "The summation here explains that we need to sum the probabilities for each word in the document we are investigating. <a href='http://scikit-learn.org/stable/modules/naive_bayes.html'>Sklearn's formulation is pretty good too.</a>\n",
    "\n",
    "1. Open code\\naive_bayes.py and look at the 'fit' method in the NaiveBayes class definition. This method calculates the prior probabilities.  In this case the prior is just the frequency of each class in the training set.\n",
    "\n",
    "2. Implement the `_compute_likelihood` method. This is the majority of work we will need to do to train the model. Go to the test file for this practicum and see what the input for the model will look like.\n",
    "\n",
    "    * The `class_counts` attribute should contain the total number of samples in all the features for each class. This is denominator (minus the smoothing) from above. The keys should be the classes.\n",
    "\n",
    "    * The `class_feature_counts` attribute should contain the number of occurrences of each word (feature) for each class. This is a dictionary of dictionaries (technically a defaultdict of Counters). This is numerator from above. You should be able to access this dictionary like this: `class_feature_counts[class y][feature j]`.\n",
    "\n",
    "    This is in fact all that we need to precompute. We will be doing the Laplace smoothing when we do predictions. As you go, you can run `nosetests tests/test_nb.py` to verify you've correctly implemented each method.\n",
    "\n",
    "3. Implement the `posteriors` method. For each row in the feature matrix `X` and for each potential label, you will need to calculate the log likelihood. You should follow the formula from above.\n",
    "\n",
    "4. The `predict` method then returns the class with the largest probability for each data point. Implement this in the `predict` method. \n",
    "\n",
    "5. Run `nosetests tests/test_nb.py` to verify you've correctly implemented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reach Goals\n",
    "\n",
    "1. Now that you can take in text and classify as being from a certain document, try using your implementation of Naive Bayes on the mini20-train and mini20-test data. More information about these datasets can be found <a href = http://ana.cachopo.org/datasets-for-single-label-text-categorization>at this webpage</a>.\n",
    "\n",
    "2. This time, modify your code to read in tf-idf data. In the spam.csv folder, there is a tf-idf representation of e-mails, some of which are spam. Use your version of Naive Bayes to classify the e-mails, then check it against sklearn's Multinomial NB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to implement Naive Bayes with a tf-idf vector as an input.\n",
    "\n",
    "## Background\n",
    "\n",
    "- Naive Bayes primarily relies on the Bayes Theorem:\n",
    "\n",
    "  $$p(y|x) = \\frac{p(x|y) \\times p(y)}{p(x)}$$\n",
    "\n",
    "  <br>\n",
    "\n",
    "  where \n",
    "\n",
    "  - $p(y|x)$ is the probability of observing a particular label / class given the data (posterior)\n",
    "  - $p(x|y)$ is the probability of observing the data given a particular label / class (likelihood)\n",
    "  - $p(y)$ is the probability of observing the a particular label / class (prior)\n",
    "  - $p(x)$ is the probability of observing the data\n",
    "\n",
    "  <br>\n",
    "\n",
    "- It is assumed that $p(x)$ is constant, and therefore we can ignore the term and rewrite the formulation for Naive Bayes as:\n",
    "\n",
    "  $$p(y|x) \\propto p(x|y) \\times p(y)$$\n",
    "\n",
    "  <br>\n",
    "\n",
    "- In more concrete terms, we can express the likelihood of observing the data as the joint probability of observing all the features in the data:\n",
    "\n",
    "  $$p(x|y) = p(x_i|y) \\cdot p(x_{i+1}|y) \\cdot p(x_{i+2}|y) \\cdot \\text{...} \\cdot p(x_n|y)$$\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "- We would compute the likelihood based on exisiting data and set a prior based on the class distribution\n",
    "- Based on the likelihood and prior, we can then compute the probability observing a certain class given I have observed feature i two times and  feature i+1 3 times:\n",
    "\n",
    "  $$p(y|x) \\propto p(x_i|y)^2 \\times p(x_{i+1}|y)^3 \\times p(y)$$\n",
    "\n",
    "  <br>\n",
    "\n",
    "- To take the log form of the above formulation, we will get:\n",
    "\n",
    "  $$log(p(y|x)) \\propto 2log(p(x_i|y)) + 3log(p(x_{i+1}|y)) + log(p(y))$$\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "- The general form to compute the posterior would be:\n",
    "\n",
    "  $$log(p(y|x)) \\propto \\sum_{i=1}^n  x_i log(p(x_i|y)) + log(p(y))$$\n",
    "\n",
    "  <br>\n",
    "  \n",
    "- To compute the likelihood of observing a certain feature given a class, $p(x_i|y)$:\n",
    "\n",
    "  $$p(x_i|y) = \\frac{S_{y,i} + \\alpha}{S_y + \\alpha p}$$\n",
    "  \n",
    "  where \n",
    "  - p is the number of features\n",
    "  - $\\alpha$ is a smoothing terming which prevents undefined probability, usually set to 1\n",
    "  - $S_{y,i}$ is the sum of all of the $i^{th}$ features for all the datapoints in class $y$\n",
    "  - $S_y$ is the sum of all of the features for all the datapoints in class $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3env]",
   "language": "python",
   "name": "Python [py3env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
