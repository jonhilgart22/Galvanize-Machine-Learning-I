{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 4.1 Lecture\n",
    "\n",
    "# A review of estimators: MAP, MOM, and MLE\n",
    "\n",
    "\n",
    "## By the end of this Lecture you will be Able to:\n",
    "1. Construct a new estimator using any of the above methods\n",
    "2. Describe the intuition of Naive Bayes\n",
    "\n",
    "\n",
    "### The Devils in the Details\n",
    "\n",
    "The development of a new machine learning model always seems to happen in concert with its cost (or \"objective\" or \"loss\") function. This is often due to the fact that the optimization (as opposed to tuning) of an algorithm is by far the hardest part of getting it to work for you. The process of choosing the algorithm you hope to use to optimize is directly related to the nature of the cost function. This makes the cost function, and understanding how you go about getting it, one of the most important aspects to understanding machine learning.\n",
    "\n",
    "In this lecture we will revisit this topic both in more depth and generality. In particular we focus on the problem of obtaining a parameter $\\theta$ for a model given a set of experimental observations, $X$.\n",
    "\n",
    "### References\n",
    "https://www.youtube.com/watch?v=pIIEmUEnjhY  \n",
    "https://en.wikipedia.org/wiki/Generalized_method_of_moments  \n",
    "http://home.uchicago.edu/~lhansen/palgrave.pdf - Hansen's summary paper (winner of 2013 nobel in econ for this work)  \n",
    "https://en.wikipedia.org/wiki/Law_of_large_numbers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOM \n",
    "\n",
    "The method-of-moments (MOM), often most correctly known as the GMM or generalized method-of-moments, is a way of getting an estimate for a model parameter when there too difficult to conduct a likelihood (probabilistic) analysis. \n",
    "\n",
    "GMM is an estimator constructed on minimal assumptions, (e.g. the BLUE conditions) making it possible to use in cases where we know very little about the data itself. It is relatively common in econometrics where the system under study is so large and relationships within are so vague as to render standard assumptions impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moments: A refresher\n",
    "\n",
    "Moments are a way of producing a description of distribution behavior in terms of its mean. The moment of $X$ of order $n$ about $a$ is given as the expectation value of $(X-a)^{n}$:\n",
    "\n",
    "$$\\mathbb{E}[(X-a)^{n}]$$\n",
    "\n",
    "( recall that for a discrete variable, $\\mathbb{E}[x] = \\sum x p(x)$ and for a continuous variable over $ a \\leq x \\leq b$, $\\mathbb{E}[x] = \\int_{a}^{b} xp(x)dx$\n",
    "\n",
    "Moments about 0 are called the *raw moments*. Moments about the mean, $\\mu_{x}$ are called the *central moments*. In theory, if we had all the moments to a distribution, we could calculate the distribution itself. \n",
    "\n",
    "(It is common to also call the measured moment $\\mu$ as it is itself an average.)\n",
    "\n",
    "We are all familiar with the *sample moments* regarding a sequence of measurements of a random variable, for example the official estimated mean:\n",
    "\n",
    "$$\\mu = \\dfrac{1}{n}\\sum_{i}^{n}{[X_{i}]}$$\n",
    "\n",
    "in general, the $k$th sample moment is given by: \n",
    "\n",
    "$$m_{k} = \\dfrac{1}{n}\\sum_{i}^{n}{[X_{i}^{k}]}$$\n",
    "\n",
    "These are both estimates of the actual means. By the law of large numbers, these estimates converge to the true values in the infinite sample condition. \n",
    "\n",
    "$\\lim\\limits_{x\\to \\infty} m_k = \\mu_{k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension to Estimates of Model parameters\n",
    "\n",
    "Extending the above concept out to more complex models, suppose that we choose to impute the probability density function of the sampled variable $X$ with a parameter $\\theta$ and a density function $f(x|\\theta)$.\n",
    "\n",
    "Then we can write the *$k$th theoretical moments $\\mu_{k}$* of the distribution using a similar formula as above:\n",
    "\n",
    "$$\\mathbb{E}[X^{k}]= \\sum x^{k} f(x|\\theta)$$\n",
    "\n",
    "and for a continuous variable over $ a \\leq x \\leq b$, \n",
    "\n",
    "$$\\mathbb{E}[X^{k}] = \\int_{a}^{b} x^{k} f(x|\\theta)dx$$\n",
    "\n",
    "In principle, we can equate the estimate of the expectation value to a mean for a given model, and thereby create an equation that solves for $\\theta$.\n",
    "\n",
    "### Example: \n",
    "\n",
    "Suppose you have the following distribution of discrete observations, with the given probability mass function $P[X,\\theta]$:\n",
    "\n",
    "$$X = [0,1,2,3,4]$$\n",
    "\n",
    "With the estimated probability of each X as follows:\n",
    "\n",
    "$$P(X|\\theta) = \\dfrac{2\\theta}{3}, \\dfrac{\\theta}{3}, \\dfrac{2(1-\\theta)}{3}, \\dfrac{(1-\\theta)}{3}, \\dfrac{(1-2\\theta)}{3}$$\n",
    "\n",
    "And we observe the following series of events X:\n",
    "\n",
    "X = {3, 3, 3, 1, 1, 0, 1, 1, 4, 2, 2, 1, 1}\n",
    "\n",
    "The mean ($\\mu_1$) of X is 1.3.\n",
    "\n",
    "The first theoretical (model) mean is:\n",
    "\n",
    "$$\\sum x f(x|\\theta) = 0*\\dfrac{2\\theta}{3}+1*\\dfrac{(\\theta)}{3}+2*\\dfrac{2(1-\\theta)}{3}+3*\\dfrac{(1-\\theta)}{3}+4*\\dfrac{(1-2\\theta)}{3} = \\dfrac{11}{3} - \\dfrac{14\\theta}{3}$$\n",
    "\n",
    "Setting these two means equal to each other, we get the following (likely rather bad) estimate:\n",
    "\n",
    "$$\\dfrac{11}{3} - \\dfrac{14\\theta}{3} = 1.3$$\n",
    "\n",
    "$$\\theta = 0.507$$\n",
    "\n",
    "## Generalization of MOM (GMM)\n",
    "\n",
    "Suppose we have $k$ $\\theta$s. In this case we cannot solve with a single measurement of mean (the first sample moment). We have to take this all the way up to the $k$th sample moment, in order to form $k$ simultaneous equations.\n",
    "\n",
    "$$\\mu_{1} = \\mathbb{E}[X]= \\sum x f(x|\\theta_{1}, \\theta_{2}, ..., \\theta_{k})$$\n",
    "$$\\mu_{2} = \\mathbb{E}[X^{2}]= \\sum x^{2} f(x|\\theta_{1}, \\theta_{2}, ..., \\theta_{k})$$\n",
    "$$\\mu_{3} = \\mathbb{E}[X^{3}]= \\sum x^{3} f(x|\\theta_{1}, \\theta_{2}, ..., \\theta_{k})$$\n",
    "$$ \\vdots $$\n",
    "$$\\mu_{k} = \\mathbb{E}[X^{k}]= \\sum x^{k} f(x|\\theta_{1}, \\theta_{2}, ..., \\theta_{k})$$\n",
    "\n",
    "This assumes, of course that we *can* get all of these moments from the sample of data that we have. This is not always possible. \n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "$$ X = [1,2,3]$$\n",
    "\n",
    "$$P(X) = \\dfrac{\\theta_{1}}{3}, \\dfrac{1+\\theta_{1}-\\theta_{2}}{3}, \\dfrac{1+2\\theta_{1}-2\\theta_{2}}{3}$$\n",
    "\n",
    "\n",
    "$$ X_{sample} = {1, 2, 1, 4, 1, 1, 2, 2, 3, 3, 4, 1, 2, 1}$$\n",
    "$$ X_{sample}^{2} = {1, 4, 1, 16, 1, 1, 4, 4, 9, 9, 16, 1, 4, 1}$$\n",
    "\n",
    "$$\\sum x f(x|\\theta)  = 1*\\dfrac{\\theta_{1}}{3}+2*\\dfrac{1+\\theta_{1}-\\theta_{2}}{3} + 3*\\dfrac{1+2\\theta_{1}-2\\theta_{2}}{3}$$\n",
    "\n",
    "$$\\sum x^{2} f(x|\\theta)  = 1*\\dfrac{\\theta_{1}}{3}+4*\\dfrac{1+\\theta_{1}-\\theta_{2}}{3} + 9*\\dfrac{1+2\\theta_{1}-2\\theta_{2}}{3}$$\n",
    "\n",
    "$$8\\theta_{2}-9\\theta_{1} = 5$$\n",
    "\n",
    "$$22 \\theta_{2}-23 \\theta_{1} = 13$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42857143],\n",
       "       [ 0.14285714]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "X_1_mean = np.mean([1, 2, 1, 4, 1, 1, 2, 2, 3, 3, 4, 1, 2, 1])\n",
    "X_2_mean = np.mean([1, 4, 1, 16, 1, 1, 4, 4, 9, 9, 16, 1, 4, 1])\n",
    "from numpy import linalg as la\n",
    "X = np.array([[-9, 8],[-23, 22]])\n",
    "y = [[5],[13]]\n",
    "\n",
    "la.inv(X).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to actual problems, it is most common to use a GMM method that sets up the equations and solves them as above, making small adjustments to enable invertibility.\n",
    "\n",
    "### Advantages\n",
    "* Simple to calculate\n",
    "* Consistent (if we keep increasing the sample size, we eventually obtain accurate estimates)\n",
    "### Disadvantages\n",
    "* Often biased\n",
    "* Sometimes gives estimates outside the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE\n",
    "\n",
    "Maximum-a-posteriori (MAP) and Maximum Likelihood  estimators (MLE) emerge from the same basic principle, Bayes' law. Bayes' law gives us a relationship between the two conditionals for each variable $A$ and $B$:\n",
    "\n",
    "$$P(B|A) = \\dfrac{P(A|B)P(B)}{P(A)}$$\n",
    "\n",
    "### Formal definition of the probabilistic model\n",
    "\n",
    "When we define a (parametric) model, we typically refer to a closed-form function that yields a prediction $y$ given $X$ and a set of parameters $\\bf{\\theta}$ (in this case, we generalize this statement to mean a vector or list of parameters).\n",
    "\n",
    "In order to train the model, we must fit the function's parameters to produce the best fit between the experimentally-determined data and the output of the model.\n",
    "\n",
    "The maximum likelihood estimate attempts to simply optimize the parameters $\\theta$ of a model, attempting to maximize the prediction power of the model directly on the training set. Our formulation need only predict $X$ given the current set of model parameters, $\\bf{\\theta}$. \n",
    "\n",
    "In this case, we work to maximize the probability that we correctly find $y$ given $X$, \n",
    "\n",
    "$$f_{Y|X}(Y|X, \\theta)$$\n",
    "\n",
    "Where we find the estimated theta at the local maximum of the above expression:\n",
    "\n",
    "$$\\hat{\\theta} \\leftarrow argmax_{\\theta}(f_{Y|X}(Y|X, \\theta))$$\n",
    "\n",
    "The probability function $f_{Y|X}$ is constructed directly from a known distribution function. (This has a direct relationship to least-square estimation (LSE) where we try to minimize the sum of errors between the predicted and actual $y_i$...in other words, $\\sum_{i}^{N}(y_{i}-f_{Y|X})$ )\n",
    "\n",
    "### Example: Coin Flipping\n",
    "\n",
    "Recall that the probability of a sequence of $n$ events is given by the chain rule of probability:\n",
    "\n",
    "$$ P(A_{n}, A_{n-1}, \\cdots, A_{1}) = P(A_{n}| A_{n-1}, \\cdot, A_{1}) \\cdot P(A_{n-1}|A_{n-2}, \\cdots, A_{1})$$\n",
    "\n",
    "$$ \\prod\\limits_{k=1}^{n} P(A_{k}|\\cap_{j=1}^{k-1} A_{j})$$\n",
    "\n",
    "The probability of a single fair coin flip landing heads (or tails) can be written as:\n",
    "\n",
    "$p(x, \\mu) = \\mu$\n",
    "\n",
    "The probability of flipping the coin with a mean probability of heads $\\mu$ n times producing a sequence $X$ of $h$ heads and $t$ tails $(n = h+t)$ outcomes can be written:\n",
    "\n",
    "$$p(X, \\mu) = \\mu^{h}(1-\\mu)^{t}$$\n",
    "\n",
    "We can convert the n counts of heads and tails into a sequence of heads and tails, and so we commonly write the above equation (where each $x_i \\in \\{0, 1\\})$:\n",
    "\n",
    "$$p(X=x_{i}, \\mu) = \\mu^{x_{1},x_{2}, \\cdots, x_{n}}(1-\\mu)^{1-x_{1},x_{2}, \\cdots, x_{n}}$$\n",
    "\n",
    "$$p(X=x_{i}, \\mu) = \\prod\\limits_{i=1}^{n} \\mu^{x_{i}}(1-\\mu)^{1-x_{i}}$$\n",
    "\n",
    "This is the probability (likelihood) of obtaining a given sequence $X$, and a Bernoulli random variable.\n",
    "\n",
    "The above equation is pretty onerous to optimize directly, so we take the **log**-likelihood:\n",
    "\n",
    "$$ log{p(X=x_{i}, \\mu)} = \\sum\\limits_{i=1}^{n} x_{i} log \\mu + (1-x_{i}) log{(1-\\mu)}$$\n",
    "\n",
    "This equation provides us a closed-form solution for the log-likelihood, so we can solve it analytically:\n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial \\mu} log{p(X=x_{i}, \\mu)} =  \\sum\\limits_{i=1}^{n} \\dfrac{\\partial}{\\partial \\mu}[ x_{i} log{\\ \\mu} + (1-x_{i}) log{(1-\\mu)}]$$\n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial \\mu} log{p(X=x_{i}, \\mu)} =  \\sum\\limits_{i=1}^{n} x_{i}\\dfrac{\\partial}{\\partial \\mu} log{\\ \\mu} + (1-x_{i}) \\dfrac{\\partial}{\\partial \\mu} log{(1-\\mu)}$$\n",
    "\n",
    "$$ \\dfrac{\\partial}{\\partial \\mu} log{p(X=x_{i}, \\mu)} =  \\dfrac{1}{\\mu} \\sum\\limits_{i=1}^{n} x_{i} - \\dfrac{1}{1-\\mu} \\sum\\limits_{i=1}^{n} (1-x_{i})$$\n",
    "\n",
    "Set the above equation to 0:\n",
    "\n",
    "$$ 0 =  \\dfrac{1}{\\mu} \\sum\\limits_{i=1}^{n} x_{i} - \\dfrac{1}{1-\\mu} \\sum\\limits_{i=1}^{n} (1-x_{i})$$\n",
    "\n",
    "$$ \\dfrac{1}{1-\\mu} \\sum\\limits_{i=1}^{n} (1-x_{i})  =  \\dfrac{1}{\\mu} \\sum\\limits_{i=1}^{n} x_{i} $$\n",
    "\n",
    "$$ \\dfrac{1-\\mu}{\\mu} =  \\dfrac{\\sum\\limits_{i=1}^{n} (1-x_{i})}{\\sum\\limits_{i=1}^{n} x_{i}} $$\n",
    "\n",
    "$$ \\dfrac{1}{\\mu} - 1=  \\dfrac{\\sum\\limits_{i=1}^{n}1}{\\sum\\limits_{i=1}^{n} x_{i}} -1 $$\n",
    "\n",
    "$$ \\dfrac{1}{\\mu} = \\dfrac{n}{\\sum\\limits_{i=1}^{n} x_{i}}  $$\n",
    "\n",
    "$$ \\hat{\\mu} = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} x_{i}  $$\n",
    "\n",
    "So this estimate is (reassuringly) the average of the outcomes. However, is this really a quality estimation of the mean, given a limited sample, i.e.:\n",
    "\n",
    "$X = [H,T,T,T]$, $\\hat{\\mu} = 0.25$\n",
    "\n",
    "$X = [H,T,H,T]$, $\\hat{\\mu} = 0.50$\n",
    "\n",
    "$X = [T,T,T,T]$, $\\hat{\\mu} = 0 $\n",
    "\n",
    "Each of these is a viable outcome for only four trials, and so requires a very large number of samples (in the limit of the law of large numbers) in order to provide any accuracy. Also note that it provides no confidence on the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP\n",
    "\n",
    "MAP attempts to overcome the limitation of MLE in that it includes information about our beliefs regarding the distribution of parameter values. \n",
    "\n",
    "In order to use MAP, **we must be in a position to know something about the distribution of both the data and the prior distribution of parameters for the chosen model.** \n",
    "\n",
    "Typically we have a closed-form equation for this probability, in the form of a single function, and we wish to find the parameter set $\\bf{\\theta}$ that maximizes (argmax) the probability that we observe X, $f(X|\\theta)$:\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$\n",
    "\n",
    "$$\\hat{\\theta}_{MAP} = \\argmax_{\\theta} f(X|\\theta)$$\n",
    "\n",
    "To perform the above calculation we have to *maximize the probability of $\\theta$ given X, $f(\\theta|x)$*. \n",
    "In the case of MLE, we assume out of hand that we know the distribution of $\\theta$ beforehand. That is to say, we have (assume) that we know the functional form of the prior distribution of $\\theta$, $g(\\theta)$. Then our predicted **posterior** probability distribution $g(\\hat{\\theta})$ given our experimentally determined $x$ and yielding the estimated parameters $\\hat{\\theta}$ is (using bayes' law):\n",
    "\n",
    "$$\\hat{\\theta}_{MAP} = \\argmax_{\\theta} g(\\theta|x)= \\argmax_{\\theta}\\dfrac{f(x|\\theta)g(\\theta)}{\\int_{\\nu}f(x|\\nu)g(\\nu)d\\nu}$$\n",
    "\n",
    "Hence we are by definition *maximizing the posterior probability of $g$* in order to obtain the $\\theta$.\n",
    "\n",
    "The integral over the dummy variable $\\nu$ is commonly called the *partition function*. It is typically disregarded as it is a constant. You will almost always see the above equation written as follows, taking the numerator and ignoring the denominator:\n",
    "\n",
    "$$\\hat{\\theta}_{MAP} = \\argmax_{\\theta}f(x|\\theta)g(\\theta) = \\argmax_{\\theta} f(x, \\theta)$$\n",
    "\n",
    "Meaning that the MAP is an estimate of the joint distribution of $X$ and $\\theta$.\n",
    "\n",
    "\n",
    "### Getting the Argmax\n",
    "\n",
    "When applied to algebraic functions (simple problems), we typically take the derivative of the estimator to find the minimum. When applied to machine learning algorithms, we typically just take the maximum estimate given a set of input variables, making sure to include $g(\\theta)$. \n",
    "\n",
    "### Example: \n",
    "\n",
    "The better estimate of a coin flip...\n",
    "\n",
    "Note that we can make a clear comparison between MLE and MAP:\n",
    "\n",
    "MLE: $\\hat{X} \\leftarrow \\argmax_{\\theta}f_{Y|X}(Y|X)$\n",
    "\n",
    "MAP: $\\hat{X} \\leftarrow \\argmax_{\\theta}f_{Y|X}(Y|X)\\ f_{X}(X)$\n",
    "\n",
    "Leaving out the spare parameter $\\theta$.\n",
    "\n",
    "In the case of the coin flip, the parameter that we are looking for is the mean, $\\mu$. \n",
    "\n",
    "What we need is that function $f_{X}(X)$, which is the prior of the mean. How can we formulate a prior that works for us here?\n",
    "\n",
    "Assuming the coin is fair, we need a continuous distribution that describes the distribution of coin flip values. The distribution of choice here is the Beta distribution, the one describing a proportion of binomial values:\n",
    "\n",
    "![beta](./images/Beta_distribution_pdf.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the distribution is\n",
    "\n",
    "$$c = \\int_{0}^{1}\\theta^{\\alpha}(1-\\theta)^{n-\\alpha}d\\theta$$\n",
    "\n",
    "$$p(\\mu|\\theta) = \\dfrac{\\theta^{\\alpha}(1-\\theta)^{n-\\alpha}}{c}$$\n",
    "\n",
    "The parameters of choice would be $\\alpha = 1$, $n=2$, meaning that we have a slight belief that the coin is fair before we start flipping.\n",
    "\n",
    "So the $f_{X}(X)$ (in this case, $X$ is the $\\theta$ we are trying to predict):\n",
    "\n",
    "$$f_{\\theta}(\\theta) = \\dfrac{1}{d}\\theta(1-\\theta)$$\n",
    "\n",
    "Applying it to our problem, we are seeking to calculate $\\mu$:\n",
    "\n",
    "$$f_{\\mu}(\\mu) = \\dfrac{1}{d}\\mu(1-\\mu)$$\n",
    "\n",
    "Recall our likelihood again based on the number of heads, where tails = n-heads:\n",
    "\n",
    "$$f_{X|\\mu}(X|\\mu) = \\mu^{h}(1-\\mu)^{n-h}$$\n",
    "\n",
    "And now we can formulate the whole posterior equation, allowing us to update our belief (Bayesian belief) based on observations:\n",
    "\n",
    "$$f_{X|\\mu}(X|\\mu)f_{\\mu}(\\mu) = \\dfrac{1}{d}\\mu^{h+1}(1-\\mu)^{n-h+1}$$\n",
    "\n",
    "Logging it we get:\n",
    "\n",
    "$$log\\ f_{X|\\mu}(X|\\mu)f_{\\mu}(\\mu) = log\\ \\dfrac{1}{d}+ (h+1)log\\ \\mu + (n-h+1)log\\ (1-\\mu) $$\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial \\mu} log\\ f_{X|\\mu}(X|\\mu)f_{\\mu}(\\mu) = \\dfrac{h+1}{\\mu}-\\dfrac{n-h+1)(1-\\mu} = 0$$\n",
    "\n",
    "$$ \\dfrac{1-\\mu}{\\mu} = \\dfrac{n-h+1}{h+1} $$\n",
    "\n",
    "$$ \\dfrac{1}{\\mu} = \\dfrac{n-h+1+h+1}{h+1} = \\dfrac{n+2}{h+1} $$\n",
    "\n",
    "$$ \\hat{\\mu} = \\dfrac{h+1}{n+2} $$\n",
    "\n",
    "So note that after we get a tail (n=1, h=0), our belief that we will get a head is $\\dfrac{1}{3}$, as opposed to 0, as would be with MLE.\n",
    "\n",
    "$X = [H,T,T,T]$, $\\hat{\\mu}_{MAP} = 0.4$\n",
    "\n",
    "$X = [H,T,H,T]$, $\\hat{\\mu}_{MAP} = 0.50$\n",
    "\n",
    "$X = [T,T,T,T]$, $\\hat{\\mu}_{MAP} = 0.16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
