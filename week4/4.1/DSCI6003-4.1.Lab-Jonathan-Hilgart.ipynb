{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI6003 4.1 Lab\n",
    "\n",
    "\n",
    "### 1: KFold CV\n",
    "\n",
    "We will implement K-fold validation **on the training dataset** of the loan dataset.\n",
    "\n",
    "We're going to use the FICO Loan dataset. We want to predict whether or not you get approved for a loan of 12% interest rate given the FICO Score, Loan Length and Loan Amount. Here's the code to load the data:\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('data/loanf.csv')\n",
    "    y = (df['Interest.Rate'] <= 12).values\n",
    "    X = df[['FICO.Score', 'Loan.Length', 'Loan.Amount']].values\n",
    "    ```\n",
    "    \n",
    "`sklearn` has its own implementation of K-fold\n",
    "(`sklearn.cross_validation.cross_val_score()`).\n",
    "However, to ensure you have an understanding of K-fold, you will implement it\n",
    "here using the more general `KFold` class in `sklearn`.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. To do this you need to manage randomly sampling **k** folds.\n",
    "\n",
    "2. Properly combining those **k** folds into a test and training set on\n",
    "   your **on the training dataset**. Outside of the k-fold, there should be\n",
    "   another set which will be referred to as the **hold-out set**.\n",
    "\n",
    "3. Train your model on your constructed training set and evaluate on the given test set\n",
    "\n",
    "3. Repeat steps __2__ and __3__ _k_ times.\n",
    "\n",
    "4. Average your results of your error metric.\n",
    "\n",
    "5. Compare the MSE for a simple **single** test/train split to your K-fold cross validated error in `4.`.\n",
    "\n",
    "6. Plot a learning curve and test vs training error curve.\n",
    "   (You might want to use: [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html) which is scikit-learn's built-in\n",
    "   function for K-fold cross validation).  See [Illustration of Learning Curves](http://www.astro.washington.edu/users/vanderplas/Astr599/notebooks/18_IntermediateSklearn) for more details.  \n",
    "\n",
    "<div style=\"background: yellow; padding:10px\">\n",
    "Once you find the optimal hyperparameters, retrain on **ALL** of your data to create your final model.\n",
    "</div>\n",
    "\n",
    "### 2: ROC Curves \n",
    "\n",
    "One of the best ways to evaluate how a classifier performs is an ROC curve. (http://en.wikipedia.org/wiki/Receiver_operating_characteristic) \n",
    "\n",
    "![](images/roc_curve.png)\n",
    "\n",
    "To understand what is actually happening with an ROC curve, we can create one ourselves.  Here is pseudo code to plot it.\n",
    "\n",
    "The `probabilities` are values in (0,1) returned from Logistic Regression. The standard default threshold is 0.5 where 0-0.5 values are interpreted as the negative class and 0.5-1 values are predicted as the positive class.\n",
    "\n",
    "The `labels` are the true values.\n",
    "\n",
    "```\n",
    "function ROC_curve(probabilities, labels):\n",
    "    Sort instances by their prediction strength (the probabilities)\n",
    "    For every instance in increasing order of probability:\n",
    "        Set the threshold to be the probability\n",
    "        Set everything above the threshold to the positive class\n",
    "        Calculate the True Positive Rate (aka sensitivity or recall)\n",
    "        Calculate the False Positive Rate (1 - specificity)\n",
    "    Return three lists: TPRs, FPRs, thresholds\n",
    "```\n",
    "\n",
    "Recall that the true positive **rate** is\n",
    "\n",
    "```\n",
    " number of true positives     number correctly predicted positive\n",
    "-------------------------- = -------------------------------------\n",
    " number of positive cases           number of positive cases\n",
    "```\n",
    "\n",
    "and the false positive **rate** is\n",
    "\n",
    "```\n",
    " number of false positives     number incorrectly predicted positive\n",
    "--------------------------- = ---------------------------------------\n",
    "  number of negative cases           number of negative cases\n",
    "```\n",
    "\n",
    "We are going to be implementing the `roc_curve` function.\n",
    "\n",
    "Here's some example code that you should be able to use to plot the ROC curve with your function. This uses a fake dataset.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           n_clusters_per_class=2, n_samples=1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "tpr, fpr, thresholds = roc_curve(probabilities, y_test)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity, Recall)\")\n",
    "plt.title(\"ROC plot of fake data\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 3: ROC Curve Implementation\n",
    "\n",
    "1. Write an ROC curve function to compute the above in `roc_curve.py`.\n",
    "\n",
    "    It should take as input the predicted probabilities and the true labels.\n",
    "\n",
    "2. Run the above code to verify that it's working correctly. You can also validate your correctness against [scikit-learns built-in function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html).\n",
    "\n",
    "3. Let's see how the roc curve looks on a real dataset. We're going to use the FICO Loan dataset. We want to predict whether or not you get approved for a loan of 12% interest rate given the FICO Score, Loan Length and Loan Amount. Here's the code to load the data:\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('data/loanf.csv')\n",
    "    y = (df['Interest.Rate'] <= 12).values\n",
    "    X = df[['FICO.Score', 'Loan.Length', 'Loan.Amount']].values\n",
    "    ```\n",
    "\n",
    "    Make sure to split your data into training and testing using sklearn's [train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html).\n",
    "\n",
    "### 4: Case Study -- Graduate School Admissions\n",
    "\n",
    "The data we will be using is the admission data on Grad school acceptances we saw before.\n",
    "\n",
    "* `admit`: whether or not the applicant was admitted to grad. school\n",
    "* `gpa`: undergraduate GPA\n",
    "* `GRE`: score of GRE test\n",
    "* `rank`: prestige of undergraduate school (1 is highest prestige, ala Harvard)\n",
    "\n",
    "Remember, we will use the GPA, GRE, and rank of the applicants to try to predict whether or not they will be accepted into graduate school.\n",
    "\n",
    "#### 5: Treating the data with a model\n",
    "\n",
    "Now we're ready to try to fit our data with Logistic Regression and today evaluate it with a ROC curve.  Remember the following from earlier in class and use sklearn to fit a logisitc regression again:\n",
    "\n",
    "    * Use sklearn's [KFold cross validation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) and [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to calculate the average accuracy, precision and recall.\n",
    "\n",
    "        Hint: Use sklearn's implementation of these scores in [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
    "\n",
    "    * The `rank` column is numerical, but as it has 4 buckets, we could also consider it to be categorical. Use panda's [get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.reshape.get_dummies.html) to binarize the column.\n",
    "\n",
    "6. Make a plot of the ROC curve (using your function defined in Part 1).\n",
    "\n",
    "7. Is it possible to pick a threshold where TPR > 60% and FPR < 40%? What is the threshold?\n",
    "\n",
    "    *Note that even if it appears to be in the middle of the graph it doesn't make the threshold 0.5.*\n",
    "\n",
    "### 6: Using the Youden Index\n",
    "\n",
    "Youden's Index (sometimes called J statistic) is similar to the F1 score in that it is a single number that describes the performance of a classifier.\n",
    "\n",
    "$$J = Sensitivity + Specificity - 1$$\n",
    "\n",
    "$$where$$\n",
    "\n",
    "$$Sensitivity = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$Specificity = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "![](http://i.stack.imgur.com/ysM0Z.png)\n",
    "\n",
    "The J statistic ranges from 0 to 1:\n",
    "* 0 indicating that the classifier does no better than random\n",
    "* 1 indicating that the test performed perfectly\n",
    "\n",
    "It can be thought of as an improvement on the F1 score since it takes into account all of the cells in a confusion matrix.  It can also be used to find the optimal threshold for a given ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "`sklearn` has its own implementation of K-fold\n",
    "(`sklearn.cross_validation.cross_val_score()`).\n",
    "However, to ensure you have an understanding of K-fold, you will implement it\n",
    "here using the more general `KFold` class in `sklearn`.\n",
    "\n",
    "1. To do this you need to manage randomly sampling **k** folds.\n",
    "\n",
    "2. Properly combining those **k** folds into a test and training set on\n",
    "   your **on the training dataset**. Outside of the k-fold, there should be\n",
    "   another set which will be referred to as the **hold-out set**.\n",
    "\n",
    "3. Train your model on your constructed training set and evaluate on the given test set\n",
    "\n",
    "3. Repeat steps __2__ and __3__ _k_ times.\n",
    "\n",
    "4. Average your results of your error metric.\n",
    "\n",
    "5. Compare the MSE for a simple **single** test/train split to your K-fold cross validated error in `4.`.\n",
    "\n",
    "6. Plot a learning curve and test vs training error curve.\n",
    "   (You might want to use: [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html) which is scikit-learn's built-in\n",
    "   function for K-fold cross validation).  See [Illustration of Learning Curves](http://www.astro.washington.edu/users/vanderplas/Astr599/notebooks/18_IntermediateSklearn) for more details.  \n",
    "\n",
    "<div style=\"background: yellow; padding:10px\">\n",
    "Once you find the optimal hyperparameters, retrain on **ALL** of your data to create your final model.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('loanf.csv')\n",
    "y = np.array((df['Interest.Rate'] <= 12).values)\n",
    "X = np.array(df[['FICO.Score', 'Loan.Length', 'Loan.Amount']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kfold_validation_logstic_regression(n_folds, x, y):\n",
    "\n",
    "    fold = KFold(n_splits=n_folds-1) ## use one less fold for training\n",
    "    #fold.split(x) ### this number of splits\n",
    "    hold_out_set=[]\n",
    "    mse_fold={}\n",
    "    count = 0\n",
    "    X_hotrain,X_hotest,y_hotrain,y_hotest = None,None,None,None\n",
    "\n",
    "    penalty_terms=['l1','l2']\n",
    "    max_iter = [i for i in range(1,100)]\n",
    "    \n",
    "    for pen in penalty_terms:        \n",
    "        for iteration_num in max_iter:\n",
    "            mse = {}\n",
    "\n",
    "            for idx,idy in fold.split(x):\n",
    "                if count+1==n_folds: # hold out set\n",
    "                    X_hotrain,X_hotest,y_hotrain,y_hotest = X[idx],X[idy],y[idx],y[idy]\n",
    "                    \n",
    "                \n",
    "                X_train,X_test,y_train,y_test = X[idx],X[idy],y[idx],y[idy]\n",
    "                \n",
    "                logit = LogisticRegression(penalty=pen,solver='liblinear',max_iter=iteration_num) #penalty L1 or L2 #max_iter\n",
    "                logit.fit(X_train,y_train)\n",
    "                mse[np.linalg.norm(np.subtract(y_test,logit.predict(X_test)))/sqrt(len(y_test))]=(pen,iteration_num)\n",
    "                count +=1\n",
    "\n",
    "            mse_fold[np.mean(list(mse.keys()))]=mse[min(mse.keys())] ## min mse as the key for the fold, the parameters as the value\n",
    "    print('The optimal parameters are :',mse_fold[min(mse_fold.keys())])\n",
    "    print('The test error for these parameters is:',min(mse_fold.keys()))\n",
    "    logit_final = LogisticRegression(penalty=mse_fold[min(mse_fold.keys())][0],solver='liblinear',max_iter=mse_fold[min(mse_fold.keys())][1])\n",
    "    logit_final.fit(X_hotrain,y_hotrain)\n",
    "    print('Final MSE for holdout set :',np.linalg.norm(np.subtract(y_hotest,logit_final.predict(X_hotest)))/sqrt(len(y_hotest)))\n",
    "    logit_total =LogisticRegression(penalty=mse_fold[min(mse_fold.keys())][0],solver='liblinear',max_iter=mse_fold[min(mse_fold.keys())][1])\n",
    "    logit_total.fit(x,y)\n",
    "    print('Final MSE for total dataset :',np.linalg.norm(np.subtract(y,logit_final.predict(x)))/sqrt(len(y)))\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanhilgart/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:26: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal parameters are : ('l1', 89)\n",
      "The test error for these parameters is: 0.371958230023\n",
      "Final MSE for holdout set : 0.32\n",
      "Final MSE for total dataset : 0.349857113691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanhilgart/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:34: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n",
      "/Users/jonathanhilgart/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:37: DeprecationWarning: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n"
     ]
    }
   ],
   "source": [
    "kfold_validation_logstic_regression(3,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
