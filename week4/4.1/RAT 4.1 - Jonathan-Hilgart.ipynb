{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RAT 4.1\n",
    "1 - What is the Bias-Variance Tradeoff?\n",
    "\n",
    "a. Explain what \"Bias\" and \"Variance\" are.\n",
    "\n",
    "b. When we overfit, what happens to bias and variance?\n",
    "\n",
    "c. When we underfit, what happens to bias and variance?\n",
    "\n",
    "d. Describe relationship between bias/variance and precision/accuracy\n",
    "\n",
    "e. How do Ridge and Lasso attempt to win at the Bias-Variance tradeoff? What exactly is being penalized?\n",
    "\n",
    "2 - Discuss Type I and Type II errors in the context of model quality.\n",
    "\n",
    "3 - Provide formulas and English language descriptions for precision, recall and F-1 statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) The bias variance tradeoff is the tradeoff between how well our model fits our training data vs our test data. Bias, is how far we are from the actual answer ( i.e. E(y) - E(Ypop) ), while variance the how different are the answers out model gives on new data that it sees.\n",
    "\n",
    "- b) When we overfit, we have low bias but high variance. This is because our model fits our training data very well (so low bias), but with each new data point it sees there is going to be a large difference in predictions (high variance).\n",
    "\n",
    "- c) When we underfit, we have higher bias model because our model complexity is not high enough to capture all of the features in the data. \n",
    "\n",
    "- d) Generally, when you decrease bias, you do so at the expense of increasing variance (and vice versa). Precision is the reliability of repeated measurements of your model. If you increase the accuracy of your model it is possible for precision to decrease (because Accuracy only counts positive over total and does not distinguish between TP and FP).\n",
    "\n",
    "\n",
    "- e) Ridge and lasso attempt to shrink the overall impact of any one feature in your model. This is so that you do not have a model that if overfit on any one feature. For each of these regressions, you are penalizing the weights associated with each feature. Therefore, the model is trying to decide if the increase in accuracy is worth the addition weight added to each feature.\n",
    "\n",
    "- 2) Type I error is the percent of the time that we incorrectly reject the null hypothesis. For a model, this is the number of times we predict a positive label even if the actual label is negative. Type II error is the percent of time that we fail to reject the null hypothesis. This would be if we predict a negative label, even if it is actually positive.\n",
    "\n",
    "\n",
    "- 3) Precision is TP / (TP+ FP) , Recall is TP / (TP + FN) , F1-statistic if the weighted average of precision and recall. F1 =$\\frac{ 2* Precision * Recall}{Precision+recall}$. Precision is the repeated accuracy of our model, recall is the number of relevant items that our model brings out and F-1 is a way to see the overall impact of precision and recall on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
