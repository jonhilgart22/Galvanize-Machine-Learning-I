{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 6003 2.3 Lecture\n",
    "\n",
    "## The curse of dimensionality \n",
    "\n",
    "The curse of dimensionality is a phrase that refers to a group of observational phenomena that occur when dealing with data of dimensionality greater than 3, i.e. vectors/matrices/tensors of length greater than three. It should be obvious that most real world data fits this description if we are to consider it in matrix format.\n",
    "\n",
    "As the number of dimensions increases, the [metric space](https://en.wikipedia.org/wiki/Metric_space) or distance relationships between points of data in the data set, becomes deformed. This leads to the following phenomena:\n",
    "\n",
    "1. Points separate into extrema. Points become either extremely close together or extremely far apart. Nearest points are often extremely far away, so that any differences between the distances tend towards being smaller.\n",
    "2. Geometric intuition breaks down. As a consequence of (1.), geometric intuition about algorithms breaks down. It becomes more difficult to interpret comparative values of cost function and thus harder to understand why the algorithm fails.\n",
    "3. The computational cost of managing long data vectors increases relative to their length. (This can become a significant performance penalty when algorithms need to loop over the length of the vector.)\n",
    "4. Exponential sampling increase. As d gets large, the number of vectors that are almost orthogonal increases rapidly. This causes sampling to become inefficient, as the number of data points required to adequately describe the sample space increases exponentially!\n",
    "\n",
    "The curse of dimensionality is well studied. Additional reading may be found in the included notes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuiting the Curse: Volume in Hyperspace\n",
    "\n",
    "Following the laws of probability, consider the complete outcome space $S$ for our system as being describable as a geometric object. If we think of the sampling process as random (uniform or normal) selection of points within the geometric object, this gives us some idea of the relationship of probability-sample and geometric relationships. Samples that land close to the center have distance relationships that are clear. As samples get closer to the edge, the points become constrained in one dimension (like a flattening effect.). This reduces one's ability to understand the complete sample space, as the samples land on the margin.\n",
    "\n",
    "Consider a square with sides of length $l$. We wish to compute volume occupied by a small margin as shown in the following figure:\n",
    "\n",
    "<img src=\"imgs/square_margin.png\" width=250/>\n",
    "\n",
    "In two dimensions, the fraction of the square's volume found within the margin is:\n",
    "\n",
    "$$V_M = l^2 - (0.9 l)^2 = l^2 (1 - 0.9^2) = 0.19 l^2$$\n",
    "\n",
    "In three dimensions the fraction of the cube's volume within the margin is:\n",
    "\n",
    "$$V_M = l^3 - (0.9 l)^3 = l^3 (1 - 0.9^3)= 0.27 l^3$$\n",
    "\n",
    "For the general case, as the dimensionality $d$ increases, the volume of the margine becomes:\n",
    "\n",
    "$$V_M = l^d (1 - 0.9^d) \\approx l^d$$\n",
    "\n",
    "\n",
    "##  QUIZ:\n",
    "\n",
    "When d=50, how much of the volume is stored in the margin? Answer: 99% Almost everything is a marginal sample.\n",
    "\n",
    "\n",
    "Many basic machine learning algorithms are based upon the same ideas that guide kNN: maintain a database of points $S$ you know how to solve, and when presented with a new datapoint x, use the solution/answer for the point in $S$ that is closest to to x. Due to the fact that the volume of the sample space is compressed to the edges as d increases, you need more than $e^d$ points in $S$ before a random point x is guaranteed to have a reasonably nearby point (thus describing a minimal K=1 neighborhood). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the effect of Dimensionality on Marginal Volume of a Hypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyperMarginRatio(d = 1, marginRatio = .1, edgeLen = 1):\n",
    "    TotVolume = edgeLen**d\n",
    "\n",
    "    HoleLen = edgeLen*(1-marginRatio)\n",
    "    HoleVolume = HoleLen**d\n",
    "\n",
    "    marginRatio = (TotVolume - HoleVolume)/TotVolume\n",
    "\n",
    "    print (\"When dimension = \" + str(d) + \" and the margins are \" + str(marginRatio*100) + \"% of the total edge length:\")\n",
    "    print (\"   Total volume = \" + str(TotVolume))\n",
    "    print (\"   Hole volume = \" + str(HoleVolume))\n",
    "    print (\"   So as a ratio,\")\n",
    "    print (str(100*marginRatio) + \"% of the volume is in the margins.\")\n",
    "    print(\"\")\n",
    "    return marginRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When dimension = 100 and the margins are 99.99734386011124% of the total edge length:\n",
      "   Total volume = 1\n",
      "   Hole volume = 2.6561398887587544e-05\n",
      "   So as a ratio,\n",
      "99.99734386011124% of the volume is in the margins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2 = hyperMarginRatio(d = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When dimension = 1 and the margins are 5.0% of the total edge length:\n",
      "   Total volume = 1\n",
      "   Hole volume = 0.95\n",
      "   So as a ratio,\n",
      "5.0% of the volume is in the margins.\n",
      "\n",
      "When dimension = 11 and the margins are 43.1199907724% of the total edge length:\n",
      "   Total volume = 1\n",
      "   Hole volume = 0.568800092276\n",
      "   So as a ratio,\n",
      "43.1199907724% of the volume is in the margins.\n",
      "\n",
      "When dimension = 21 and the margins are 65.9438373712% of the total edge length:\n",
      "   Total volume = 1\n",
      "   Hole volume = 0.340561626288\n",
      "   So as a ratio,\n",
      "65.9438373712% of the volume is in the margins.\n",
      "\n",
      "When dimension = 31 and the margins are 79.6093174254% of the total edge length:\n",
      "   Total volume = 1\n",
      "   Hole volume = 0.203906825746\n",
      "   So as a ratio,\n",
      "79.6093174254% of the volume is in the margins.\n",
      "\n",
      "When dimension = 41 and the margins are 87.7913451263% of the total edge length:\n",
      "   Total volume = 1\n",
      "   Hole volume = 0.122086548737\n",
      "   So as a ratio,\n",
      "87.7913451263% of the volume is in the margins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxD = 50\n",
    "marginRatio = .05\n",
    "\n",
    "\n",
    "marginRatios = []\n",
    "X = range(1,maxD+1, 10)\n",
    "\n",
    "for d in X:\n",
    "    appenders = round(hyperMarginRatio(d, marginRatio = marginRatio), 2)\n",
    "    marginRatios.append(appenders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x106333ad0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkf8HjXxusmNqqAIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1042e4d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, marginRatios)\n",
    "plt.title('Ratio of margins volume as a function of dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of Dimensionality on Distance in Hyperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another practical effect of high dimensionality is that the distance between nearest neighbours becomes very large. One way to observe this effect is through a simple simulation. \n",
    "\n",
    "Suppose we generate a set of random vectors $\\bf{x}$ in $\\mathbb{R}^d$ where each component $x_i$ is uniformally distributed between $-1$ and $1$, i.e., $x_i \\sim U(-1,1)$.\n",
    "\n",
    "The following figure displays a set of these vectors in $\\mathbb{R}^3$:\n",
    "\n",
    "<img src=\"imgs/uniform_points_cube.gif\" width=300/>\n",
    "\n",
    "Measuring the distance, $r$, of each point from the origin, and binning this into a histogram, we can see that the random points have a distribution capable of reaching the origin. As the dimensionality increases, the value of the smallest distance increases (why?). In other words, the closest point to the origin can be very far away. The following histograms illustrate this effect:\n",
    "\n",
    "<img src=\"imgs/distance_50d_hist.gif\" width=400/>\n",
    "<img src=\"imgs/distance_100d_hist.gif\" width=400/>\n",
    "<img src=\"imgs/distance_100000d_hist.gif\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of Dimensionality on Required Sampling Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dimensionality $d$ increases, the volume of the space increases so fast that the available data becomes sparse. Consider $N$ uniformaly distributed data points  in $d$ dimensional space. If we construct a hypercube by capturing an $r$ fraction of the total sample range of each feature, then number of points captured by this hypercube is given by:\n",
    "\n",
    "$$n = N r^d$$\n",
    "\n",
    "The following simulation illustrates this effect. In this simulation $1000$ points are generated and we're considering $20\\%$ of the range of each feature. The small rectangle in $2d$ captures $3.1\\%$ of the data points whereas the cube in $3d$ captures only $0.5\\%$ of total data points.\n",
    "\n",
    "<img src=\"imgs/sparcity_2d.png\" width=300/>\n",
    "<img src=\"imgs/sparcity_3d.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the Curse of Dimensionality: Feature Engineering\n",
    "\n",
    "To address the Curse, we need to lower the dimensionality of the metric space ($S$) in $\\mathbb{R}^n$ while maintaining the ordering within the metric space. This means that if we have a set of points $\\bf{x} \\in X$ in  we need to find corresponding set of points $\\bf{u}$ in a new metric space $U$ over the field $\\mathbb{R}^m$, such that $m < n$ and the metric space is almost preserved by some finite margin of error:\n",
    "\n",
    "$$\\|x_{i}-x_{j}\\|_2 \\leq \\|u_{i}-u_{j}\\|_2 \\leq (1+\\epsilon)\\|x_{i}-x_{j}\\|_2$$\n",
    "\n",
    "This process, applied professionally, is called **Feature engineering** and is the topic of approximately 50% of this class. Feature engineering today is largely a guess-and-check process where the ostensible engineer attempts to generate the mapping to the new metric space through methods meant to model the underlying meaning, thus making the mapping $S \\rightarrow U$. Oftentimes the quality of the feature engineering is only known after the fact. This has spurred numerous attempts at **automated feature engineering**, with mixed results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering by Reduction:\n",
    "\n",
    "A standard data vector in dimension $d$ is represented as a vector, i.e.:\n",
    "\n",
    "$${\\bf x} = x_1 {\\bf e}_1 + x_2 {\\bf e}_2 + \\cdots + x_d {\\bf e}_d = \\sum_{i=1}^{d} x_i {\\bf e}_i$$\n",
    "\n",
    "where ${\\bf e}_i$ are orthonormal basis vectors:\n",
    "\n",
    "$${\\bf e}_i^T {\\bf e}_j = 1 ~~~~~~~ \\text{if} ~~ i = j$$\n",
    "$$~~~~~~~ = 0 ~~~~~~~ \\text{if} ~~ i \\ne j$$\n",
    "\n",
    "Thus the data points reside in a $d$-dimensional *feature space*. We can represent the data set in terms of a $n\\times d$ matrix where $n$ is the number of data points (or samples).\n",
    "\n",
    "$${\\bf D} = \\begin{bmatrix} {\\bf x}_1^T \\\\ \\vdots \\\\ {\\bf x}_n^T \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Each point ${\\bf x}_i^T = [x_{i1} ~~ x_{i2} ~~ \\cdots ~~ x_{id}]$ is a vector in $d$-dimensional vector space.\n",
    "\n",
    "<img src=\"imgs/iris_data_standard_space.png\" width=200 />\n",
    "$$\\text{Iris data in standard feature space.}$$\n",
    "\n",
    "Given a set of $d$ orthonormal basis vectors, $U$, such that ${\\bf u}_i$ are orthonormal basis vectors that span $\\mathbb{R}^d$, we can express ${\\bf x}$ as\n",
    "\n",
    "$${\\bf x} = \\sum_{i=1}^{d} a_i {\\bf u}_i$$\n",
    "\n",
    "Such that\n",
    "\n",
    "$${\\bf u}_i^T {\\bf u}_j = 1 ~~~~~~~ \\text{if} ~~ i = j$$\n",
    "$$~~~~~~~~ = 0 ~~~~~~~ \\text{if} ~~ i \\ne j$$\n",
    "\n",
    "<img src=\"imgs/iris_data_rotated_space.png\" width=220 />\n",
    "$$\\text{Iris data in rotated space.}$$\n",
    "\n",
    "The transformed data matrix can be expressed as:\n",
    "\n",
    "$${\\bf A} = \\begin{bmatrix} {\\bf a}_1^T \\\\ \\vdots \\\\ {\\bf a}_n^T \\end{bmatrix}$$\n",
    "\n",
    "The new representation for each data point is obtained through a linear transformation ${\\bf a}_i = {\\bf U}^T {\\bf x}_i$, where $\\bf U$ is the matrix formed by orthonormal basis vectors: ${\\bf U} = [{\\bf u}_1 ~~ \\cdots ~~ {\\bf u}_d]$. As there are infinite possible choices for the set of orthonormal basis vectors, one natural question is whether there exists an optimal basis for expressing the data set in terms of fewer vectors. \n",
    "\n",
    "This is a natural application for Principal Component Analysis, and we shall review it here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "<img src=\"imgs/PCA.jpeg\" width=250/>\n",
    "\n",
    "\n",
    "### Quiz When is PCA going to work?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "Have a look at the relationship between correlation and the covariance of two variables.  https://en.wikipedia.org/wiki/Correlation_and_dependence .  In what circumstance is PCA going to be the most helpful?  \n",
    "\n",
    "\n",
    "## Implementing PCA\n",
    "\n",
    "### Mean and Variance\n",
    "\n",
    "The mean of the data matrix is the vector obtained as the average of all the points:\n",
    "\n",
    "$$mean({\\bf D}) = {\\bf \\mu} = \\frac{1}{n} \\sum_{i=1}^n {\\bf x}_i$$\n",
    "\n",
    "Thus the $i$th component of the mean vector ${\\bf \\mu}$ is the average of the values of $i$th column of ${\\bf D}$. That is\n",
    "\n",
    "$${\\bf \\mu} = [\\mu_1~~\\mu_2~~\\cdots~~\\mu_d]^T$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij} ~~~~~~\\text{for each }j=1,2,\\cdots,d$$\n",
    "\n",
    "The _total variance_ of the data matrix is the average squared distance of each point from the mean:\n",
    "\n",
    "$$var({\\bf D}) = \\frac{1}{n} \\sum_{i=1}^n \\| {\\bf x}_i - {\\bf \\mu} \\|^2$$\n",
    "$$var({\\bf D}) = \\frac{1}{n} \\sum_{i=1}^n \\left \\{ (x_{i1} - \\mu_1)^2 + (x_{i2} - \\mu_2)^2 + \\cdots + (x_{id} - \\mu_d)^2 \\right\\}$$\n",
    "\n",
    "Thus the total variance is the sum of variances of the columns of ${\\bf D}$. This can also be expressed as\n",
    "\n",
    "$$var({\\bf D}) = \\frac{1}{n} \\left( \\sum_{i=1}^n \\| {\\bf x}_i \\|^2 \\right ) - \\| \\mu \\|^2$$\n",
    "\n",
    "Thus the total variance is the difference between the average of the squared magnitude of the data points and the squared magnitude of the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-deviation Form\n",
    "\n",
    "In many applications we need to _center the data matrix_ by subtracting the mean from all the data points:\n",
    "\n",
    "$${\\bf \\hat{x}}_i = {\\bf x}_i - {\\bf \\mu}$$\n",
    "\n",
    "This gives us a new data matrix \n",
    "\n",
    "$${\\bf Z} = \\left[ \\begin{matrix} {\\bf \\hat{x}}_1^T \\\\ \\vdots \\\\ {\\bf \\hat{x}}_n^T \\end{matrix} \\right] = \\left[ \\begin{matrix} ({\\bf x}_1 - {\\bf \\mu})^T \\\\ \\vdots \\\\ ({\\bf x}_n - {\\bf \\mu})^T \\end{matrix} \\right] = \\left[ \\begin{matrix} x_{11} - \\mu_1 & \\cdots & x_{1d} - \\mu_d \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} - \\mu_1 & \\cdots & x_{nd} - \\mu_d \\end{matrix} \\right]$$\n",
    "\n",
    "$\\bf Z$ is called _centered data matrix_ for _mean-deviation form_, because $mean({\\bf Z}) = {\\bf 0}$, that is the mean coincides with the origin of the data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix\n",
    "\n",
    "The covariance matrix is a $d \\times d$ symmetric matrix that gives the covariance for each pair of attributes\n",
    "\n",
    "$${\\bf \\Sigma} = \\left[ \\begin{matrix} \\sigma_1^2 & \\sigma_{12} & \\cdots & \\sigma_{1d} \\\\ \\sigma_{21} & \\sigma_2^2 & \\cdots & \\sigma_{2d} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{d1}^2 & \\sigma_{d2} & \\cdots & \\sigma_d^2 \\end{matrix} \\right]$$\n",
    "\n",
    "The diagonal elements $\\sigma_j^2$ specity the variance of $j$th attribute or column of $\\bf D$, whereas the off-diagonal elements $\\sigma_{jk} = \\sigma_{kj}$ represent the covariance between pairs of columns.\n",
    "\n",
    "$$\\sigma_j^2 = \\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2$$\n",
    "\n",
    "$$\\sigma_{jk} = \\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)(x_{ik} - \\mu_k)$$\n",
    "\n",
    "If we represent columns of $\\bf Z$ with $n$-dimensional vector ${\\bf z}_j$:\n",
    "\n",
    "$${\\bf z}_j = \\left[ \\begin{matrix} x_{1j} - \\mu_j \\\\ \\vdots \\\\ x_{nj} - \\mu_j \\end{matrix} \\right]$$\n",
    "\n",
    "then we can write variances in a compact form:\n",
    "\n",
    "$$\\sigma_j^2 = \\frac{1}{n} {\\bf z}_j^T {\\bf z}_j~~~~~\\text{and}~~~~~\\sigma_{jk} = \\frac{1}{n} {\\bf z}_j^T {\\bf z}_k$$\n",
    "\n",
    "The covariance matrix can be written in a compact form using the centered data matrix as\n",
    "\n",
    "$${\\bf \\Sigma} = \\frac{1}{n} {\\bf Z}^T {\\bf Z}$$\n",
    "\n",
    "This is often called the *scatter matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix can also be written as a sum of rank-one matrices obtained as the outer product of each centered point:\n",
    "\n",
    "$${\\bf \\Sigma} = \\frac{1}{n} \\left[ \\begin{matrix} {\\bf \\hat{x}}_1 & \\cdots & {\\bf \\hat{x}}_n \\end{matrix} \\right]  \\left[ \\begin{matrix} {\\bf \\hat{x}}_1^T \\\\ \\vdots \\\\ {\\bf \\hat{x}}_n^T \\end{matrix} \\right] = \\frac{1}{n} \\sum_{i=1}^n {\\bf \\hat{x}}_i {\\bf \\hat{x}}_i^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
