{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship amongst - Problem complexity, model complexity and data set size.  \n",
    "What are these things?  Problem complexity comes with the data.  It may drive you to use a complicated model in order to match the complexity of the problem, but you might not always be able to use a complex model.  Why is that?  Simple models have few parameters to adjust.  Complicated models have many parameters to adjust.  The size of your data set determines how complicated a model you can fit.  If you have a lot of ROWs of data, then you can determine solid values for a lot of parameters.  If you don't have a lot of ROWs of data, then you can only fit a simple model.  The code below illustrates this.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import ceil\n",
    "from sklearn.datasets import load_iris\n",
    "from pandas.tools.plotting import scatter_matrix \n",
    "\n",
    "class KNN_tutorial(object):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.data = []\n",
    "\t\tself.models = None\n",
    "\t\tself.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\t\n",
    "\t\tself.predictions = None\t\n",
    "\t\tself.scores = None\n",
    "\t\tself.k = None\n",
    "\t\tself.centers = []\n",
    "\n",
    "\tdef load_data(self,s = 500,center_scale=0.7,center_points=5,surround_points=20,point_scale=0.3,centers = [[0,1],[1,0]], labels = [1,0]):\n",
    "\t\t'''\n",
    "\t\tInput:\n",
    "\t\t\t- s: (int) The value which the random seed will be set to\n",
    "\t\t\t- center_scale: (float) The standard deviation for the distribution of points around the two specified center points\n",
    "\t\t\t- center_points: (int) Number of points around each of the initial center points\n",
    "\t\t\t- surround_points: (int) Number of points to surround the points generated around the centers \n",
    "\t\t\t- point_scale: (float) Standard deviation for the points surrounding the points generated around the centers\n",
    "\t\t\t- centers: (2D Array) Location of the first center points\n",
    "\t\t\t- labels: (Array) Labels for the algorithm to classify\n",
    "\t\tOutput:\n",
    "\t\t\t- Randomly generated data created from the given parameters\n",
    "\t\t'''\n",
    "\t\tself.data = []\n",
    "\t\tself.models = None\n",
    "\t\tself.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\t\n",
    "\t\tself.predictions = None\t\n",
    "\t\tself.scores = None\n",
    "\t\tself.k = None\n",
    "\t\tself.centers = []\n",
    "\t\tnp.random.seed(seed = s)\n",
    "\t\n",
    "\t\tfor center,label in zip(centers,labels):\n",
    "\t\t\tgenerate_points = np.random.normal(loc=center,\n",
    "\t\t\t\t\t\t\t   scale=center_scale,\n",
    "\t\t\t\t\t\t\t   size = (center_points,2))\n",
    "\n",
    "\t\t\tgenerate_points_labels = np.insert(generate_points,2,label,axis = 1)\n",
    "\t\t\tself.centers.append(generate_points_labels)\n",
    "\n",
    "\t\t\tclusters = []\n",
    "\t\t\tfor center_point in generate_points:\n",
    "\t\t\t\tnormal_clusters = np.random.normal(loc=center_point,\n",
    "\t\t\t\t\t\t\t\t   scale=point_scale,\n",
    "\t\t\t\t\t\t\t\t   size = (surround_points,2))\n",
    "\n",
    "\t\t\t\tnormal_clusters = np.insert(normal_clusters,2,label,axis = 1)\n",
    "\t\t\t\tclusters.append(normal_clusters)\n",
    "\n",
    "\t\t\tclusters = np.array(clusters).reshape(-1,3)\n",
    "\t\t\tself.data.append(clusters) \n",
    "\n",
    "\t\tself.data = np.array(self.data).reshape(-1,3)\n",
    "\t\tself.centers = np.array(self.centers).reshape(-1,3)\n",
    "\t\tnp.random.shuffle(self.data)\t\t\n",
    "\n",
    "\tdef plot_data(self):\n",
    "\t\tplt.scatter(self.data[:,0],self.data[:,1],c=self.data[:,2],cmap=plt.cm.Blues)\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef plot_centers(self):\n",
    "\t\tplt.scatter(self.centers[:,0],self.centers[:,1],c=self.centers[:,2],cmap=plt.cm.Blues)\n",
    "\t\tplt.show()\t\n",
    "\n",
    "\tdef KNN_fit(self,k):\n",
    "\t\tself.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "\t\t\t\t\ttrain_test_split(self.data[:,:2],self.data[:,2])\n",
    "\t\tself.k = np.array(k)\t\n",
    "\t\tself.models = []\n",
    "\t\tfor k_neighbors in k:\n",
    "\t\t\tknn = KNeighborsClassifier(n_neighbors = k_neighbors) \n",
    "\t\t\tknn.fit(self.X_train,self.y_train)\n",
    "\t\t\tself.models.append(knn)\n",
    "\n",
    "\n",
    "\tdef KNN_predict(self,data):\n",
    "\t\tself.predictions = []\n",
    "\t\tfor model in self.models:\n",
    "\t\t\tpredictions = model.predict(data)\n",
    "\t\t\tall_predictions.append(predictions)\n",
    "\n",
    "\tdef KNN_score(self):\n",
    "\t\tself.scores = []\t\t\t\t\n",
    "\t\tfor model in self.models:\n",
    "\t\t\ttrain_scores = model.score(self.X_train,self.y_train)\n",
    "\t\t\ttest_scores = model.score(self.X_test,self.y_test)\n",
    "\t\t\tself.scores.append([train_scores,test_scores])\n",
    "\t\tself.scores = np.array(self.scores) \n",
    "\n",
    "\tdef plot_scores(self):\n",
    "\t\tplt.plot(len(self.data) - self.k, 1 - self.scores[:,0], c='r', label='Training Data')\n",
    "\t\tplt.plot(len(self.data) - self.k, 1 - self.scores[:,1], c='g', label='Test Data')\n",
    "\t\tplt.legend()\n",
    "\t\tplt.xlabel('Complexity')\n",
    "\t\tplt.ylabel('Error')\n",
    "\t\tplt.show()\n",
    "\t\t\n",
    "\tdef plot_decision_boundary(self):\n",
    "\t\trows = ceil(len(self.k)/2.)\n",
    "\t\th = 0.02\n",
    "\t\tx_min, x_max = self.data[:,0].min() - 1, self.data[:,0].max() + 1\n",
    "\t\ty_min, y_max = self.data[:,1].min() - 1, self.data[:,1].max() + 1\n",
    "\t\txx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "\t\t\t\t     np.arange(y_min, y_max, h))\n",
    "\n",
    "\t\t\n",
    "\t\tfor i,model in enumerate(self.models):\n",
    "\t\t\tplt.subplot(rows,2,i + 1)\n",
    "\t\t\tplt.title('Number of Neighbors: ' + str(self.k[i]))\n",
    "\t\t\tplt.scatter(self.data[:,0],self.data[:,1],c=self.data[:,2], cmap=plt.cm.Blues)\n",
    "\t\t\tZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\t\t\tZ = Z.reshape(xx.shape)\n",
    "\t\t\tplt.contour(xx,yy,Z)\t\t\n",
    "\t\tplt.tight_layout(pad=0.8)\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef load_wine_data(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef iris_scatter_matrix(self):\n",
    "\t\tiris = load_iris()\n",
    "\t\tcolumn_names = iris.feature_names \n",
    "\t\tiris_data = pd.DataFrame(iris.data,columns = column_names)\n",
    "\t\tiris_target = iris.target\n",
    "\t\tscatter_matrix(iris_data,c=iris_target) \n",
    "\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNN_tutorial()\n",
    "knn.load_data(centers=[[1,0],[0,1],[1,1]],center_scale=0.4,point_scale=0.3,s=631,center_points=5,surround_points=20,labels=[0,1,2])\n",
    "knn.KNN_fit([2,5,6,7,51])\n",
    "knn.KNN_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn.plot_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning with `sklearn`\n",
    "\n",
    "`sklearn` is a best-in-breed machine learning library for Python that we will use extensively in this class.  It also has one of the best APIs designs out there (with a [paper](http://arxiv.org/pdf/1309.0238.pdf) even written about the design) and is very modular and flexible.  As such it has a bit of a learning curve, but once you can think in the `sklearn` way for one algorithm/model you can apply that general knowledge to any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data\n",
    "\n",
    "Typically you have an external dataset that you will be working with and even if it is clean, you will need to manipulate/transform it to create features.  And as such you will load your dataset with something like `numpy` or `pandas`\n",
    "\n",
    "We will be performing a simple linear regression on a Lending Club [dataset](https://www.lendingclub.com/info/download-data.action) of interest rates for individual loans.  To start we will need to slightly prepare our data with `pandas` to get it ready for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interest.Rate</th>\n",
       "      <th>FICO.Score</th>\n",
       "      <th>Loan.Length</th>\n",
       "      <th>Monthly.Income</th>\n",
       "      <th>Loan.Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.31</td>\n",
       "      <td>670</td>\n",
       "      <td>36</td>\n",
       "      <td>4891.67</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.72</td>\n",
       "      <td>670</td>\n",
       "      <td>36</td>\n",
       "      <td>3575.00</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.27</td>\n",
       "      <td>665</td>\n",
       "      <td>36</td>\n",
       "      <td>4250.00</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21.67</td>\n",
       "      <td>670</td>\n",
       "      <td>60</td>\n",
       "      <td>14166.67</td>\n",
       "      <td>28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.98</td>\n",
       "      <td>665</td>\n",
       "      <td>36</td>\n",
       "      <td>6666.67</td>\n",
       "      <td>22000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Interest.Rate  FICO.Score  Loan.Length  Monthly.Income  Loan.Amount\n",
       "6           15.31         670           36         4891.67         6000\n",
       "11          19.72         670           36         3575.00         2000\n",
       "12          14.27         665           36         4250.00        10625\n",
       "13          21.67         670           60        14166.67        28000\n",
       "21          21.98         665           36         6666.67        22000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('loanf.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interest.Rate     0\n",
       "FICO.Score        0\n",
       "Loan.Length       0\n",
       "Monthly.Income    1\n",
       "Loan.Amount       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interest.Rate     0\n",
       "FICO.Score        0\n",
       "Loan.Length       0\n",
       "Monthly.Income    0\n",
       "Loan.Amount       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a feature matrix\n",
    "\n",
    "Remember from lecture that for any machine learning model we have **Features** (or a feature matrix) and a **Target** (or response/dependent variable from statistics parlance).  In the `sklearn` API we need to separate these from our initial data matrix.\n",
    "\n",
    "> NOTE: `sklearn` expects as input a `numpy` array/matrix. Often if you pass in a `DataFrame` Python can convert/coerce the DataFrame into a `numpy` array alright, but it is a best practice to do this conversion yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FICO.Score</th>\n",
       "      <th>Loan.Length</th>\n",
       "      <th>Monthly.Income</th>\n",
       "      <th>Loan.Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>670</td>\n",
       "      <td>36</td>\n",
       "      <td>4891.67</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>670</td>\n",
       "      <td>36</td>\n",
       "      <td>3575.00</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>665</td>\n",
       "      <td>36</td>\n",
       "      <td>4250.00</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>670</td>\n",
       "      <td>60</td>\n",
       "      <td>14166.67</td>\n",
       "      <td>28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>665</td>\n",
       "      <td>36</td>\n",
       "      <td>6666.67</td>\n",
       "      <td>22000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FICO.Score  Loan.Length  Monthly.Income  Loan.Amount\n",
       "6          670           36         4891.67         6000\n",
       "11         670           36         3575.00         2000\n",
       "12         665           36         4250.00        10625\n",
       "13         670           60        14166.67        28000\n",
       "21         665           36         6666.67        22000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.iloc[:, 1:]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     15.31\n",
       "11    19.72\n",
       "12    14.27\n",
       "13    21.67\n",
       "21    21.98\n",
       "Name: Interest.Rate, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.iloc[:, 0]\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = features.as_matrix()\n",
    "y = labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: \n",
      "[[   670.       36.     4891.67   6000.  ]\n",
      " [   670.       36.     3575.     2000.  ]\n",
      " [   665.       36.     4250.    10625.  ]\n",
      " ..., \n",
      " [   810.       36.     9250.    27000.  ]\n",
      " [   765.       36.     7083.33  25000.  ]\n",
      " [   740.       60.     8903.25  16000.  ]]\n",
      "\n",
      "\n",
      "Labels: \n",
      "[ 15.31  19.72  14.27 ...,   6.62  10.75  14.09]\n"
     ]
    }
   ],
   "source": [
    "print \"Features: \\n\", X\n",
    "print \"\\n\\nLabels: \\n\", y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The API\n",
    "\n",
    "`sklearn` has a **very** Object Oriented interface and it is import to be aware of this when building models.  It is important to note that (almost) every model/transform/object in `sklearn` is an `Estimator` object.  What is an `Estimator`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model to data X (and y)\"\"\"\n",
    "        self.some_attribute = self.some_fitting_method(X, y)\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make prediction based on passed features\"\"\"\n",
    "        pred = self.make_prediction(X_test)\n",
    "        return pred\n",
    "    \n",
    "model = Estimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Estimator` class defines a `fit()` method as well as a `predict()` method.  For an instance of an `Estimator` stored in a variable `model`:\n",
    "\n",
    "* `model.fit`: fits the model with the passed in training data.  For supervised models, it also accepts a second argument `y` that corresponds to the labels (`model.fit(X, y)`.  For unsupervised models, there are no labels so you only need to pass in the feature matrix (`model.fit(X)`)\n",
    "    > Since the interface is very OO, the instance itself stores the results of the `fit` internally.  And as such you must always `fit()` before you `predict()` on the same object.\n",
    "* `model.predict`: predicts new labels for any new datapoints passed in (`model.predict(X_test)`) and returns an array equal in length to the number of rows of what is passed in containing the predicted labels.\n",
    "\n",
    "There are 3(ish) types of subclass of estimator:\n",
    "\n",
    "* Supervised\n",
    "* Unsupervised\n",
    "* Feature Processing\n",
    "\n",
    "#### Supervised\n",
    "\n",
    "Supervised estimators in addition to the above methods typically also have:\n",
    "\n",
    "* `model.predict_proba: For classifiers that have a notion of probability (or some measure of confidence in a prediction) this method returns those \"probabilities\".  The label with the highest probability is what is returned by the `model.predict()` mehod from above.\n",
    "* `model.score`: For both classification and regression models, this method returns some measure of validation of the model (which is configurable).  For example, in regression the default is typically R^2 and classification it is accuracy.\n",
    "\n",
    "#### Unsupervised\n",
    "\n",
    "Some estimators in the library implement what is referred to as the **transformer** interface.  Unsupervised in this case refers to any method that does not need labels, including (but not limited to) unsupervised classifiers, preprocessing (like tf-idf), dimensionality reduction, etc.\n",
    "\n",
    "The **transformer** interface defines (usually) two additional methods:\n",
    "\n",
    "* `model.transform`: Given an unsupervised model, transform the input into a new basis (or feature space). This accepts on argument (usually a feature matrix) and returns a matrix of the input transformed. Note: You need to `fit()` the model before you transform it.\n",
    "* `model.fit_transform`: For some models you may not need to `fit()` and `transform()` separately.  In these cases it is more convenient to do both at the same time.  And that is precisely what `fit_transform()` does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see this in action!\n",
    "\n",
    "We will be trying to predict the loan **interest rate** based on the FICO score, loan length, monthly income, and loan amount:\n",
    "\n",
    "$$Interest.Rate = \\beta_0 + \\beta_1 \\cdot FICO.Score + \\beta_2 \\cdot Loan.Length + \\beta_3 \\cdot Monthly.Income + \\beta_4 \\cdot Loan.Amount$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training split: \n",
      "\n",
      "1874 1874\n",
      "\n",
      "\n",
      "The testing split: \n",
      "\n",
      "625 625\n"
     ]
    }
   ],
   "source": [
    "print \"The training split: \\n\"\n",
    "print len(X_train), len(y_train)\n",
    "print \"\\n\\nThe testing split: \\n\"\n",
    "print len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of an estimator\n",
    "clf = LinearRegression()\n",
    "\n",
    "# fit the estimator (notice I do not save any return value in a variable)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "# predict (but only after we have trained!)\n",
    "predictions = clf.predict(X_test)\n",
    "print len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "[ -8.61337860e-02   1.37349607e-01  -3.62467583e-05   1.45157401e-04]\n",
      "\n",
      "\n",
      "Residual sum of squares: 4.28\n",
      "\n",
      "\n",
      "Variance score: 0.75\n"
     ]
    }
   ],
   "source": [
    "# The coefficients\n",
    "print 'Coefficients: \\n', clf.coef_\n",
    "# The mean square error\n",
    "print(\"\\n\\nResidual sum of squares: %.2f\"\n",
    "      % np.mean((predictions - y_test) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('\\n\\nVariance score: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
